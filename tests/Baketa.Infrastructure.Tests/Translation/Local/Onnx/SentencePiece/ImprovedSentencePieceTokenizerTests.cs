using System;
using System.IO;
using System.Linq;
using Baketa.Core.Translation.Models;
using Baketa.Infrastructure.Translation.Local.Onnx.SentencePiece;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Xunit;
using Xunit.Abstractions;

namespace Baketa.Infrastructure.Tests.Translation.Local.Onnx.SentencePiece;

/// <summary>
/// ImprovedSentencePieceTokenizerã®ãƒ†ã‚¹ãƒˆ
/// </summary>
public class ImprovedSentencePieceTokenizerTests : SentencePieceTestBase, IDisposable
{
    private readonly ITestOutputHelper _output;
    private readonly ILogger<ImprovedSentencePieceTokenizer> _logger;
    private readonly string _testModelPath;
    private readonly string _tempDirectory;
    private bool _disposed;

    // ãƒ†ã‚¹ãƒˆç”¨ã®å®šæ•°é…åˆ—ï¼ˆCA1861è­¦å‘Šå¯¾å¿œï¼‰
    private static readonly int[][] TestTokenArrays =
    [
        [1, 2, 3],
        [100, 200, 300, 400],
        [1000],
        [5000, 10000, 15000]
    ];

    private static readonly int[] TestTokens = [123, 456, 789, 1000];
    private static readonly int[] TestTokensForDispose = [1, 2, 3];

    public ImprovedSentencePieceTokenizerTests(ITestOutputHelper output)
    {
        _output = output;
        _logger = NullLogger<ImprovedSentencePieceTokenizer>.Instance;
        
        // ãƒ†ã‚¹ãƒˆç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        _tempDirectory = Path.Combine(Path.GetTempPath(), "BaketaImprovedTest_" + Guid.NewGuid().ToString("N")[..8]);
        Directory.CreateDirectory(_tempDirectory);
        
        _testModelPath = Path.Combine(_tempDirectory, "improved-test-model.model");
        CreateTestModelFile(_testModelPath);
        
        _output.WriteLine($"ğŸ§ª æ”¹è‰¯ç‰ˆSentencePieceTokenizerãƒ†ã‚¹ãƒˆé–‹å§‹");
        _output.WriteLine($"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {_tempDirectory}");
    }

    [Fact]
    public void Constructor_ValidModelPath_InitializesSuccessfully()
    {
        // Arrange & Act
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Assert
        Assert.Equal(_testModelPath, tokenizer.ModelPath);
        Assert.Equal("ImprovedSentencePiece_improved-test-model", tokenizer.TokenizerId);
        Assert.Equal("Improved SentencePiece Tokenizer (improved-test-model)", tokenizer.Name);
        Assert.Equal(32000, tokenizer.VocabularySize);
        Assert.True(tokenizer.IsInitialized);
        
        _output.WriteLine($"âœ… åŸºæœ¬ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®ç¢ºèªå®Œäº†");
        _output.WriteLine($"   TokenizerId: {tokenizer.TokenizerId}");
        _output.WriteLine($"   Name: {tokenizer.Name}");
        _output.WriteLine($"   VocabularySize: {tokenizer.VocabularySize}");
        _output.WriteLine($"   IsRealSentencePieceAvailable: {tokenizer.IsRealSentencePieceAvailable}");
    }

    [Fact]
    public void Constructor_NonExistentModelPath_ThrowsInvalidOperationException()
    {
        // Arrange
        var nonExistentPath = Path.Combine(_tempDirectory, "non-existent.model");

        // Act & Assert
        Assert.Throws<InvalidOperationException>(() => 
            new ImprovedSentencePieceTokenizer(nonExistentPath, _logger));
        
        _output.WriteLine("âœ… å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ä¾‹å¤–å‡¦ç†ç¢ºèª");
    }

    [Fact]
    public void IsRealSentencePieceAvailable_WithDummyModel_ReturnsFalse()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act
        var isAvailable = tokenizer.IsRealSentencePieceAvailable;

        // Assert
        // ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€å®Ÿéš›ã®SentencePieceã¯åˆ©ç”¨ä¸å¯
        Assert.False(isAvailable);
        
        _output.WriteLine($"âœ… SentencePieceåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯: {isAvailable}");
    }

    [Fact]
    public void Tokenize_FallbackMode_ReturnsTokens()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
        string[] testTexts =
        [
            "Hello World",
            "Test tokenization",
            "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ",
            "Mixed English and æ—¥æœ¬èª"
        ];

        foreach (var text in testTexts)
        {
            // Act
            var tokens = tokenizer.Tokenize(text);

            // Assert
            Assert.NotNull(tokens);
            Assert.NotEmpty(tokens);
            Assert.All(tokens, token => Assert.True(token >= 0 && token < tokenizer.VocabularySize));
            
            _output.WriteLine($"âœ… '{text}' â†’ [{string.Join(", ", tokens)}] ({tokens.Length} tokens)");
        }
    }

    [Fact]
    public void Decode_FallbackMode_ReturnsText()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        foreach (var tokens in TestTokenArrays)
        {
            // Act
            var decoded = tokenizer.Decode(tokens);

            // Assert
            Assert.NotNull(decoded);
            Assert.NotEmpty(decoded);
            
            _output.WriteLine($"âœ… [{string.Join(", ", tokens)}] â†’ '{decoded}'");
        }
    }

    [Fact]
    public void RoundTrip_FallbackMode_MaintainsConsistency()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
        string[] originalTexts =
        [
            "Hello",
            "World",
            "Test",
            "Tokenization"
        ];

        foreach (var originalText in originalTexts)
        {
            // Act
            var tokens = tokenizer.Tokenize(originalText);
            var decoded = tokenizer.Decode(tokens);

            // Assert
            Assert.NotNull(tokens);
            Assert.NotEmpty(tokens);
            Assert.NotNull(decoded);
            
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã§ã¯å®Œå…¨ãªå¾©å…ƒã¯æœŸå¾…ã§ããªã„ãŒã€
            // åŸºæœ¬çš„ãªä¸€è²«æ€§ã‚’ç¢ºèªï¼ˆåŒã˜å…¥åŠ›ã«å¯¾ã—ã¦åŒã˜å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ï¼‰
            var reTokenized = tokenizer.Tokenize(originalText);
            Assert.Equal(tokens, reTokenized);
            
            _output.WriteLine($"âœ… '{originalText}' â†’ {tokens.Length} tokens â†’ '{decoded}' â†’ ä¸€è²«æ€§ç¢ºèª");
        }
    }

    [Fact]
    public void DecodeToken_SingleToken_ReturnsString()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        foreach (var token in TestTokens)
        {
            // Act
            var decoded = tokenizer.DecodeToken(token);

            // Assert
            Assert.NotNull(decoded);
            Assert.NotEmpty(decoded);
            
            _output.WriteLine($"âœ… Token {token} â†’ '{decoded}'");
        }
    }

    [Fact]
    public void GetSpecialTokens_Always_ReturnsValidTokens()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act
        var specialTokens = tokenizer.GetSpecialTokens();

        // Assert
        Assert.NotNull(specialTokens);
        Assert.True(specialTokens.UnknownId >= 0);
        Assert.True(specialTokens.BeginOfSentenceId >= 0);
        Assert.True(specialTokens.EndOfSentenceId >= 0);
        Assert.True(specialTokens.PaddingId >= 0);
        
        _output.WriteLine($"âœ… ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³:");
        _output.WriteLine($"   <unk>: {specialTokens.UnknownId}");
        _output.WriteLine($"   <s>: {specialTokens.BeginOfSentenceId}");
        _output.WriteLine($"   </s>: {specialTokens.EndOfSentenceId}");
        _output.WriteLine($"   <pad>: {specialTokens.PaddingId}");
    }

    [Fact]
    public void Tokenize_EmptyText_ReturnsEmptyArray()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act
        var tokens = tokenizer.Tokenize(string.Empty);

        // Assert
        Assert.NotNull(tokens);
        Assert.Empty(tokens);
        
        _output.WriteLine("âœ… ç©ºæ–‡å­—åˆ—ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ç¢ºèª");
    }

    [Fact]
    public void Tokenize_VeryLongText_ThrowsTokenizationException()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger, maxInputLength: 100);
        var longText = new string('A', 101);

        // Act & Assert
        var exception = Assert.Throws<TokenizationException>(() => tokenizer.Tokenize(longText));
        Assert.Contains("æœ€å¤§é•·", exception.Message, StringComparison.Ordinal);
        Assert.Equal(longText, exception.InputText);
        Assert.Equal("improved-test-model", exception.ModelName);
        
        _output.WriteLine("âœ… é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§ã®ä¾‹å¤–å‡¦ç†ç¢ºèª");
    }

    [Fact]
    public void Tokenize_NullText_ThrowsArgumentNullException()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act & Assert
        Assert.Throws<ArgumentNullException>(() => tokenizer.Tokenize(null!));
        
        _output.WriteLine("âœ… nullå…¥åŠ›ã§ã®ä¾‹å¤–å‡¦ç†ç¢ºèª");
    }

    [Fact]
    public void Decode_NullTokenArray_ThrowsArgumentNullException()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act & Assert
        Assert.Throws<ArgumentNullException>(() => tokenizer.Decode(null!));
        
        _output.WriteLine("âœ… nullé…åˆ—ã§ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ä¾‹å¤–å‡¦ç†ç¢ºèª");
    }

    [Fact(Skip = "OPUS-MTãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚scripts/download_opus_mt_models.ps1ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")]
    public void CompareWithRealImplementation_SameBehavior()
    {
        // Arrange
        using var improvedTokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
        var realLogger = NullLogger<RealSentencePieceTokenizer>.Instance;
        using var realTokenizer = new RealSentencePieceTokenizer(_testModelPath, realLogger);
        
        string[] testTexts =
        [
            "Hello World",
            "Test comparison",
            "æ—¥æœ¬èªãƒ†ã‚¹ãƒˆ"
        ];

        foreach (var text in testTexts)
        {
            // Act
            var improvedTokens = improvedTokenizer.Tokenize(text);
            var realTokens = realTokenizer.Tokenize(text);
            
            var improvedDecoded = improvedTokenizer.Decode(improvedTokens);
            var realDecoded = realTokenizer.Decode(realTokens);

            // Assert - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…åŒå£«ãªã®ã§åŒã˜çµæœãŒæœŸå¾…ã•ã‚Œã‚‹
            Assert.Equal(realTokens, improvedTokens);
            Assert.Equal(realDecoded, improvedDecoded);
            
            _output.WriteLine($"âœ… å®Ÿè£…æ¯”è¼ƒ '{text}': åŒä¸€çµæœç¢ºèª");
        }
    }

    [Fact]
    public void PerformanceComparison_MeasuresLatency()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
        var testText = "Performance test text for latency measurement";
        var iterations = 100;

        // Warmup
        for (int i = 0; i < 10; i++)
        {
            _ = tokenizer.Tokenize(testText);
        }

        // Act - æ¸¬å®š
        var stopwatch = System.Diagnostics.Stopwatch.StartNew();
        
        for (int i = 0; i < iterations; i++)
        {
            _ = tokenizer.Tokenize(testText);
        }
        
        stopwatch.Stop();

        // Assert & Report
        var avgLatencyMs = (double)stopwatch.ElapsedMilliseconds / iterations;
        
        _output.WriteLine($"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š:");
        _output.WriteLine($"   ç·æ™‚é–“: {stopwatch.ElapsedMilliseconds}ms ({iterations} å›å®Ÿè¡Œ)");
        _output.WriteLine($"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avgLatencyMs:F2}ms");
        
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ãªã®ã§æ¯”è¼ƒçš„é«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        Assert.True(avgLatencyMs < 10, $"å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ10msã‚’è¶…ãˆã¦ã„ã¾ã™: {avgLatencyMs:F2}ms");
    }

    [Fact]
    public void Dispose_CalledMultipleTimes_DoesNotThrow()
    {
        // Arrange
        var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act & Assert
        tokenizer.Dispose();
        tokenizer.Dispose(); // 2å›ç›®ã®å‘¼ã³å‡ºã—
        
        _output.WriteLine("âœ… è¤‡æ•°å›Disposeå‘¼ã³å‡ºã—ç¢ºèª");
    }

    [Fact]
    public void OperationsAfterDispose_ThrowObjectDisposedException()
    {
        // Arrange
        var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
        tokenizer.Dispose();

        // Act & Assert
        Assert.Throws<ObjectDisposedException>(() => tokenizer.Tokenize("test"));
        Assert.Throws<ObjectDisposedException>(() => tokenizer.Decode(TestTokensForDispose));
        Assert.Throws<ObjectDisposedException>(() => tokenizer.DecodeToken(1));
        Assert.Throws<ObjectDisposedException>(() => tokenizer.GetSpecialTokens());
        
        _output.WriteLine("âœ… Disposeå¾Œã®æ“ä½œä¾‹å¤–å‡¦ç†ç¢ºèª");
    }

    [Theory]
    [InlineData("")]
    [InlineData(" ")]
    [InlineData("å˜ä¸€")]
    [InlineData("Hello World")]
    [InlineData("ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆã§ã™")]
    [InlineData("Mixed æ—¥æœ¬èª and English ãƒ†ã‚­ã‚¹ãƒˆ")]
    [InlineData("Numbers 123 and symbols !@#$%")]
    public void Tokenize_VariousInputs_HandlesGracefully(string input)
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);

        // Act
        var tokens = tokenizer.Tokenize(input);

        // Assert
        Assert.NotNull(tokens);
        
        if (string.IsNullOrEmpty(input))
        {
            Assert.Empty(tokens);
        }
        else
        {
            Assert.All(tokens, token => Assert.True(token >= 0 && token < tokenizer.VocabularySize));
        }
        
        _output.WriteLine($"âœ… å¤šæ§˜ãªå…¥åŠ›ãƒ†ã‚¹ãƒˆ '{input}': {tokens.Length} tokens");
    }

    [Fact]
    public void EdgeCases_LargeTokenArrays_HandlesCorrectly()
    {
        // Arrange
        using var tokenizer = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
        var largeTokenArray = Enumerable.Range(0, 1000).ToArray();

        // Act
        var decoded = tokenizer.Decode(largeTokenArray);

        // Assert
        Assert.NotNull(decoded);
        Assert.NotEmpty(decoded);
        
        _output.WriteLine($"âœ… å¤§ããªãƒˆãƒ¼ã‚¯ãƒ³é…åˆ—ãƒ†ã‚¹ãƒˆ: {largeTokenArray.Length} tokens â†’ {decoded.Length} chars");
    }

    [Fact]
    public void TokenizerFactory_CanCreateMultipleInstances()
    {
        // Arrange & Act
        ImprovedSentencePieceTokenizer[] tokenizers = new ImprovedSentencePieceTokenizer[5];
        
        try
        {
            for (int i = 0; i < tokenizers.Length; i++)
            {
                tokenizers[i] = new ImprovedSentencePieceTokenizer(_testModelPath, _logger);
            }

            // Assert
            for (int i = 0; i < tokenizers.Length; i++)
            {
                Assert.True(tokenizers[i].IsInitialized);
                
                // å„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒç‹¬ç«‹ã—ã¦å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                var tokens = tokenizers[i].Tokenize($"Test {i}");
                Assert.NotNull(tokens);
                Assert.NotEmpty(tokens);
            }
            
            _output.WriteLine($"âœ… è¤‡æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆãƒ†ã‚¹ãƒˆ: {tokenizers.Length} instances");
        }
        finally
        {
            // ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            foreach (var tokenizer in tokenizers)
            {
                tokenizer?.Dispose();
            }
        }
    }

    private static void CreateTestModelFile(string filePath)
    {
        var content = @"# Improved Test SentencePiece Model
trainer_spec {
  model_type: UNIGRAM
  vocab_size: 32000
}
normalizer_spec {
  name: ""nfkc""
  add_dummy_prefix: true
}
pieces { piece: ""<unk>"" score: 0 type: UNKNOWN }
pieces { piece: ""<s>"" score: 0 type: CONTROL }
pieces { piece: ""</s>"" score: 0 type: CONTROL }
pieces { piece: ""<pad>"" score: 0 type: CONTROL }
pieces { piece: ""Hello"" score: -1.0 type: NORMAL }
pieces { piece: ""World"" score: -1.1 type: NORMAL }
pieces { piece: ""Test"" score: -1.2 type: NORMAL }
";
        File.WriteAllText(filePath, content);
    }

    protected virtual void Dispose(bool disposing)
    {
        if (!_disposed)
        {
            if (disposing)
            {
                // ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
                try
                {
                    if (Directory.Exists(_tempDirectory))
                    {
                        Directory.Delete(_tempDirectory, true);
                    }
                }
                catch (IOException)
                {
                    // ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã®å¤±æ•—ã¯ç„¡è¦–
                }
                catch (UnauthorizedAccessException)
                {
                    // ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡Œã‚‚ç„¡è¦–
                }
                
                _output.WriteLine("ğŸ æ”¹è‰¯ç‰ˆSentencePieceTokenizerãƒ†ã‚¹ãƒˆå®Œäº†");
            }
            _disposed = true;
        }
    }

    public void Dispose()
    {
        Dispose(true);
        GC.SuppressFinalize(this);
    }
}
