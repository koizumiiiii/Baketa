using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Baketa.Infrastructure.Translation.Local.Onnx.SentencePiece;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Xunit;
using Xunit.Abstractions;

namespace Baketa.Infrastructure.Tests.Translation.Local.Onnx.SentencePiece.Performance;

/// <summary>
/// SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
/// </summary>
public class SentencePiecePerformanceTests : IDisposable
{
    private readonly ITestOutputHelper _output;
    private readonly ILogger<RealSentencePieceTokenizer> _logger;
    private readonly string _tempDirectory;
    private readonly string _testModelPath;
    private bool _disposed;

    public SentencePiecePerformanceTests(ITestOutputHelper output)
    {
        _output = output;
        _logger = NullLogger<RealSentencePieceTokenizer>.Instance;
        
        // ãƒ†ã‚¹ãƒˆç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
        _tempDirectory = Path.Combine(Path.GetTempPath(), "BaketaPerfTest_" + Guid.NewGuid().ToString("N")[..8]);
        Directory.CreateDirectory(_tempDirectory);
        
        _testModelPath = Path.Combine(_tempDirectory, "perf-test.model");
        CreateTestModelFile(_testModelPath);
        
        _output.WriteLine($"ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹: {DateTime.Now:yyyy-MM-dd HH:mm:ss}");
        _output.WriteLine($"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {_tempDirectory}");
    }

    [Fact]
    public void TokenizePerformance_SingleText_MeasuresLatency()
    {
        // Arrange
        using var tokenizer = new RealSentencePieceTokenizer(_testModelPath, _logger);
        var testText = "This is a performance test for tokenization latency measurement.";
        var warmupIterations = 10;
        var measurementIterations = 100;

        // Warmup
        for (int i = 0; i < warmupIterations; i++)
        {
            _ = tokenizer.Tokenize(testText);
        }

        // Act - æ¸¬å®š
        var stopwatch = Stopwatch.StartNew();
        
        for (int i = 0; i < measurementIterations; i++)
        {
            _ = tokenizer.Tokenize(testText);
        }
        
        stopwatch.Stop();

        // Assert & Report
        var totalMs = stopwatch.ElapsedMilliseconds;
        var avgLatencyMs = (double)totalMs / measurementIterations;
        var throughputPerSec = 1000.0 / avgLatencyMs;

        _output.WriteLine($"ğŸ“Š å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
        _output.WriteLine($"   ç·æ™‚é–“: {totalMs}ms ({measurementIterations} å›å®Ÿè¡Œ)");
        _output.WriteLine($"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avgLatencyMs:F2}ms");
        _output.WriteLine($"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughputPerSec:F1} req/sec");
        
        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆæš«å®šå®Ÿè£…ãªã®ã§ç·©ã„åŸºæº–ï¼‰
        Assert.True(avgLatencyMs < 50, $"å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ50msã‚’è¶…ãˆã¦ã„ã¾ã™: {avgLatencyMs:F2}ms");
    }

    [Fact]
    public void TokenizeBatchPerformance_MultipleTexts_MeasuresThroughput()
    {
        // Arrange
        using var tokenizer = new RealSentencePieceTokenizer(_testModelPath, _logger);
        var testTexts = new[]
        {
            "Short text.",
            "This is a medium length sentence for performance testing.",
            "This is a much longer text that contains multiple sentences and various types of content to test the tokenizer's performance with different input lengths and complexity levels.",
            "æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚",
            "ã“ã‚Œã¯æ—¥æœ¬èªã¨è‹±èªãŒæ··åœ¨ã—ãŸmixed language textã§ã™ã€‚",
            "Performance testing with numbers: 123, 456.789, and symbols: @#$%^&*()"
        };

        var batchSize = testTexts.Length;
        var iterations = 50;

        // Warmup
        for (int i = 0; i < 5; i++)
        {
            foreach (var text in testTexts)
            {
                _ = tokenizer.Tokenize(text);
            }
        }

        // Act - ãƒãƒƒãƒå‡¦ç†æ¸¬å®š
        var stopwatch = Stopwatch.StartNew();
        
        for (int i = 0; i < iterations; i++)
        {
            foreach (var text in testTexts)
            {
                _ = tokenizer.Tokenize(text);
            }
        }
        
        stopwatch.Stop();

        // Assert & Report
        var totalTexts = batchSize * iterations;
        var totalMs = stopwatch.ElapsedMilliseconds;
        var throughputPerSec = (double)totalTexts / totalMs * 1000;
        var avgLatencyMs = (double)totalMs / totalTexts;

        _output.WriteLine($"ğŸ“Š ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
        _output.WriteLine($"   å‡¦ç†ãƒ†ã‚­ã‚¹ãƒˆæ•°: {totalTexts} ({batchSize} texts Ã— {iterations} iterations)");
        _output.WriteLine($"   ç·æ™‚é–“: {totalMs}ms");
        _output.WriteLine($"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avgLatencyMs:F2}ms/text");
        _output.WriteLine($"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughputPerSec:F1} texts/sec");
        
        Assert.True(throughputPerSec > 10, $"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒ10 texts/secã‚’ä¸‹å›ã£ã¦ã„ã¾ã™: {throughputPerSec:F1}");
    }

    [Fact]
    public void DecodePerformance_TokenArrays_MeasuresLatency()
    {
        // Arrange
        using var tokenizer = new RealSentencePieceTokenizer(_testModelPath, _logger);
        var testTexts = new[]
        {
            "Hello world",
            "Performance test",
            "Tokenization and decoding",
            "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",
            "Mixed English and æ—¥æœ¬èª text"
        };

        // äº‹å‰ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        var tokenArrays = testTexts.Select(tokenizer.Tokenize).ToArray();
        var iterations = 100;

        // Warmup
        for (int i = 0; i < 10; i++)
        {
            foreach (var tokens in tokenArrays)
            {
                _ = tokenizer.Decode(tokens);
            }
        }

        // Act - ãƒ‡ã‚³ãƒ¼ãƒ‰æ¸¬å®š
        var stopwatch = Stopwatch.StartNew();
        
        for (int i = 0; i < iterations; i++)
        {
            foreach (var tokens in tokenArrays)
            {
                _ = tokenizer.Decode(tokens);
            }
        }
        
        stopwatch.Stop();

        // Assert & Report
        var totalDecodes = tokenArrays.Length * iterations;
        var totalMs = stopwatch.ElapsedMilliseconds;
        var avgLatencyMs = (double)totalMs / totalDecodes;
        var throughputPerSec = (double)totalDecodes / totalMs * 1000;

        _output.WriteLine($"ğŸ“Š ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
        _output.WriteLine($"   ãƒ‡ã‚³ãƒ¼ãƒ‰å›æ•°: {totalDecodes}");
        _output.WriteLine($"   ç·æ™‚é–“: {totalMs}ms");
        _output.WriteLine($"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avgLatencyMs:F2}ms/decode");
        _output.WriteLine($"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughputPerSec:F1} decodes/sec");
        
        Assert.True(avgLatencyMs < 10, $"ãƒ‡ã‚³ãƒ¼ãƒ‰å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ10msã‚’è¶…ãˆã¦ã„ã¾ã™: {avgLatencyMs:F2}ms");
    }

    [Fact]
    public void RoundTripPerformance_TokenizeAndDecode_MeasuresEndToEnd()
    {
        // Arrange
        using var tokenizer = new RealSentencePieceTokenizer(_testModelPath, _logger);
        var testTexts = new[]
        {
            "Simple text",
            "Medium complexity sentence with punctuation!",
            "Complex text with numbers (123), symbols (@#$), and mixed content.",
            "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ",
            "Mixed language text with æ—¥æœ¬èª and English."
        };

        var iterations = 50;

        // Warmup
        for (int i = 0; i < 5; i++)
        {
            foreach (var text in testTexts)
            {
                var tokens = tokenizer.Tokenize(text);
                _ = tokenizer.Decode(tokens);
            }
        }

        // Act - ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆãƒªãƒƒãƒ—æ¸¬å®š
        var stopwatch = Stopwatch.StartNew();
        
        for (int i = 0; i < iterations; i++)
        {
            foreach (var text in testTexts)
            {
                var tokens = tokenizer.Tokenize(text);
                _ = tokenizer.Decode(tokens);
            }
        }
        
        stopwatch.Stop();

        // Assert & Report
        var totalRoundTrips = testTexts.Length * iterations;
        var totalMs = stopwatch.ElapsedMilliseconds;
        var avgLatencyMs = (double)totalMs / totalRoundTrips;
        var throughputPerSec = (double)totalRoundTrips / totalMs * 1000;

        _output.WriteLine($"ğŸ“Š ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆãƒªãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
        _output.WriteLine($"   ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆãƒªãƒƒãƒ—å›æ•°: {totalRoundTrips}");
        _output.WriteLine($"   ç·æ™‚é–“: {totalMs}ms");
        _output.WriteLine($"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avgLatencyMs:F2}ms/roundtrip");
        _output.WriteLine($"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughputPerSec:F1} roundtrips/sec");
        
        Assert.True(avgLatencyMs < 30, $"ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆãƒªãƒƒãƒ—å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ30msã‚’è¶…ãˆã¦ã„ã¾ã™: {avgLatencyMs:F2}ms");
    }

    [Fact]
    public async Task ConcurrentPerformance_ParallelTokenization_MeasuresScalability()
    {
        // Arrange
        var tokenizerCount = Environment.ProcessorCount;
        var tokenizers = new List<RealSentencePieceTokenizer>();
        
        try
        {
            // è¤‡æ•°ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            for (int i = 0; i < tokenizerCount; i++)
            {
                tokenizers.Add(new RealSentencePieceTokenizer(_testModelPath, _logger));
            }

            var testText = "Concurrent performance test with parallel tokenization processing.";
            var tasksPerTokenizer = 20;
            var totalTasks = tokenizerCount * tasksPerTokenizer;

            // Act - ä¸¦è¡Œå‡¦ç†æ¸¬å®š
            var stopwatch = Stopwatch.StartNew();
            
            var tasks = new List<Task>();
            for (int i = 0; i < tokenizerCount; i++)
            {
                var tokenizer = tokenizers[i];
                var tokenizerTasks = Enumerable.Range(0, tasksPerTokenizer)
                    .Select(_ => Task.Run(() =>
                    {
                        var tokens = tokenizer.Tokenize(testText);
                        return tokenizer.Decode(tokens);
                    }));
                tasks.AddRange(tokenizerTasks);
            }
            
            await Task.WhenAll(tasks).ConfigureAwait(true);
            stopwatch.Stop();

            // Assert & Report
            var totalMs = stopwatch.ElapsedMilliseconds;
            var throughputPerSec = (double)totalTasks / totalMs * 1000;
            var avgLatencyMs = (double)totalMs / totalTasks;

            _output.WriteLine($"ğŸ“Š ä¸¦è¡Œå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
            _output.WriteLine($"   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æ•°: {tokenizerCount}");
            _output.WriteLine($"   ç·ã‚¿ã‚¹ã‚¯æ•°: {totalTasks}");
            _output.WriteLine($"   ç·æ™‚é–“: {totalMs}ms");
            _output.WriteLine($"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avgLatencyMs:F2}ms/task");
            _output.WriteLine($"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughputPerSec:F1} tasks/sec");
            
            Assert.True(throughputPerSec > 50, $"ä¸¦è¡Œã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒ50 tasks/secã‚’ä¸‹å›ã£ã¦ã„ã¾ã™: {throughputPerSec:F1}");
        }
        finally
        {
            // ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            foreach (var tokenizer in tokenizers)
            {
                tokenizer.Dispose();
            }
        }
    }

    [Fact]
    public void MemoryUsageProfile_LongRunning_MonitorsMemoryConsumption()
    {
        // Arrange
        using var tokenizer = new RealSentencePieceTokenizer(_testModelPath, _logger);
        var testTexts = GenerateTestTexts(1000); // å¤§é‡ã®ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        GC.Collect();
        GC.WaitForPendingFinalizers();
        GC.Collect();
        
        var initialMemory = GC.GetTotalMemory(false);

        // Act - é•·æ™‚é–“å®Ÿè¡Œãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        var stopwatch = Stopwatch.StartNew();
        var processedCount = 0;
        
        foreach (var text in testTexts)
        {
            var tokens = tokenizer.Tokenize(text);
            _ = tokenizer.Decode(tokens);
            processedCount++;
            
            // 100ä»¶ã”ã¨ã«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
            if (processedCount % 100 == 0)
            {
                var currentMemory = GC.GetTotalMemory(false);
                var memoryIncrease = currentMemory - initialMemory;
                
                // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒç•°å¸¸ã«å¢—åŠ ã—ã¦ã„ã‚‹å ´åˆã¯è­¦å‘Š
                if (memoryIncrease > 50 * 1024 * 1024) // 50MBé–¾å€¤
                {
                    _output.WriteLine($"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è­¦å‘Š: {processedCount} ä»¶å‡¦ç†å¾Œã€{memoryIncrease / 1024 / 1024}MBå¢—åŠ ");
                }
            }
        }
        
        stopwatch.Stop();

        // Final memory check
        GC.Collect();
        GC.WaitForPendingFinalizers();
        GC.Collect();
        
        var finalMemory = GC.GetTotalMemory(false);
        var totalMemoryIncrease = finalMemory - initialMemory;

        // Assert & Report
        _output.WriteLine($"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:");
        _output.WriteLine($"   å‡¦ç†ä»¶æ•°: {processedCount}");
        _output.WriteLine($"   å‡¦ç†æ™‚é–“: {stopwatch.ElapsedMilliseconds}ms");
        _output.WriteLine($"   åˆæœŸãƒ¡ãƒ¢ãƒª: {initialMemory / 1024 / 1024:F1}MB");
        _output.WriteLine($"   æœ€çµ‚ãƒ¡ãƒ¢ãƒª: {finalMemory / 1024 / 1024:F1}MB");
        _output.WriteLine($"   ãƒ¡ãƒ¢ãƒªå¢—åŠ : {totalMemoryIncrease / 1024 / 1024:F1}MB");
        
        // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒãªã„ã“ã¨ã‚’ç¢ºèªï¼ˆç·©ã„åŸºæº–ï¼‰
        Assert.True(totalMemoryIncrease < 100 * 1024 * 1024, 
            $"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ100MBã‚’è¶…ãˆã¦å¢—åŠ ã—ã¦ã„ã¾ã™: {totalMemoryIncrease / 1024 / 1024:F1}MB");
    }

    [Fact]
    public void CompareImplementations_RealVsTemporary_PerformanceDifference()
    {
        // Arrange
        using var realTokenizer = new RealSentencePieceTokenizer(_testModelPath, _logger);
#pragma warning disable CS0618 // Type or member is obsolete - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã®ãŸã‚ä¸€æ™‚çš„ã«ä½¿ç”¨
        var tempLogger = NullLogger<TemporarySentencePieceTokenizer>.Instance;
        using var tempTokenizer = new TemporarySentencePieceTokenizer(_testModelPath, "temp-model", tempLogger);
        
        // TemporarySentencePieceTokenizerã‚’åˆæœŸåŒ–
        var initResult = tempTokenizer.Initialize();
        Assert.True(initResult, "TemporarySentencePieceTokenizerã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ");
#pragma warning restore CS0618 // Type or member is obsolete
        
        var testTexts = new[]
        {
            "Comparison test text",
            "Performance comparison between implementations",
            "æ—¥æœ¬èªæ¯”è¼ƒãƒ†ã‚¹ãƒˆ",
            "Mixed comparison ãƒ†ã‚­ã‚¹ãƒˆ with various content"
        };

        var iterations = 50;

        // Real implementation measurement
        var realStopwatch = Stopwatch.StartNew();
        for (int i = 0; i < iterations; i++)
        {
            foreach (var text in testTexts)
            {
                var tokens = realTokenizer.Tokenize(text);
                _ = realTokenizer.Decode(tokens);
            }
        }
        realStopwatch.Stop();

        // Temporary implementation measurement
        var tempStopwatch = Stopwatch.StartNew();
        for (int i = 0; i < iterations; i++)
        {
            foreach (var text in testTexts)
            {
                var tokens = tempTokenizer.Tokenize(text);
                _ = tempTokenizer.Decode(tokens);
            }
        }
        tempStopwatch.Stop();

        // Assert & Report
        var totalOperations = testTexts.Length * iterations;
        
        // ã‚ˆã‚Šç¢ºå®Ÿãªæ™‚é–“æ¸¬å®šã®ãŸã‚ã€Ticksã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦ã§æ¸¬å®š
        var realTimeMs = Math.Max(0.001, realStopwatch.Elapsed.TotalMilliseconds);
        var tempTimeMs = Math.Max(0.001, tempStopwatch.Elapsed.TotalMilliseconds);
        
        // éå¸¸ã«é«˜é€Ÿãªå ´åˆã®å¯¾ç­–ã¨ã—ã¦ã€æœ€å°å®Ÿè¡Œæ™‚é–“ã‚’è¨­å®š
        if (realTimeMs < 1.0 && tempTimeMs < 1.0)
        {
            _output.WriteLine($"ğŸ“Š å®Ÿè£…æ¯”è¼ƒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
            _output.WriteLine($"   Realå®Ÿè£…: {realTimeMs:F3}ms (éå¸¸ã«é«˜é€Ÿ)");
            _output.WriteLine($"   Temporaryå®Ÿè£…: {tempTimeMs:F3}ms (éå¸¸ã«é«˜é€Ÿ)");
            _output.WriteLine($"   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”: æ¸¬å®šå›°é›£ï¼ˆä¸¡å®Ÿè£…ã¨ã‚‚ååˆ†é«˜é€Ÿï¼‰");
            
            // ä¸¡æ–¹ã¨ã‚‚ååˆ†é«˜é€Ÿãªã®ã§ãƒ†ã‚¹ãƒˆæˆåŠŸã¨ã™ã‚‹
            Assert.True(true, "ä¸¡å®Ÿè£…ã¨ã‚‚ååˆ†é«˜é€Ÿã§ã™");
            return;
        }
        
        var realThroughput = totalOperations / realTimeMs * 1000;
        var tempThroughput = totalOperations / tempTimeMs * 1000;
        
        // å®‰å…¨ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¨ˆç®—
        double performanceRatio;
        string comparisonText;
        
        var timeDifference = Math.Abs(realTimeMs - tempTimeMs);
        
        if (timeDifference < 0.1)
        {
            // ã»ã¼åŒã˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå·®ãŒ0.1msæœªæº€ï¼‰
            performanceRatio = 1.0;
            comparisonText = "åŒç­‰";
        }
        else if (realTimeMs < tempTimeMs)
        {
            // Realå®Ÿè£…ã®æ–¹ãŒé€Ÿã„
            performanceRatio = tempTimeMs / realTimeMs;
            comparisonText = $"Realå®Ÿè£…ãŒ{performanceRatio:F2}å€é€Ÿã„";
        }
        else
        {
            // Temporaryå®Ÿè£…ã®æ–¹ãŒé€Ÿã„
            performanceRatio = realTimeMs / tempTimeMs;
            comparisonText = $"Temporaryå®Ÿè£…ãŒ{performanceRatio:F2}å€é€Ÿã„";
        }

        _output.WriteLine($"ğŸ“Š å®Ÿè£…æ¯”è¼ƒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:");
        _output.WriteLine($"   Realå®Ÿè£…: {realTimeMs:F2}ms ({realThroughput:F1} ops/sec)");
        _output.WriteLine($"   Temporaryå®Ÿè£…: {tempTimeMs:F2}ms ({tempThroughput:F1} ops/sec)");
        _output.WriteLine($"   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”: {comparisonText}");
        
        // NaN/Infinityã®å³å¯†ãƒã‚§ãƒƒã‚¯
        Assert.True(double.IsFinite(performanceRatio), 
            $"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”ãŒç„¡åŠ¹ãªå€¤ã§ã™: {performanceRatio}");
        
        // ã‚ˆã‚Šç¾å®Ÿçš„ãªé–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆ1000å€ä»¥å†…ï¼‰
        Assert.True(performanceRatio <= 1000, 
            $"å®Ÿè£…é–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ãŒå¤§ãã™ãã¾ã™: {performanceRatio:F2}å€");
    }

    private static void CreateTestModelFile(string filePath)
    {
        var content = @"# Performance Test SentencePiece Model
trainer_spec {
  model_type: UNIGRAM
  vocab_size: 2000
}
normalizer_spec {
  name: ""nfkc""
  add_dummy_prefix: true
}
pieces { piece: ""<unk>"" score: 0 type: UNKNOWN }
pieces { piece: ""<s>"" score: 0 type: CONTROL }
pieces { piece: ""</s>"" score: 0 type: CONTROL }
# Common English words
pieces { piece: ""the"" score: -1.0 type: NORMAL }
pieces { piece: ""and"" score: -1.1 type: NORMAL }
pieces { piece: ""test"" score: -1.2 type: NORMAL }
pieces { piece: ""performance"" score: -1.3 type: NORMAL }
# Common Japanese characters
pieces { piece: ""ã®"" score: -1.4 type: NORMAL }
pieces { piece: ""ã¯"" score: -1.5 type: NORMAL }
pieces { piece: ""ãƒ†ã‚¹ãƒˆ"" score: -1.6 type: NORMAL }
";
        File.WriteAllText(filePath, content);
    }

    private static IEnumerable<string> GenerateTestTexts(int count)
    {
        var patterns = new[]
        {
            "Short text {0}",
            "Medium length sentence for performance testing number {0}",
            "This is a longer sentence that contains multiple words and various types of content for testing purposes with ID {0}",
            "æ—¥æœ¬èªãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ {0}",
            "Mixed language text {0} with æ—¥æœ¬èª content",
            "Performance test with numbers: {0}, symbols: @#$%^&*(), and punctuation!"
        };

        for (int i = 0; i < count; i++)
        {
            var pattern = patterns[i % patterns.Length];
            yield return string.Format(System.Globalization.CultureInfo.InvariantCulture, pattern, i);
        }
    }

    protected virtual void Dispose(bool disposing)
    {
        if (!_disposed)
        {
            if (disposing)
            {
                // ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
                try
                {
                    if (Directory.Exists(_tempDirectory))
                    {
                        Directory.Delete(_tempDirectory, true);
                    }
                }
                catch (IOException)
                {
                    // ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã®å¤±æ•—ã¯ç„¡è¦–
                }
                catch (UnauthorizedAccessException)
                {
                    // ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®å•é¡Œã‚‚ç„¡è¦–
                }
                
                _output.WriteLine($"ğŸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†: {DateTime.Now:yyyy-MM-dd HH:mm:ss}");
            }
            _disposed = true;
        }
    }

    public void Dispose()
    {
        Dispose(true);
        GC.SuppressFinalize(this);
    }
}
