using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Globalization;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Baketa.Core.Translation.Models;
using Baketa.Infrastructure.Translation.Local.Onnx.SentencePiece;
using Baketa.Infrastructure.Translation.Local.Onnx.SentencePiece.Native;
using FluentAssertions;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Xunit;
using Xunit.Abstractions;

namespace Baketa.Infrastructure.Tests.Translation.Local.Onnx.SentencePiece;

/// <summary>
/// Native vs Real Tokenizer ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
/// çµ±åˆåŠ¹æœã®å®šé‡çš„è©•ä¾¡
/// </summary>
public class TokenizerPerformanceBenchmarks(ITestOutputHelper output) : IDisposable
{
    private readonly string _projectRoot = GetProjectRootDirectory();
    private bool _disposed;

    [Fact]
    public async Task CompareTokenizers_ProcessingSpeed_NativeShouldBeFaster()
    {
        // Arrange
        var modelPath = Path.Combine(_projectRoot, "Models", "SentencePiece", "opus-mt-ja-en.model");
        
        if (!File.Exists(modelPath))
        {
            output.WriteLine($"âš ï¸  Skipping test: Model file not found at {modelPath}");
            return;
        }

        var testTexts = GenerateTestDataset(500); // 500ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆ
        
        output.WriteLine($"ğŸ Tokenizer Performance Benchmark");
        output.WriteLine($"ğŸ“Š Dataset: {testTexts.Count:N0} texts");
        output.WriteLine($"ğŸ“ Total characters: {testTexts.Sum(t => t.Length):N0}");
        output.WriteLine($"ğŸ“‚ Model: {Path.GetFileName(modelPath)}");
        output.WriteLine("");

        // Native Tokenizer ãƒ†ã‚¹ãƒˆ
        var nativeResults = await BenchmarkNativeTokenizer(modelPath, testTexts);
        
        // Real Tokenizer ãƒ†ã‚¹ãƒˆ
        var realResults = await BenchmarkRealTokenizer(modelPath, testTexts);

        // çµæœæ¯”è¼ƒã¨ãƒ¬ãƒãƒ¼ãƒˆ
        GeneratePerformanceReport(nativeResults, realResults);
        
        // ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ - Native ãŒé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…
        nativeResults.AverageLatencyMs.Should().BeLessOrEqualTo(realResults.AverageLatencyMs * 1.2, 
            "Native tokenizer should be faster or comparable to Real tokenizer");
    }

    [Fact]
    public async Task CompareTokenizers_MemoryUsage_NativeShouldBeEfficient()
    {
        // Arrange
        var modelPath = Path.Combine(_projectRoot, "Models", "SentencePiece", "opus-mt-ja-en.model");
        
        if (!File.Exists(modelPath))
        {
            output.WriteLine($"âš ï¸  Skipping test: Model file not found at {modelPath}");
            return;
        }

        var testTexts = GenerateTestDataset(100); // ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆç”¨

        output.WriteLine($"ğŸ’¾ Memory Usage Benchmark");
        output.WriteLine($"ğŸ“Š Dataset: {testTexts.Count:N0} texts");
        output.WriteLine("");

        // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ
        var nativeMemory = await MeasureMemoryUsage("Native", () => 
            BenchmarkNativeTokenizerMemory(modelPath, testTexts));
            
        var realMemory = await MeasureMemoryUsage("Real", () => 
            BenchmarkRealTokenizerMemory(modelPath, testTexts));

        output.WriteLine($"ğŸ“ˆ Memory Comparison:");
        output.WriteLine($"  Native: {nativeMemory:N0} bytes");
        output.WriteLine($"  Real:   {realMemory:N0} bytes");
        output.WriteLine($"  Ratio:  {(double)nativeMemory / realMemory:F2}x");

        // Native ã®æ–¹ãŒãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„ã“ã¨ã‚’æœŸå¾…
        nativeMemory.Should().BeLessOrEqualTo((long)(realMemory * 1.5), 
            "Native tokenizer should be memory efficient");
    }

    [Fact]
    public async Task CompareTokenizers_Accuracy_ShouldProduceSimilarResults()
    {
        // Arrange
        var modelPath = Path.Combine(_projectRoot, "Models", "SentencePiece", "opus-mt-ja-en.model");
        
        if (!File.Exists(modelPath))
        {
            output.WriteLine($"âš ï¸  Skipping test: Model file not found at {modelPath}");
            return;
        }

        var testTexts = new[]
        {
            "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼",
            "ã“ã‚Œã¯ç¿»è¨³ãƒ†ã‚¹ãƒˆã§ã™ã€‚",
            "OPUS-MTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ¯”è¼ƒå®Ÿé¨“",
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­",
            "æ—¥æœ¬èªã¨è‹±èªã®ç¿»è¨³ç²¾åº¦ç¢ºèª"
        };

        output.WriteLine($"ğŸ¯ Accuracy Comparison Test");
        output.WriteLine($"ğŸ“Š Test phrases: {testTexts.Length}");
        output.WriteLine("");

        using var loggerFactory = LoggerFactory.Create(builder => builder.AddConsole());

        // Native Tokenizer
        using var nativeTokenizer = await OpusMtNativeTokenizer.CreateAsync(modelPath);
        
        // Real Tokenizer
        var realLogger = loggerFactory.CreateLogger<RealSentencePieceTokenizer>();
        using var realTokenizer = new RealSentencePieceTokenizer(modelPath, realLogger);

        var results = new List<AccuracyResult>();

        foreach (var text in testTexts)
        {
            var nativeTokens = nativeTokenizer.Tokenize(text);
            var realTokens = realTokenizer.Tokenize(text);
            
            var nativeDecoded = nativeTokenizer.Decode(nativeTokens);
            var realDecoded = realTokenizer.Decode(realTokens);

            var result = new AccuracyResult
            {
                Input = text,
                NativeTokens = nativeTokens,
                RealTokens = realTokens,
                NativeDecoded = nativeDecoded,
                RealDecoded = realDecoded,
                TokenCountMatch = nativeTokens.Length == realTokens.Length,
                DecodingMatch = string.Equals(nativeDecoded, realDecoded, StringComparison.Ordinal)
            };
            
            results.Add(result);
            
            output.WriteLine($"ğŸ“ '{text}'");
            output.WriteLine($"  Native: {nativeTokens.Length} tokens â†’ '{nativeDecoded}'");
            output.WriteLine($"  Real:   {realTokens.Length} tokens â†’ '{realDecoded}'");
            output.WriteLine($"  Match:  Tokens={result.TokenCountMatch}, Decoded={result.DecodingMatch}");
            output.WriteLine("");
        }

        // ç²¾åº¦ã‚µãƒãƒªãƒ¼
        var tokenAccuracy = results.Count(r => r.TokenCountMatch) / (double)results.Count;
        var decodingAccuracy = results.Count(r => r.DecodingMatch) / (double)results.Count;
        
        output.WriteLine($"ğŸ“Š Accuracy Summary:");
        output.WriteLine($"  Token Count Match: {tokenAccuracy:P1}");
        output.WriteLine($"  Decoding Match:    {decodingAccuracy:P1}");

        // æœ€ä½é™ã®ç²¾åº¦è¦æ±‚
        tokenAccuracy.Should().BeGreaterThan(0.6, "Token count should be reasonably similar");
    }

    private async Task<BenchmarkResult> BenchmarkNativeTokenizer(string modelPath, List<string> testTexts)
    {
        output.WriteLine($"ğŸ”¥ Benchmarking Native Tokenizer...");
        
        using var tokenizer = await OpusMtNativeTokenizer.CreateAsync(modelPath);
        
        // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for (int i = 0; i < Math.Min(10, testTexts.Count); i++)
        {
            tokenizer.Tokenize(testTexts[i]);
        }

        // å®Ÿæ¸¬å®š
        var sw = Stopwatch.StartNew();
        var totalTokens = 0;
        
        foreach (var text in testTexts)
        {
            var tokens = tokenizer.Tokenize(text);
            totalTokens += tokens.Length;
        }
        
        sw.Stop();

        var result = new BenchmarkResult
        {
            Implementation = "Native",
            TotalTexts = testTexts.Count,
            TotalTokens = totalTokens,
            TotalTimeMs = sw.ElapsedMilliseconds,
            AverageLatencyMs = (double)sw.ElapsedMilliseconds / testTexts.Count,
            TokensPerSecond = totalTokens / (sw.ElapsedMilliseconds / 1000.0)
        };

        output.WriteLine($"  â±ï¸  Total time: {result.TotalTimeMs:N0}ms");
        output.WriteLine($"  ğŸ“Š Avg latency: {result.AverageLatencyMs:F2}ms/text");
        output.WriteLine($"  ğŸš€ Throughput: {result.TokensPerSecond:N0} tokens/sec");
        output.WriteLine("");

        return result;
    }

    private async Task<BenchmarkResult> BenchmarkRealTokenizer(string modelPath, List<string> testTexts)
    {
        await Task.Yield(); // éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã®è­¦å‘Šã‚’è§£æ¶ˆ
        
        output.WriteLine($"ğŸ”¥ Benchmarking Real Tokenizer...");
        
        using var loggerFactory = LoggerFactory.Create(builder => builder.AddConsole());
        var logger = loggerFactory.CreateLogger<RealSentencePieceTokenizer>();
        using var tokenizer = new RealSentencePieceTokenizer(modelPath, logger);
        
        // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for (int i = 0; i < Math.Min(10, testTexts.Count); i++)
        {
            tokenizer.Tokenize(testTexts[i]);
        }

        // å®Ÿæ¸¬å®š
        var sw = Stopwatch.StartNew();
        var totalTokens = 0;
        
        foreach (var text in testTexts)
        {
            var tokens = tokenizer.Tokenize(text);
            totalTokens += tokens.Length;
        }
        
        sw.Stop();

        var result = new BenchmarkResult
        {
            Implementation = "Real",
            TotalTexts = testTexts.Count,
            TotalTokens = totalTokens,
            TotalTimeMs = sw.ElapsedMilliseconds,
            AverageLatencyMs = (double)sw.ElapsedMilliseconds / testTexts.Count,
            TokensPerSecond = totalTokens / (sw.ElapsedMilliseconds / 1000.0)
        };

        output.WriteLine($"  â±ï¸  Total time: {result.TotalTimeMs:N0}ms");
        output.WriteLine($"  ğŸ“Š Avg latency: {result.AverageLatencyMs:F2}ms/text");
        output.WriteLine($"  ğŸš€ Throughput: {result.TokensPerSecond:N0} tokens/sec");
        output.WriteLine("");

        return result;
    }

    private async Task<long> BenchmarkNativeTokenizerMemory(string modelPath, List<string> testTexts)
    {
        using var tokenizer = await OpusMtNativeTokenizer.CreateAsync(modelPath);
        
        foreach (var text in testTexts)
        {
            tokenizer.Tokenize(text);
        }
        
        return GC.GetTotalMemory(true);
    }

    private Task<long> BenchmarkRealTokenizerMemory(string modelPath, List<string> testTexts)
    {
        using var loggerFactory = LoggerFactory.Create(builder => builder.AddConsole());
        var logger = loggerFactory.CreateLogger<RealSentencePieceTokenizer>();
        using var tokenizer = new RealSentencePieceTokenizer(modelPath, logger);
        
        foreach (var text in testTexts)
        {
            tokenizer.Tokenize(text);
        }
        
        return Task.FromResult(GC.GetTotalMemory(true));
    }

    private async Task<long> MeasureMemoryUsage(string implementation, Func<Task<long>> operation)
    {
        // ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        GC.Collect();
        GC.WaitForPendingFinalizers();
        GC.Collect();
        
        var beforeMemory = GC.GetTotalMemory(false);
        var afterMemory = await operation();
        
        return afterMemory - beforeMemory;
    }

    private void GeneratePerformanceReport(BenchmarkResult native, BenchmarkResult real)
    {
        output.WriteLine($"ğŸ“Š Performance Comparison Report");
        output.WriteLine($"================================");
        output.WriteLine("");
        
        output.WriteLine($"â±ï¸  Processing Time:");
        output.WriteLine($"  Native: {native.TotalTimeMs:N0}ms");
        output.WriteLine($"  Real:   {real.TotalTimeMs:N0}ms");
        output.WriteLine($"  Ratio:  {(double)native.TotalTimeMs / real.TotalTimeMs:F2}x");
        output.WriteLine("");
        
        output.WriteLine($"ğŸ“Š Average Latency:");
        output.WriteLine($"  Native: {native.AverageLatencyMs:F2}ms/text");
        output.WriteLine($"  Real:   {real.AverageLatencyMs:F2}ms/text");
        output.WriteLine($"  Improvement: {((real.AverageLatencyMs - native.AverageLatencyMs) / real.AverageLatencyMs * 100):F1}%");
        output.WriteLine("");
        
        output.WriteLine($"ğŸš€ Throughput:");
        output.WriteLine($"  Native: {native.TokensPerSecond:N0} tokens/sec");
        output.WriteLine($"  Real:   {real.TokensPerSecond:N0} tokens/sec");
        output.WriteLine($"  Improvement: {((native.TokensPerSecond - real.TokensPerSecond) / real.TokensPerSecond * 100):F1}%");
        output.WriteLine("");

        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ¤å®š
        var performanceGain = (real.AverageLatencyMs - native.AverageLatencyMs) / real.AverageLatencyMs;
        var status = performanceGain switch
        {
            > 0.2 => "ğŸŸ¢ Excellent (+20%)",
            > 0.1 => "ğŸŸ¡ Good (+10%)",
            > 0.0 => "ğŸŸ  Marginal",
            _ => "ğŸ”´ Slower"
        };
        
        output.WriteLine($"ğŸ¯ Performance Status: {status}");
    }

    private List<string> GenerateTestDataset(int count)
    {
        var templates = new[]
        {
            "ã“ã‚“ã«ã¡ã¯ã€{0}ã§ã™ã€‚",
            "ä»Šæ—¥ã¯{0}ã«ã¤ã„ã¦è©±ã—ã¾ã™ã€‚",
            "{0}ã®ç¿»è¨³ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚",
            "ã‚²ãƒ¼ãƒ å†…ã®{0}ã‚·ã‚¹ãƒ†ãƒ ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚",
            "{0}æ©Ÿèƒ½ã®æ€§èƒ½ã‚’æ¸¬å®šã—ã¦ã„ã¾ã™ã€‚",
            "OPUS-MT{0}ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯",
            "{0}ã®å‡¦ç†é€Ÿåº¦ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
            "ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³{0}ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚",
            "{0}ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚",
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ{0}ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚"
        };

        var keywords = new[]
        {
            "ã‚·ã‚¹ãƒ†ãƒ ", "æ©Ÿèƒ½", "ãƒ†ã‚¹ãƒˆ", "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯", "æ€§èƒ½",
            "ç¿»è¨³", "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼", "ã‚¨ãƒ³ã‚¸ãƒ³", "å‡¦ç†", "å®Ÿè¡Œ",
            "ç¢ºèª", "æ¸¬å®š", "æ¯”è¼ƒ", "çµ±åˆ", "æœ€é©åŒ–",
            "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ãƒ‡ãƒ¼ã‚¿", "çµæœ", "åŠ¹ç‡", "é€Ÿåº¦"
        };

        var random = new Random(42); // å›ºå®šã‚·ãƒ¼ãƒ‰
        var texts = new List<string>();

        for (int i = 0; i < count; i++)
        {
            var template = templates[random.Next(templates.Length)];
            var keyword = keywords[random.Next(keywords.Length)];
            texts.Add(string.Format(CultureInfo.InvariantCulture, template, keyword));
        }

        return texts;
    }

    private static string GetProjectRootDirectory()
    {
        var currentDir = Directory.GetCurrentDirectory();
        while (currentDir != null && !File.Exists(Path.Combine(currentDir, "Baketa.sln")))
        {
            currentDir = Directory.GetParent(currentDir)?.FullName;
        }
        return currentDir ?? throw new DirectoryNotFoundException("Project root not found");
    }

    public void Dispose()
    {
        if (_disposed)
            return;
            
        _disposed = true;
        GC.SuppressFinalize(this);
    }
}

/// <summary>
/// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
/// </summary>
public class BenchmarkResult
{
    public string Implementation { get; set; } = string.Empty;
    public int TotalTexts { get; set; }
    public int TotalTokens { get; set; }
    public long TotalTimeMs { get; set; }
    public double AverageLatencyMs { get; set; }
    public double TokensPerSecond { get; set; }
}

/// <summary>
/// ç²¾åº¦æ¯”è¼ƒçµæœ
/// </summary>
public class AccuracyResult
{
    public string Input { get; set; } = string.Empty;
    public int[] NativeTokens { get; set; } = [];
    public int[] RealTokens { get; set; } = [];
    public string NativeDecoded { get; set; } = string.Empty;
    public string RealDecoded { get; set; } = string.Empty;
    public bool TokenCountMatch { get; set; }
    public bool DecodingMatch { get; set; }
}
