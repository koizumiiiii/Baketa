using System;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Baketa.Infrastructure.Translation.Local.Onnx.SentencePiece.Native;
using Microsoft.Extensions.Logging;
using Xunit;
using Xunit.Abstractions;

namespace Baketa.Infrastructure.Tests.Translation;

/// <summary>
/// å˜ä¸€SentencePieceãƒ¢ãƒ‡ãƒ«ã®è©³ç´°åˆ†æãƒ†ã‚¹ãƒˆ
/// </summary>
public class SingleModelDetailedAnalysisTest(ITestOutputHelper output)
{
    [Fact]
    public void HuggingFaceTargetModel_DetailedAnalysis_ShouldRevealActualVocabSize()
    {
        output.WriteLine("=== HuggingFace target.spm è©³ç´°åˆ†æ ===");
        
        using var loggerFactory = LoggerFactory.Create(builder =>
            builder.AddConsole()
                   .SetMinimumLevel(LogLevel.Debug));
        
        var logger = loggerFactory.CreateLogger<OpusMtNativeTokenizer>();
        var modelPath = @"E:\dev\Baketa\Models\HuggingFace\opus-mt-ja-en\target.spm";
        
        try
        {
            output.WriteLine($"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {modelPath}");
            
            var fileInfo = new FileInfo(modelPath);
            output.WriteLine($"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {fileInfo.Length:N0} bytes");
            output.WriteLine($"æœ€çµ‚æ›´æ–°: {fileInfo.LastWriteTime}");
            
            // SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
            var tokenizer = new OpusMtNativeTokenizer(modelPath, "HuggingFace Target", logger);
            
            output.WriteLine($"âœ… å®Ÿéš›ã®èªå½™ã‚µã‚¤ã‚º: {tokenizer.VocabularySize:N0}");
            
            // ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDç¢ºèª
            var bosId = tokenizer.GetSpecialTokenId("BOS");
            var eosId = tokenizer.GetSpecialTokenId("EOS");
            var unkId = tokenizer.GetSpecialTokenId("UNK");
            var padId = tokenizer.GetSpecialTokenId("PAD");
            
            output.WriteLine($"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ - BOS: {bosId}, EOS: {eosId}, UNK: {unkId}, PAD: {padId}");
            
            // ONNXãƒ¢ãƒ‡ãƒ«ã¨ã®ä¸€è‡´åº¦ãƒã‚§ãƒƒã‚¯
            const int expectedVocabSize = 60716;
            var difference = Math.Abs(tokenizer.VocabularySize - expectedVocabSize);
            var matchPercentage = (1.0 - (double)difference / expectedVocabSize) * 100;
            
            output.WriteLine($"æœŸå¾…èªå½™ã‚µã‚¤ã‚º: {expectedVocabSize:N0}");
            output.WriteLine($"å®Ÿéš›èªå½™ã‚µã‚¤ã‚º: {tokenizer.VocabularySize:N0}");
            output.WriteLine($"å·®åˆ†: {difference:N0}");
            output.WriteLine($"ONNXä¸€è‡´åº¦: {matchPercentage:F2}%");
            
            if (tokenizer.VocabularySize == expectedVocabSize)
            {
                output.WriteLine("ğŸ¯ PERFECT MATCH! ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒæ­£è§£ã§ã™");
            }
            else if (difference < 1000)
            {
                output.WriteLine("âœ… éå¸¸ã«è¿‘ã„ - ä½¿ç”¨å¯èƒ½ãªå€™è£œ");
            }
            else
            {
                output.WriteLine($"âŒ å¤§ããªå·® - ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯èªå½™ã‚µã‚¤ã‚ºä¸ä¸€è‡´ãŒç™ºç”Ÿ");
            }
            
            // ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            var testTexts = new[] { "ãƒ†ã‚¹ãƒˆ", "Hello", "ã“ã‚“ã«ã¡ã¯", "ç¿»è¨³" };
            
            foreach (var testText in testTexts)
            {
                var tokens = tokenizer.Tokenize(testText);
                var maxTokenId = tokens.Length > 0 ? tokens.Max() : 0;
                
                output.WriteLine($"ãƒ†ã‚¹ãƒˆ '{testText}' -> [{string.Join(", ", tokens)}] (æœ€å¤§ID: {maxTokenId})");
                
                if (tokens.Any(t => t >= tokenizer.VocabularySize))
                {
                    output.WriteLine($"âš ï¸ è­¦å‘Š: èªå½™ç¯„å›²å¤–ã®ãƒˆãƒ¼ã‚¯ãƒ³IDæ¤œå‡º (èªå½™ã‚µã‚¤ã‚º: {tokenizer.VocabularySize})");
                }
            }
            
            // ãƒ•ã‚¡ã‚¤ãƒ«ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚µã‚¤ã‚ºã‹ãƒã‚§ãƒƒã‚¯
            output.WriteLine($"\n=== ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æ ===");
            output.WriteLine($"ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {fileInfo.Length:N0} bytes");
            
            // HuggingFaceå…¬å¼ã®target.spmã¯802KBã¨å ±å‘Šã•ã‚Œã¦ã„ã‚‹
            const long expectedFileSize = 802 * 1024; // 802KB
            var fileSizeDiff = Math.Abs(fileInfo.Length - expectedFileSize);
            var fileSizeMatch = fileSizeDiff < (expectedFileSize * 0.1); // 10%ä»¥å†…ãªã‚‰ä¸€è‡´ã¨ã¿ãªã™
            
            output.WriteLine($"æœŸå¾…ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {expectedFileSize:N0} bytes (802KB)");
            output.WriteLine($"ã‚µã‚¤ã‚ºå·®åˆ†: {fileSizeDiff:N0} bytes");
            output.WriteLine($"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºä¸€è‡´: {(fileSizeMatch ? "âœ…" : "âŒ")}");
            
            if (!fileSizeMatch)
            {
                output.WriteLine("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒæœŸå¾…å€¤ã¨å¤§ããç•°ãªã‚Šã¾ã™ã€‚å¤ã„ã¾ãŸã¯ä¸å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚");
            }
            
            // çµè«–
            output.WriteLine($"\n=== åˆ†æçµè«– ===");
            if (tokenizer.VocabularySize == expectedVocabSize)
            {
                output.WriteLine("âœ… ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ­£ã—ã„èªå½™ã‚µã‚¤ã‚ºã‚’æŒã¡ã¾ã™");
            }
            else
            {
                output.WriteLine("âŒ ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æœŸå¾…ã•ã‚Œã‚‹èªå½™ã‚µã‚¤ã‚ºã¨ä¸€è‡´ã—ã¾ã›ã‚“");
                output.WriteLine("ğŸ’¡ æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™");
            }

        }
        catch (Exception ex)
        {
            output.WriteLine($"âŒ ã‚¨ãƒ©ãƒ¼: {ex.Message}");
            if (ex.InnerException != null)
            {
                output.WriteLine($"   è©³ç´°: {ex.InnerException.Message}");
            }
            throw;
        }
    }
}
