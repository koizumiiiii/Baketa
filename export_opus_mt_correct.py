#!/usr/bin/env python3
"""
Helsinki-NLP/opus-mt-ja-enãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ã„èªå½™å¯¾å¿œã§ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Geminiã®æ¨å¥¨ã«åŸºã¥ãå®Ÿè£…
"""

import os
import sys
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from optimum.onnxruntime import ORTModelForSeq2SeqLM

def export_opus_mt_correct():
    """
    Helsinki-NLP/opus-mt-ja-enã‚’æ­£ã—ã„èªå½™å¯¾å¿œã§ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    """
    # è¨­å®š
    model_id = "Helsinki-NLP/opus-mt-ja-en"
    base_dir = Path(__file__).parent  # E:\dev\Baketa
    output_dir = base_dir / "Models" / "ONNX_Corrected"
    
    print(f"=== OPUS-MT æ­£ã—ã„èªå½™å¯¾å¿œ ONNX Export ===")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model_id}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    
    try:
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\n1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}")
        print(f"   ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³:")
        print(f"     - BOS: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
        print(f"     - EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        print(f"     - UNK: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
        print(f"     - PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
        
        print(f"\n2. PyTorchãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
        # from_transformers=True ã§PyTorchãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ONNXå¤‰æ›
        model = ORTModelForSeq2SeqLM.from_pretrained(model_id, from_transformers=True)
        
        print(f"\n3. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜ä¸­...")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†!")
        print(f"ä¿å­˜å…ˆ: {output_dir}")
        print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        for item in sorted(output_dir.iterdir()):
            if item.is_file():
                size_mb = item.stat().st_size / (1024 * 1024)
                print(f"   - {item.name} ({size_mb:.1f} MB)")
        
        # é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        important_files = [
            "encoder_model.onnx",
            "decoder_model.onnx", 
            "tokenizer.json",
            "vocab.json"
        ]
        
        print(f"\nğŸ” é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:")
        for filename in important_files:
            filepath = output_dir / filename
            exists = filepath.exists()
            status = "âœ…" if exists else "âŒ"
            print(f"   {status} {filename}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ§ª ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ:")
        test_text = "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼"
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)
        print(f"   - å…¥åŠ›: '{test_text}'")
        print(f"   - ãƒˆãƒ¼ã‚¯ãƒ³ID: {tokens}")
        print(f"   - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: {max(tokens) if tokens else 0}")
        print(f"   - ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: '{decoded}'")
        
        # èªå½™ã‚µã‚¤ã‚ºã¨æ•´åˆæ€§ç¢ºèª
        vocab_size = tokenizer.vocab_size
        max_token_id = max(tokens) if tokens else 0
        print(f"\nğŸ“Š èªå½™æ•´åˆæ€§:")
        print(f"   - èªå½™ã‚µã‚¤ã‚º: {vocab_size:,}")
        print(f"   - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: {max_token_id}")
        print(f"   - æ•´åˆæ€§: {'âœ… OK' if max_token_id < vocab_size else 'âŒ NG'}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªä¸­...")
    
    try:
        import transformers
        import optimum
        import sentencepiece
        print(f"âœ… transformers: {transformers.__version__}")
        print(f"âœ… optimum: {optimum.__version__}")
        print(f"âœ… sentencepiece: åˆ©ç”¨å¯èƒ½")
    except ImportError as e:
        print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³: {e}")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("  pip install transformers optimum[onnxruntime] sentencepiece torch")
        sys.exit(1)
    
    print()
    success = export_opus_mt_correct()
    
    if success:
        print("\nğŸ‰ OPUS-MTæ­£ã—ã„èªå½™å¯¾å¿œã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ!")
        print("\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. Models/ONNX_Corrected/tokenizer.json ã‚’ç¢ºèª")
        print("2. Models/ONNX_Corrected/vocab.json ã‚’ç¢ºèª")
        print("3. C#å®Ÿè£…ã§ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ©ç”¨")
        print("4. èªå½™ã‚µã‚¤ã‚ºä¸æ•´åˆå•é¡Œã®è§£æ±ºç¢ºèª")
    else:
        print("\nğŸ’¥ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)