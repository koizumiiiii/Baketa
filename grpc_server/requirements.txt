# Baketa gRPC Translation Server Dependencies
# Phase 2.2: Python gRPCã‚µãƒ¼ãƒãƒ¼å®Ÿè£…

# gRPC (æœ€æ–°ç‰ˆ)
# protobuf 5.xç³»ã«å›ºå®šï¼ˆGoogle AI, grpcio-status, open-clip-torchã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
grpcio>=1.60.0
# ðŸ”¥ [ISSUE#164 PROTOBUF_FIX] grpcio-tools 1.69.xã«åˆ¶é™ - Protobuf 5.xäº’æ›ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®ãŸã‚
# 1.70ä»¥é™ã¯Protobuf 6.xã§ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã—ã¦ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
grpcio-tools>=1.60.0,<1.70.0
protobuf>=5.26.1,<6.0

# NLLB-200 Translation Model (æ—¢å­˜requirements.txtãƒ™ãƒ¼ã‚¹)
transformers>=4.30.0
# ðŸ”¥ [PACKAGE_SIZE_FIX] torchå‰Šé™¤ï¼ˆç´„200MBå‰Šæ¸›ï¼‰
# GPUæ¤œå‡ºã¯ctranslate2.get_device_count("cuda")ã‚’ä½¿ç”¨
# torch>=2.0.0
sentencepiece>=0.1.99

# Data Processing
numpy>=1.24.0

# System Monitoring
psutil>=5.9.0

# ðŸ”¥ [PHASE1.1] GPU/VRAMç›£è¦–ç”¨ï¼ˆNVIDIA GPUç’°å¢ƒï¼‰
pynvml>=11.5.0

# Optional: GPU support (uncomment if CUDA is available)
# torch[cuda]>=2.0.0

# Phase 2.2.1: CTranslate2 for optimized inference (80% memory reduction: 2.4GB -> 500MB)
ctranslate2>=3.20.0

# ðŸ”¥ [ALPHA_0.1.2] HuggingFace Hub integration for automatic model download
huggingface_hub>=0.20.0

# Development & Testing
pytest>=7.0.0
pytest-asyncio>=0.23.0
