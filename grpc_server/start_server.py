# ğŸ”¥ [ULTRATHINK_FIX] Phase 6: å…¨warningså®Œå…¨æŠ‘åˆ¶ï¼ˆimportæ–‡ã‚ˆã‚Šå‰ã«å®Ÿè¡Œï¼‰
# å•é¡Œ: torch.cuda/transformersã®è­¦å‘ŠãŒstderrã§ãƒãƒ³ã‚° â†’ C#ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•å¤±æ•—
# è§£æ±ºç­–: importå‰ã«warningsæŠ‘åˆ¶ + ç’°å¢ƒå¤‰æ•°PYTHONWARNINGSè¨­å®š
import warnings
warnings.filterwarnings('ignore')

# ğŸ”¥ [ULTRATHINK_FIX] Phase 6: Pythonçµ„ã¿è¾¼ã¿warningsç’°å¢ƒå¤‰æ•°è¨­å®š
import os
os.environ["PYTHONWARNINGS"] = "ignore"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # TensorFlowè­¦å‘ŠæŠ‘åˆ¶
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # HuggingFaceä¸¦åˆ—åŒ–ç„¡åŠ¹

# ğŸ”¥ [Issue #198] CUDA DLLãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼ˆPhase 3: miniconda PATHé™¤å¤–ï¼‰
# å•é¡Œ: minicondaã®CUDA DLL (cublas64_12.dll) ãŒPATHã«ã‚ã‚‹ã¨torchã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«OSErrorãŒç™ºç”Ÿ
# CUDA_VISIBLE_DEVICES="" ã ã‘ã§ã¯ä¸ååˆ† - torchã¯PATHä¸Šã®DLLã‚’ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã™ã‚‹
# è§£æ±ºç­–: minicondaã®ãƒ‘ã‚¹ã‚’PATHç’°å¢ƒå¤‰æ•°ã‹ã‚‰é™¤å¤–ã—ã¦ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys

def _sanitize_path_for_cuda():
    """minicondaã®CUDA DLLãƒ‘ã‚¹ã‚’PATHã‹ã‚‰é™¤å¤–"""
    path = os.environ.get("PATH", "")
    path_parts = path.split(os.pathsep)

    # miniconda/anacondaã®ãƒ‘ã‚¹ã‚’é™¤å¤–ï¼ˆCUDA DLLç«¶åˆã‚’é˜²æ­¢ï¼‰
    sanitized_parts = []
    excluded_parts = []
    for part in path_parts:
        part_lower = part.lower()
        if "miniconda" in part_lower or "anaconda" in part_lower:
            excluded_parts.append(part)
        else:
            sanitized_parts.append(part)

    if excluded_parts:
        print(f"[INFO] CUDA DLLç«¶åˆé˜²æ­¢: PATH ã‹ã‚‰ä»¥ä¸‹ã‚’é™¤å¤–ã—ã¾ã—ãŸ:", file=sys.stderr)
        for p in excluded_parts:
            print(f"  - {p}", file=sys.stderr)
        os.environ["PATH"] = os.pathsep.join(sanitized_parts)
        return True
    return False

def _check_cuda_availability():
    """CTranslate2ã®CUDAã‚µãƒãƒ¼ãƒˆã‚’ç›´æ¥ãƒã‚§ãƒƒã‚¯ï¼ˆDLLãƒ­ãƒ¼ãƒ‰ã‚ˆã‚Šä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰"""
    try:
        import ctranslate2
        cuda_types = ctranslate2.get_supported_compute_types("cuda")
        if cuda_types:
            print(f"[INFO] CTranslate2 CUDAå¯¾å¿œ: {cuda_types}", file=sys.stderr)
            return True
    except Exception as e:
        print(f"[INFO] CTranslate2 CUDAãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
    return False

# Phase 3: minicondaãƒ‘ã‚¹ã‚’é™¤å¤–ã—ã¦CUDA DLLç«¶åˆã‚’é˜²æ­¢
_path_sanitized = _sanitize_path_for_cuda()

# ctranslate2ã‚’å…ˆã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import ctranslate2

# CTranslate2ã®CUDAã‚µãƒãƒ¼ãƒˆã‚’ç›´æ¥ãƒã‚§ãƒƒã‚¯
_cuda_available = _check_cuda_availability()
if not _cuda_available:
    # CUDAãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã®ã¿ç’°å¢ƒå¤‰æ•°ã§ç„¡åŠ¹åŒ–
    print("[INFO] CUDAåˆ©ç”¨ä¸å¯: CPUãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã—ã¾ã™", file=sys.stderr)
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    os.environ["CUDA_HOME"] = ""

"""
gRPC Translation Server Startup Script
Phase 2.2: ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

Usage:
    python start_server.py [--port PORT] [--host HOST] [--heavy-model]

Examples:
    python start_server.py
    python start_server.py --port 50051 --host localhost
    python start_server.py --heavy-model  # Use 1.3B model instead of 600M
"""

import asyncio
import argparse
import logging
import signal
import sys
import faulthandler  # ğŸ”¥ [PHASE1.3] Windowså›ºæœ‰ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ¤œå‡ºç”¨
import traceback  # ğŸ”¥ [PHASE1.3] ä¾‹å¤–ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹å‡ºåŠ›ç”¨
from pathlib import Path

# ğŸ”¥ [HOTFIX alpha-0.1.12] Python Embeddableç‰ˆã§ã®protosãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¿®æ­£
# Root cause: python310._pthã®"."ã¯vendor/python/ã‚’åŸºæº–ã¨ã™ã‚‹ãŸã‚ã€
#             WorkingDirectory=grpc_serverã§ã‚‚ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒsys.pathã«å«ã¾ã‚Œãªã„
# Fix: æ˜ç¤ºçš„ã«ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«è¿½åŠ 
if os.getcwd() not in sys.path:
    sys.path.insert(0, os.getcwd())

import grpc
from grpc import aio
# ğŸ”¥ [PACKAGE_SIZE_FIX] torchå‰Šé™¤ï¼ˆç´„200MBå‰Šæ¸›ï¼‰
# GPUæ¤œå‡ºã¯ctranslate2.get_device_count()ã‚’ä½¿ç”¨
import ctranslate2

# Protoç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã«ãªã‚Šã¾ã™ï¼‰
from protos import translation_pb2_grpc

from translation_server import TranslationServicer
from engines.ctranslate2_engine import CTranslate2Engine
from resource_monitor import ResourceMonitor  # Phase 1.1: GPU/VRAMç›£è¦–

# ğŸ”§ [UNICODE_FIX] Windowsç’°å¢ƒã§ã®UnicodeEncodeErrorå¯¾ç­–
# sys.stdout/stderrã‚’UTF-8ã«å†è¨­å®šï¼ˆcp932 â†’ utf-8ï¼‰
# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ­ã‚°å‡ºåŠ›æ™‚ã®Unicodeã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢
# ğŸ”¥ [PYINSTALLER_FIX] ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãªã—ãƒ¢ãƒ¼ãƒ‰ã§ã¯stdout/stderrãŒNoneã®å ´åˆãŒã‚ã‚‹
try:
    if sys.stdout is not None and hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    if sys.stderr is not None and hasattr(sys.stderr, 'reconfigure'):
        sys.stderr.reconfigure(encoding='utf-8')
except (AttributeError, OSError):
    # PyInstaller --noconsole ãƒ¢ãƒ¼ãƒ‰ã§ã¯stdout/stderrãŒç„¡åŠ¹
    pass

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('translation_server.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class GracefulShutdown:
    """ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self):
        self.shutdown_event = asyncio.Event()

    def __enter__(self):
        loop = asyncio.get_event_loop()

        def signal_handler(signum, frame):
            logger.info(f"Received signal {signum}, shutting down gracefully...")
            asyncio.create_task(self._async_shutdown())

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        return self

    def __exit__(self, exc_type, exc_value, traceback):
        pass

    async def _async_shutdown(self):
        self.shutdown_event.set()

    async def wait_for_shutdown(self):
        await self.shutdown_event.wait()


async def serve(host: str, port: int, model_path_arg: str | None = None):
    """gRPCã‚µãƒ¼ãƒãƒ¼èµ·å‹•

    Args:
        host: ãƒã‚¤ãƒ³ãƒ‰ãƒ›ã‚¹ãƒˆï¼ˆä¾‹: "localhost", "0.0.0.0"ï¼‰
        port: ãƒãƒ¼ãƒˆç•ªå·ï¼ˆä¾‹: 50051ï¼‰
        model_path_arg: [Issue #185] C#ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

    Note:
        NLLB-200-distilled-1.3B (CTranslate2 int8) ã‚’ä½¿ç”¨ã€‚
        600Mã‹ã‚‰ã®ç²¾åº¦å‘ä¸Šã«ã‚ˆã‚Šã€æ—¥æœ¬èªç¿»è¨³å“è³ªãŒå¤§å¹…ã«æ”¹å–„ã€‚
    """
    logger.info("=" * 80)
    logger.info("Baketa gRPC Translation Server Starting...")
    logger.info("=" * 80)

    # CTranslate2ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆNLLB-200-distilled-1.3Bï¼‰ã‚’ä½¿ç”¨
    logger.info("Initializing CTranslate2 translation engine...")

    # [Issue #185] ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ±ºå®š
    # å„ªå…ˆé †ä½: 1. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•° 2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆ%APPDATA%\Baketa\Models\nllb-200-1.3B-ct2ï¼‰
    if model_path_arg:
        model_path = Path(model_path_arg)
        logger.info(f"[Issue #185] Using model path from command line: {model_path}")
    else:
        # ğŸ”¥ [ALPHA_0.1.2] HuggingFace Hubçµ±åˆ: ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆã‚’%APPDATA%\Baketa\Modelsã«å¤‰æ›´
        # Geminiæ¨å¥¨: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å…ˆã¸ã®æ›¸ãè¾¼ã¿ã¯ç®¡ç†è€…æ¨©é™ãŒå¿…è¦ãªãŸã‚ã€APPDATAã‚’ä½¿ç”¨
        # ğŸš€ [Translation Quality] NLLB-200-distilled-1.3B ã«ç§»è¡Œï¼ˆ600Mã‹ã‚‰ç²¾åº¦å‘ä¸Šï¼‰
        appdata = os.environ.get('APPDATA', os.path.expanduser('~'))
        model_path = Path(appdata) / "Baketa" / "Models" / "nllb-200-1.3B-ct2"
        logger.info(f"[Issue #185] Using default model path: {model_path}")

    # ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ãƒ»è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if not model_path.exists() or not (model_path / "model.bin").exists():
        logger.info("=" * 80)
        logger.info("Model not found. Downloading from HuggingFace Hub...")
        logger.info("Repository: OpenNMT/nllb-200-distilled-1.3B-ct2-int8")
        logger.info("Size: ~1.3GB | This may take several minutes...")
        logger.info("=" * 80)
        model_path.mkdir(parents=True, exist_ok=True)

        try:
            # ğŸ”¥ [GEMINI_RECOMMENDATION] éåŒæœŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å›é¿ï¼‰
            from huggingface_hub import snapshot_download
            from functools import partial

            loop = asyncio.get_running_loop()
            download_func = partial(
                snapshot_download,
                repo_id="OpenNMT/nllb-200-distilled-1.3B-ct2-int8",
                local_dir=str(model_path),
                revision="main"  # TODO: ç‰¹å®šã®ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã«å›ºå®šï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å‘ä¸Šï¼‰
            )
            await loop.run_in_executor(None, download_func)
            logger.info("=" * 80)
            logger.info("Model download completed successfully.")
            logger.info("=" * 80)
        except Exception as e:
            logger.error("=" * 80)
            logger.error(f"Model download failed: {e}")
            logger.error("Please check:")
            logger.error("  1. Internet connection is available")
            logger.error("  2. Disk space is sufficient (~1.3GB)")
            logger.error("  3. HuggingFace Hub is accessible")
            logger.error("=" * 80)
            raise RuntimeError(f"Failed to download model from HuggingFace Hub: {e}")
    else:
        logger.info("Model found locally. Skipping download.")

    # ğŸ”¥ [PACKAGE_SIZE_FIX] GPUæ¤œå‡ºã‚’pynvmlï¼ˆæ—¢å­˜ä¾å­˜ï¼‰ã§å®Ÿè¡Œï¼ˆtorchä¸è¦ï¼‰
    # ğŸ”¥ [HOTFIX Issue #170] ctranslate2.get_device_count()ã¯å­˜åœ¨ã—ãªã„ãŸã‚ã€pynvmlã‚’ä½¿ç”¨
    # Root cause: ctranslate2 4.6.0ã«get_device_count()ãŒå­˜åœ¨ã—ãªã„
    # Fix: pynvmlï¼ˆæ—¢ã«requirements.txtã«å«ã¾ã‚Œã¦ã„ã‚‹ï¼‰ã§CUDAæ¤œå‡º
    # ğŸ”¥ [GEMINI_REVIEW] finallyå¥ã§pynvml.nvmlShutdown()ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
    is_cuda_available = False
    nvml_initialized = False
    try:
        import pynvml
        pynvml.nvmlInit()
        nvml_initialized = True
        device_count = pynvml.nvmlDeviceGetCount()
        is_cuda_available = device_count > 0
        if is_cuda_available:
            # GPUæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            gpu_name = pynvml.nvmlDeviceGetName(handle)
            # byteså‹ã®å ´åˆã¯UTF-8ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆç’°å¢ƒã«ã‚ˆã£ã¦bytesã‚’è¿”ã™å ´åˆãŒã‚ã‚‹ï¼‰
            if isinstance(gpu_name, bytes):
                gpu_name = gpu_name.decode('utf-8')
            logger.info(f"ğŸ® GPU detection successful: {device_count} CUDA device(s) found")
            logger.info(f"   Primary GPU: {gpu_name}")
    except Exception as e:
        # Fallback: GPUæ¤œå‡ºå¤±æ•—æ™‚ã¯CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œ
        logger.warning(f"âš ï¸ GPU detection failed ({e.__class__.__name__}), falling back to CPU mode")
        is_cuda_available = False
    finally:
        # ğŸ”¥ [GEMINI_REVIEW] nvmlInit()ãŒæˆåŠŸã—ãŸå ´åˆã®ã¿Shutdown()ã‚’å®Ÿè¡Œ
        if nvml_initialized:
            pynvml.nvmlShutdown()

    engine = CTranslate2Engine(
        model_path=str(model_path),  # %APPDATA%\Baketa\Models\nllb-200-1.3B-ct2
        device="cuda" if is_cuda_available else "cpu",
        compute_type="int8"
    )

    logger.info("Loading NLLB model (this may take a few minutes)...")
    await engine.load_model()
    logger.info("NLLB model loaded successfully")

    # gRPCã‚µãƒ¼ãƒãƒ¼ä½œæˆ
    logger.info("Creating gRPC server...")
    # ğŸ”§ [GEMINI_DEEP_FIX] å®Œå…¨ãªKeepAliveè¨­å®š - ä¸­é–“æ©Ÿå™¨ã«ã‚ˆã‚‹åˆ‡æ–­ã‚’é˜²æ­¢
    # æ ¹æœ¬åŸå› : ä¸­é–“æ©Ÿå™¨ï¼ˆãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã€NATç­‰ï¼‰ãŒ112ç§’ã‚¢ã‚¤ãƒ‰ãƒ«ã§TCPåˆ‡æ–­
    # è§£æ±ºç­–: ã‚µãƒ¼ãƒãƒ¼å´ã‹ã‚‰30ç§’é–“éš”ã§PINGã‚’é€ä¿¡ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®PINGã‚‚è¨±å¯
    server = aio.server(options=[
        # â˜…â˜…â˜… ã‚µãƒ¼ãƒãƒ¼å´ã®KeepAliveè¨­å®š â˜…â˜…â˜…
        ('grpc.keepalive_time_ms', 30000),  # 30ç§’ã”ã¨ã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç”Ÿå­˜ç¢ºèªPINGã‚’é€ä¿¡
        ('grpc.keepalive_timeout_ms', 10000),  # PINGã®å¿œç­”å¾…ã¡æ™‚é–“
        ('grpc.keepalive_permit_without_calls', True),  # RPCãŒãªãã¦ã‚‚PINGã‚’è¨±å¯ï¼ˆã‚¢ã‚¤ãƒ‰ãƒ«ä¸­ã‚‚æ¥ç¶šç¶­æŒï¼‰
        ('grpc.http2.min_time_between_pings_ms', 10000),  # PINGã®æœ€ä½é–“éš”
        ('grpc.http2.max_pings_without_data', 0),  # ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã®PINGå›æ•°åˆ¶é™ã‚’ç„¡åŠ¹åŒ–
        ('grpc.http2.min_ping_interval_without_data_ms', 10000),  # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®PINGæœ€ä½é–“éš”
    ])

    # TranslationServiceã‚’ç™»éŒ²
    servicer = TranslationServicer(engine)
    translation_pb2_grpc.add_TranslationServiceServicer_to_server(servicer, server)

    # ãƒªã‚¹ãƒ‹ãƒ³ã‚°ã‚¢ãƒ‰ãƒ¬ã‚¹è¨­å®š
    listen_addr = f'{host}:{port}'
    server.add_insecure_port(listen_addr)

    logger.info(f"Starting gRPC server on {listen_addr}...")
    await server.start()

    logger.info("=" * 80)
    logger.info(f"gRPC Translation Server is running on {listen_addr}")
    logger.info(f"   Engine: {engine.__class__.__name__}")
    logger.info(f"   Model: {engine.model_name}")
    logger.info(f"   Device: {engine.device}")

    # ğŸ”¥ [PHASE8_FIX] PythonServerManager.WaitForServerReadyAsync()äº’æ›æ€§ã®ãŸã‚[SERVER_START]å‡ºåŠ›
    # C#å´ãŒStdErrã‚’ç›£è¦–ã—ã¦ã„ã‚‹ãŸã‚ã€sys.stderrã«ç›´æ¥å‡ºåŠ›
    # ğŸ”¥ [PYINSTALLER_FIX] ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãªã—ãƒ¢ãƒ¼ãƒ‰ã§ã¯stderrãŒNoneã¾ãŸã¯ç„¡åŠ¹
    try:
        if sys.stderr is not None:
            sys.stderr.write("[SERVER_START]\n")
            sys.stderr.flush()  # å³åº§ã«å‡ºåŠ›
            logger.info("[SERVER_START] signal sent to stderr for C# detection")
        else:
            logger.info("[SERVER_START] signal skipped (stderr unavailable)")
    except (OSError, AttributeError):
        # PyInstaller --noconsole ãƒ¢ãƒ¼ãƒ‰ã§ã¯æ›¸ãè¾¼ã¿ãŒã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
        logger.info("[SERVER_START] signal skipped (stderr write failed)")
    logger.info(f"   Supported languages: {', '.join(engine.get_supported_languages())}")
    logger.info("=" * 80)
    logger.info("Press Ctrl+C to stop the server")

    # ğŸ”¥ [PHASE1.1] GPU/VRAMãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹ï¼ˆGeminiæœ€é‡è¦æ¨å¥¨ï¼‰
    resource_monitor = ResourceMonitor(enable_gpu_monitoring=True)
    await resource_monitor.start_monitoring(interval_seconds=300)  # 5åˆ†ã”ã¨
    logger.info("[PHASE1.1] Resource monitoring started (CPU RAM + GPU VRAM + Handles)")

    # ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å¾…æ©Ÿ
    with GracefulShutdown() as shutdown_handler:
        try:
            await shutdown_handler.wait_for_shutdown()
        except KeyboardInterrupt:
            logger.info("Received KeyboardInterrupt, shutting down...")

    # ã‚µãƒ¼ãƒãƒ¼åœæ­¢
    logger.info("Stopping gRPC server...")
    await server.stop(grace=5.0)  # 5ç§’ã®ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ”ãƒªã‚ªãƒ‰
    logger.info("gRPC server stopped")

    # ğŸ”¥ [PHASE1.1] ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    await resource_monitor.stop_monitoring()
    resource_monitor.cleanup()
    logger.info("[PHASE1.1] Resource monitoring cleanup completed")


def global_exception_handler(exc_type, exc_value, exc_traceback):
    """ğŸ”¥ [PHASE1.3] ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ - ã™ã¹ã¦ã®æœªå‡¦ç†ä¾‹å¤–ã‚’ãƒ­ã‚°å‡ºåŠ›"""
    if issubclass(exc_type, KeyboardInterrupt):
        # KeyboardInterruptã¯é€šå¸¸å‡¦ç†
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return

    logger.critical("=" * 80)
    logger.critical("ğŸš¨ [PHASE1.3] UNCAUGHT EXCEPTION - CRITICAL ERROR")
    logger.critical("=" * 80)
    logger.critical(f"Exception Type: {exc_type.__name__}")
    logger.critical(f"Exception Value: {exc_value}")
    logger.critical("Traceback:")
    logger.critical("".join(traceback.format_exception(exc_type, exc_value, exc_traceback)))
    logger.critical("=" * 80)


def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚¹ & ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
    # ğŸ”¥ [PHASE1.3] faulthandleræœ‰åŠ¹åŒ– - SIGSEGVç­‰ã®OS-levelã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ¤œå‡º
    faulthandler.enable(file=sys.stderr, all_threads=True)
    logger.info("[PHASE1.3] faulthandler enabled - OS-level crash detection active")

    # ğŸ”¥ [PHASE1.3] ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­ç½®
    sys.excepthook = global_exception_handler
    logger.info("[PHASE1.3] Global exception handler installed")

    parser = argparse.ArgumentParser(
        description="Baketa gRPC Translation Server"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=50051,
        help="gRPC server port (default: 50051)"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="gRPC server host (default: 127.0.0.1 for localhost only, use 0.0.0.0 for all interfaces)"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )
    # [Issue #185] ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’C#ã‹ã‚‰æŒ‡å®šå¯èƒ½ã«
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="Path to CTranslate2 model directory (default: %APPDATA%/Baketa/Models/nllb-200-1.3B-ct2)"
    )

    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.setLevel(logging.DEBUG)

    # è¨­å®šæƒ…å ±è¡¨ç¤º
    logger.info("Server configuration:")
    logger.info(f"  Host: {args.host}")
    logger.info(f"  Port: {args.port}")
    logger.info(f"  Model: NLLB-200-distilled-1.3B (CTranslate2 int8)")
    logger.info(f"  Model path: {args.model_path or '(default)'}")
    logger.info(f"  Debug mode: {args.debug}")

    # asyncioã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã§ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    try:
        asyncio.run(
            serve(
                host=args.host,
                port=args.port,
                model_path_arg=args.model_path  # [Issue #185] ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹å¼•æ•°ã‚’æ¸¡ã™
            )
        )
    except KeyboardInterrupt:
        logger.info("Server interrupted by user")
    except Exception as e:
        logger.exception(f"Server error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
