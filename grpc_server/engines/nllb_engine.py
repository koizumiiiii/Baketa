"""
NLLB-200 Translation Engine
Phase 2.2: NLLB-200ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…
"""

import os
import sys
import time
import logging
from typing import List, Tuple, Optional
import asyncio

# Issue #198: CUDA DLLãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼å¯¾ç­–
# ã‚°ãƒ­ãƒ¼ãƒãƒ«importå‰ã«CUDAåˆ©ç”¨å¯å¦ã‚’å®‰å…¨ã«ãƒã‚§ãƒƒã‚¯
def _safe_import_torch():
    """CUDA DLLã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã—ã¦torchã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    try:
        import torch
        return torch
    except OSError as e:
        # CUDA DLLãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ï¼ˆcublas64_12.dllç­‰ï¼‰
        logger = logging.getLogger(__name__)
        logger.warning(f"CUDA DLLãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        logger.info("CUDAç„¡åŠ¹åŒ–ã—ã¦torchã‚’å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™")
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        import torch
        return torch

torch = _safe_import_torch()
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

from .base import (
    TranslationEngine,
    ModelNotLoadedError,
    UnsupportedLanguageError,
    TextTooLongError,
    ModelInferenceError,
    BatchSizeExceededError
)

logger = logging.getLogger(__name__)


class NllbEngine(TranslationEngine):
    """NLLB-200ãƒ™ãƒ¼ã‚¹ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³

    Meta NLLB-200ãƒ¢ãƒ‡ãƒ« (facebook/nllb-200-distilled-600M) ã‚’ä½¿ç”¨
    GPUæœ€é©åŒ–ã€ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ
    """

    # è¨€èªãƒãƒƒãƒ”ãƒ³ã‚°: ISO 639-1 â†’ NLLB-200 BCP-47ã‚³ãƒ¼ãƒ‰
    LANGUAGE_MAPPING = {
        "en": "eng_Latn",
        "ja": "jpn_Jpan",
        "zh": "zho_Hans",  # ç°¡ä½“å­—ä¸­å›½èª
        "zh-cn": "zho_Hans",
        "zh-tw": "zho_Hant",  # ç¹ä½“å­—ä¸­å›½èª
        "ko": "kor_Hang",  # éŸ“å›½èª
        "es": "spa_Latn",  # ã‚¹ãƒšã‚¤ãƒ³èª
        "fr": "fra_Latn",  # ãƒ•ãƒ©ãƒ³ã‚¹èª
        "de": "deu_Latn",  # ãƒ‰ã‚¤ãƒ„èª
        "ru": "rus_Cyrl",  # ãƒ­ã‚·ã‚¢èª
        "ar": "arb_Arab",  # ã‚¢ãƒ©ãƒ“ã‚¢èª
    }

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    LIGHTWEIGHT_MODEL = "facebook/nllb-200-distilled-600M"  # ç´„2.4GB
    HEAVY_MODEL = "facebook/nllb-200-distilled-1.3B"        # ç´„5GB

    # ãƒãƒƒãƒå‡¦ç†è¨­å®š
    MAX_BATCH_SIZE = 32
    MAX_TEXT_LENGTH = 512  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    def __init__(self, use_heavy_model: bool = False):
        super().__init__()
        self.model_name = self.HEAVY_MODEL if use_heavy_model else self.LIGHTWEIGHT_MODEL
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"Using device: {self.device}")

    async def load_model(self) -> None:
        """NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        self.logger.info(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {self.model_name}")
        start_time = time.time()

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            self.logger.info("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.logger.info("Tokenizer loaded successfully")

            # ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ãƒ­ãƒ¼ãƒ‰
            self.logger.info("ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ãƒ­ãƒ¼ãƒ‰ä¸­ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ï¼‰...")
            try:
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16,  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åŠæ¸›
                    device_map="auto"           # è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é…ç½®
                )
                self.logger.info("Model loaded successfully (GPU)")
            except torch.cuda.OutOfMemoryError:
                self.logger.warning("GPU memory insufficient - Fallback to CPU")
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu"
                )
                self.device = "cpu"
                self.logger.info("Model loaded successfully (CPU)")

            # ãƒ‡ãƒã‚¤ã‚¹é…ç½®ï¼ˆdevice_mapã§è‡ªå‹•é…ç½®æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if not hasattr(self.model, "hf_device_map"):
                self.model = self.model.to(self.device)

            self.model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰

            load_time = time.time() - start_time
            self.logger.info(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            await self._warmup_model()

            self.is_loaded = True
            self.logger.info("NLLB-200 engine ready")

        except ImportError as e:
            self.logger.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
            raise ModelNotLoadedError(f"Required library missing: {e}")
        except OSError as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model file load failed: {e}")
        except Exception as e:
            self.logger.error(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model load failed: {e}")

    async def _warmup_model(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›æ¨è«–ã®é…å»¶å›é¿ï¼‰"""
        self.logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")

        try:
            # è‹±èªâ†’æ—¥æœ¬èª
            await self.translate("Hello", "en", "ja")
            self.logger.info("è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")

            # æ—¥æœ¬èªâ†’è‹±èª
            await self.translate("ã“ã‚“ã«ã¡ã¯", "ja", "en")
            self.logger.info("æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")

        except Exception as e:
            self.logger.warning(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—ï¼ˆç„¡è¦–ï¼‰: {e}")

    def _get_nllb_lang_code(self, lang_code: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’NLLB-200å½¢å¼ã«å¤‰æ›

        Args:
            lang_code: ISO 639-1ã‚³ãƒ¼ãƒ‰ ("en", "ja"ç­‰)

        Returns:
            NLLB-200 BCP-47ã‚³ãƒ¼ãƒ‰ ("eng_Latn", "jpn_Jpan"ç­‰)

        Raises:
            UnsupportedLanguageError: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èª
        """
        if not lang_code or not isinstance(lang_code, str):
            raise UnsupportedLanguageError(f"Invalid language code: {lang_code}")

        normalized = lang_code.lower()

        # ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        if normalized in self.LANGUAGE_MAPPING:
            return self.LANGUAGE_MAPPING[normalized]

        # ãƒãƒƒãƒ”ãƒ³ã‚°ã«ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        raise UnsupportedLanguageError(
            f"Unsupported language: {lang_code}. "
            f"Supported: {list(self.LANGUAGE_MAPPING.keys())}"
        )

    async def translate(
        self,
        text: str,
        source_lang: str,
        target_lang: str
    ) -> Tuple[str, float]:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³"""
        if not self.is_loaded or not self.model or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")

        # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼
        if not text or not isinstance(text, str):
            raise ValueError(f"Invalid text: {text}")

        if len(text.strip()) == 0:
            return ("", 0.0)

        # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
        src_code = self._get_nllb_lang_code(source_lang)
        tgt_code = self._get_nllb_lang_code(target_lang)

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.MAX_TEXT_LENGTH
            )

            # ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒã‚§ãƒƒã‚¯
            if inputs.input_ids.shape[1] > self.MAX_TEXT_LENGTH:
                raise TextTooLongError(
                    f"Text too long: {inputs.input_ids.shape[1]} tokens "
                    f"(max: {self.MAX_TEXT_LENGTH})"
                )

            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # ç¿»è¨³å®Ÿè¡Œï¼ˆasyncio.to_threadã§éåŒæœŸåŒ–ï¼‰
            def _generate():
                self.tokenizer.src_lang = src_code
                # ğŸ”¥ [FIX] NllbTokenizerFastã§ã¯ convert_tokens_to_ids() ã‚’ä½¿ç”¨
                tgt_lang_id = self.tokenizer.convert_tokens_to_ids(tgt_code)

                # ğŸ” [DEBUG] tgt_lang_idã®å€¤ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                self.logger.debug(f"tgt_code: {tgt_code}, tgt_lang_id: {tgt_lang_id}")

                with torch.no_grad():
                    generated_tokens = self.model.generate(
                        **inputs,
                        forced_bos_token_id=tgt_lang_id,
                        max_new_tokens=self.MAX_TEXT_LENGTH,
                        num_beams=5,
                        early_stopping=True
                    )
                return generated_tokens

            generated_tokens = await asyncio.to_thread(_generate)

            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            translated_text = self.tokenizer.batch_decode(
                generated_tokens,
                skip_special_tokens=True
            )[0]

            # NLLB-200ã¯ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢éå¯¾å¿œã®ãŸã‚-1.0ã‚’è¿”ã™
            return (translated_text, -1.0)

        except UnsupportedLanguageError:
            raise
        except TextTooLongError:
            raise
        except torch.cuda.OutOfMemoryError as e:
            raise ModelInferenceError(f"GPU memory insufficient: {e}")
        except Exception as e:
            raise ModelInferenceError(f"Translation failed: {e}")

    async def translate_batch(
        self,
        texts: List[str],
        source_lang: str,
        target_lang: str
    ) -> List[Tuple[str, float]]:
        """ãƒãƒƒãƒç¿»è¨³ï¼ˆGPUæœ€é©åŒ–ï¼‰"""
        if not self.is_loaded or not self.model or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if len(texts) > self.MAX_BATCH_SIZE:
            raise BatchSizeExceededError(
                f"Batch size {len(texts)} exceeds maximum {self.MAX_BATCH_SIZE}"
            )

        # ç©ºãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_texts = [t for t in texts if t and t.strip()]
        if not valid_texts:
            return [("", 0.0) for _ in texts]

        # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
        src_code = self._get_nllb_lang_code(source_lang)
        tgt_code = self._get_nllb_lang_code(target_lang)

        try:
            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(
                valid_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.MAX_TEXT_LENGTH
            )

            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œï¼ˆasyncio.to_threadã§éåŒæœŸåŒ–ï¼‰
            def _generate_batch():
                self.tokenizer.src_lang = src_code
                # ğŸ”¥ [FIX] NllbTokenizerFastã§ã¯ convert_tokens_to_ids() ã‚’ä½¿ç”¨
                tgt_lang_id = self.tokenizer.convert_tokens_to_ids(tgt_code)

                # ğŸ” [DEBUG] tgt_lang_idã®å€¤ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                self.logger.debug(f"[BATCH] tgt_code: {tgt_code}, tgt_lang_id: {tgt_lang_id}")

                with torch.no_grad():
                    generated_tokens = self.model.generate(
                        **inputs,
                        forced_bos_token_id=tgt_lang_id,
                        max_new_tokens=self.MAX_TEXT_LENGTH,
                        num_beams=5,
                        early_stopping=True
                    )
                return generated_tokens

            generated_tokens = await asyncio.to_thread(_generate_batch)

            # ãƒãƒƒãƒãƒ‡ã‚³ãƒ¼ãƒ‰
            translated_texts = self.tokenizer.batch_decode(
                generated_tokens,
                skip_special_tokens=True
            )

            # çµæœã‚’å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã¨åŒã˜é †åºã§è¿”ã™
            results = []
            valid_idx = 0
            for text in texts:
                if text and text.strip():
                    results.append((translated_texts[valid_idx], -1.0))
                    valid_idx += 1
                else:
                    results.append(("", 0.0))

            return results

        except Exception as e:
            raise ModelInferenceError(f"Batch translation failed: {e}")

    async def is_ready(self) -> bool:
        """æº–å‚™å®Œäº†ç¢ºèª"""
        return self.is_loaded and self.model is not None and self.tokenizer is not None

    def get_supported_languages(self) -> List[str]:
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹è¨€èªã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ"""
        return list(self.LANGUAGE_MAPPING.keys())
