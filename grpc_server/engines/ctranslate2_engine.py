"""
CTranslate2 Translation Engine
Phase 2.2.1: CTranslate2æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£… (NLLB-200-distilled-1.3B)

ç‰¹å¾´:
- NLLB-200-distilled-1.3B ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼ˆ600Mã‹ã‚‰ç²¾åº¦å‘ä¸Šï¼‰
- int8é‡å­åŒ–ã«ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆç´„5.5GBä½¿ç”¨ï¼‰
- 20-30%æ¨è«–é«˜é€ŸåŒ–
- å¤šè¨€èªç¿»è¨³å¯¾å¿œï¼ˆ200è¨€èªä»¥ä¸Šï¼‰

ãƒ¢ãƒ‡ãƒ«ã‚½ãƒ¼ã‚¹: OpenNMT/nllb-200-distilled-1.3B-ct2-int8

ğŸ”¥ [Issue #185] torch/transformersä¾å­˜ã‚’å‰Šé™¤
- tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆRustè£½ã€è»½é‡ï¼‰ã‚’ç›´æ¥ä½¿ç”¨
- ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚º ~450MBå‰Šæ¸›
"""

import asyncio
import time
import logging
import gc  # ğŸ”¥ [PHASE1.2] æ˜ç¤ºçš„GCå®Ÿè¡Œç”¨
from pathlib import Path
from typing import List, Tuple, Optional
from threading import Lock
from concurrent.futures import ThreadPoolExecutor

import ctranslate2
from tokenizers import Tokenizer  # ğŸ”¥ [Issue #185] transformers â†’ tokenizers (è»½é‡)

from .base import (
    TranslationEngine,
    ModelNotLoadedError,
    UnsupportedLanguageError,
    TextTooLongError,
    ModelInferenceError,
    BatchSizeExceededError
)

logger = logging.getLogger(__name__)


class CTranslate2Engine(TranslationEngine):
    """CTranslate2ãƒ™ãƒ¼ã‚¹ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³

    NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’CTranslate2ã§ãƒ­ãƒ¼ãƒ‰ã—ã€int8é‡å­åŒ–ã«ã‚ˆã‚Š
    80%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã¨20-30%é«˜é€ŸåŒ–ã‚’å®Ÿç¾
    """

    # è¨€èªãƒãƒƒãƒ”ãƒ³ã‚°: ISO 639-1 â†’ NLLB-200 BCP-47ã‚³ãƒ¼ãƒ‰
    LANGUAGE_MAPPING = {
        "en": "eng_Latn",
        "ja": "jpn_Jpan",
        "zh": "zho_Hans",  # ç°¡ä½“å­—ä¸­å›½èª
        "zh-cn": "zho_Hans",
        "zh-tw": "zho_Hant",  # ç¹ä½“å­—ä¸­å›½èª
        "ko": "kor_Hang",  # éŸ“å›½èª
        "es": "spa_Latn",  # ã‚¹ãƒšã‚¤ãƒ³èª
        "fr": "fra_Latn",  # ãƒ•ãƒ©ãƒ³ã‚¹èª
        "de": "deu_Latn",  # ãƒ‰ã‚¤ãƒ„èª
        "ru": "rus_Cyrl",  # ãƒ­ã‚·ã‚¢èª
        "ar": "arb_Arab",  # ã‚¢ãƒ©ãƒ“ã‚¢èª
    }

    # ãƒãƒƒãƒå‡¦ç†è¨­å®š
    MAX_BATCH_SIZE = 32
    MAX_TEXT_LENGTH = 512  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    def __init__(
        self,
        model_path: str = "models/nllb-200-ct2",
        device: str = "cpu",
        compute_type: str = "int8",
        max_workers: int = 4
    ):
        """
        Args:
            model_path: CTranslate2å¤‰æ›æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ï¼ˆcpu, cuda, autoï¼‰
            compute_type: è¨ˆç®—å‹ï¼ˆint8, int16, float16, float32ï¼‰
            max_workers: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        super().__init__()
        self.model_path = Path(model_path)
        self.device = device
        self.compute_type = compute_type
        self.max_workers = max_workers
        self.model_name = f"CTranslate2 ({compute_type})"

        self.translator: Optional[ctranslate2.Translator] = None
        self.tokenizer: Optional[Tokenizer] = None  # ğŸ”¥ [Issue #185] tokenizers.Tokenizer

        self.executor = ThreadPoolExecutor(max_workers=max_workers)

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¸¦åˆ—ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ï¼ˆRace Conditionå¯¾ç­–ï¼‰
        self.tokenizer_lock = Lock()

        # ğŸ”¥ [PHASE1.2] ãƒ¡ãƒ¢ãƒªç®¡ç†æœ€é©åŒ–ï¼ˆGeminiæ¨å¥¨ï¼‰
        self.translation_count = 0  # ç¿»è¨³å›æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.max_translations_before_gc = 1000  # 1000å›ã”ã¨ã«GCå®Ÿè¡Œ

        self.logger.info(f"CTranslate2 Engine initialized")
        self.logger.info(f"  Model Path: {self.model_path}")
        self.logger.info(f"  Device: {self.device}")
        self.logger.info(f"  Compute Type: {self.compute_type}")

    async def load_model(self) -> None:
        """CTranslate2ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        import os
        self.logger.info(f"CTranslate2ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {self.model_path}")
        self.logger.info(f"  Current Working Directory: {os.getcwd()}")
        self.logger.info(f"  Model Path (absolute): {self.model_path.absolute()}")
        self.logger.info(f"  Model Path exists: {self.model_path.exists()}")
        start_time = time.time()

        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª
            if not self.model_path.exists():
                raise ModelNotLoadedError(
                    f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_path}\n"
                    f"WorkingDirectory: {os.getcwd()}\n"
                    f"Absolute path: {self.model_path.absolute()}\n"
                    f"convert_nllb_to_ctranslate2.pyã§å¤‰æ›ã—ã¦ãã ã•ã„"
                )

            # Translatorãƒ­ãƒ¼ãƒ‰
            self.logger.info("TranslatoråˆæœŸåŒ–ä¸­...")
            self.translator = ctranslate2.Translator(
                str(self.model_path),
                device=self.device,
                compute_type=self.compute_type,
                inter_threads=self.max_workers,
                intra_threads=1,  # ğŸ”¥ [PHASE1.2] ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«åˆ¶é™
                max_queued_batches=2  # ğŸ”¥ [PHASE1.2] ãƒãƒƒãƒã‚­ãƒ¥ãƒ¼åˆ¶é™ï¼ˆVRAMçˆ†ç™ºé˜²æ­¢ï¼‰
            )
            self.logger.info("Translatorãƒ­ãƒ¼ãƒ‰å®Œäº†")
            self.logger.info(f"  Device: {self.translator.device}")
            self.logger.info(f"  Compute Type: {self.translator.compute_type}")

            # ğŸ”¥ [Issue #185] tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§tokenizer.jsonã‚’ç›´æ¥ãƒ­ãƒ¼ãƒ‰
            # transformers/torchä¸è¦ã§è»½é‡åŒ–ï¼ˆ~450MBå‰Šæ¸›ï¼‰
            tokenizer_path = self.model_path / "tokenizer.json"
            self.logger.info(f"Tokenizer ãƒ­ãƒ¼ãƒ‰ä¸­: {tokenizer_path}")
            if not tokenizer_path.exists():
                raise ModelNotLoadedError(
                    f"tokenizer.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tokenizer_path}\n"
                    f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« tokenizer.json ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                )
            self.tokenizer = Tokenizer.from_file(str(tokenizer_path))
            self.logger.info("Tokenizer ãƒ­ãƒ¼ãƒ‰æˆåŠŸ (tokenizers library)")
            self.logger.info(f"  Vocabulary size: {self.tokenizer.get_vocab_size()}")

            load_time = time.time() - start_time
            self.logger.info(f"CTranslate2ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")

            # is_loaded ã‚’å…ˆã«è¨­å®šï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§translate()ã‚’å‘¼ã¶ãŸã‚ï¼‰
            self.is_loaded = True

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            await self._warmup_model()
            total_time = time.time() - start_time
            self.logger.info(f"CTranslate2 engine ready - Total time: {total_time:.2f}ç§’")
            self.logger.info("NLLB-200-distilled-1.3B (int8) loaded - ~5.5GB memory")

        except ImportError as e:
            self.logger.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
            raise ModelNotLoadedError(f"Required library missing: {e}")
        except OSError as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model file load failed: {e}")
        except Exception as e:
            self.logger.error(f"CTranslate2ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"CTranslate2 model load failed: {e}")

    async def _warmup_model(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›æ¨è«–ã®é…å»¶å›é¿ï¼‰"""
        self.logger.info("CTranslate2ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")

        try:
            # è‹±èªâ†’æ—¥æœ¬èª
            await self.translate("Hello", "en", "ja")
            self.logger.info("è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")

            # æ—¥æœ¬èªâ†’è‹±èª
            await self.translate("ã“ã‚“ã«ã¡ã¯", "ja", "en")
            self.logger.info("æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")

        except Exception as e:
            self.logger.warning(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—ï¼ˆç„¡è¦–ï¼‰: {e}")

    def _get_nllb_lang_code(self, lang_code: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’NLLB-200å½¢å¼ã«å¤‰æ›

        Args:
            lang_code: ISO 639-1ã‚³ãƒ¼ãƒ‰ ("en", "ja"ç­‰)

        Returns:
            NLLB-200 BCP-47ã‚³ãƒ¼ãƒ‰ ("eng_Latn", "jpn_Jpan"ç­‰)

        Raises:
            UnsupportedLanguageError: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èª
        """
        if not lang_code or not isinstance(lang_code, str):
            raise UnsupportedLanguageError(f"Invalid language code: {lang_code}")

        normalized = lang_code.lower()

        # ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        if normalized in self.LANGUAGE_MAPPING:
            return self.LANGUAGE_MAPPING[normalized]

        # ãƒãƒƒãƒ”ãƒ³ã‚°ã«ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        raise UnsupportedLanguageError(
            f"Unsupported language: {lang_code}. "
            f"Supported: {list(self.LANGUAGE_MAPPING.keys())}"
        )

    def _encode_text(self, text: str, source_lang: str) -> List[str]:
        """ğŸ”¥ [Issue #185] tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            source_lang: å…ƒè¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆISO 639-1ï¼‰

        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ

        Raises:
            ModelNotLoadedError: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æœªåˆæœŸåŒ–
        """
        if not self.tokenizer:
            raise ModelNotLoadedError("TokenizerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # è¨€èªã‚³ãƒ¼ãƒ‰å–å¾—ï¼ˆNLLB-200å½¢å¼: eng_Latn, jpn_Jpanï¼‰
            nllb_lang_code = self.LANGUAGE_MAPPING.get(source_lang, source_lang)

            # tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
            with self.tokenizer_lock:
                encoding = self.tokenizer.encode(text)

            # ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ãƒªã‚¹ãƒˆã‚’å–å¾—
            tokens = encoding.tokens

            # NLLBå½¢å¼: è¨€èªã‚³ãƒ¼ãƒ‰ã‚’å…ˆé ­ã«è¿½åŠ ã—ã€</s>ã‚’æœ«å°¾ã«è¿½åŠ 
            # ä¾‹: [eng_Latn, â–Hello, â–world, </s>]
            tokens = [nllb_lang_code] + tokens + ["</s>"]

            return tokens

        except Exception as e:
            self.logger.error(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Tokenization error: {e}")

    def _decode_tokens(self, tokens: List[str]) -> str:
        """ğŸ”¥ [Issue #185] tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒ‡ã‚³ãƒ¼ãƒ‰

        Args:
            tokens: ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ

        Returns:
            ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

        Raises:
            ModelNotLoadedError: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æœªåˆæœŸåŒ–
        """
        if not self.tokenizer:
            raise ModelNotLoadedError("TokenizerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # ğŸ”¥ [GEMINI_REVIEW] LANGUAGE_MAPPINGã‹ã‚‰å‹•çš„ç”Ÿæˆï¼ˆå°†æ¥ã®è¨€èªè¿½åŠ æ™‚ã®ä¿®æ­£æ¼ã‚Œé˜²æ­¢ï¼‰
            nllb_language_codes = set(self.LANGUAGE_MAPPING.values())
            special_tokens = {"<s>", "</s>", "<pad>", "<unk>"}

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_tokens = [
                token for token in tokens
                if token not in special_tokens and token not in nllb_language_codes
            ]

            # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›ã—ã¦ã‹ã‚‰ãƒ‡ã‚³ãƒ¼ãƒ‰
            # tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®decode()ã¯IDãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹
            with self.tokenizer_lock:
                token_ids = [
                    self.tokenizer.token_to_id(token)
                    for token in filtered_tokens
                    if self.tokenizer.token_to_id(token) is not None
                ]
                decoded_text = self.tokenizer.decode(token_ids)

            # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            return decoded_text.strip()

        except Exception as e:
            self.logger.error(f"ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Decoding error: {e}")

    async def translate(
        self,
        text: str,
        source_lang: str,
        target_lang: str
    ) -> Tuple[str, float]:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³"""
        if not self.is_loaded or not self.translator or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")

        # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼
        if not text or not isinstance(text, str):
            raise ValueError(f"Invalid text: {text}")

        if len(text.strip()) == 0:
            return ("", 0.0)

        # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
        src_code = self._get_nllb_lang_code(source_lang)
        tgt_code = self._get_nllb_lang_code(target_lang)

        # ğŸ”§ [ENGINE_DEBUG] å…¥åŠ›æƒ…å ±ãƒ­ã‚°
        self.logger.info(f"[ENGINE_TRANSLATE_INPUT] src_code: {src_code}, tgt_code: {tgt_code}")
        self.logger.info(f"[ENGINE_TRANSLATE_INPUT] Text length: {len(text)}, Text: {text[:100]}...")

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆsource_langã‚’æ¸¡ã™ï¼‰
            source_tokens = self._encode_text(text, source_lang)

            # ğŸ”§ [ENGINE_DEBUG] ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºçµæœãƒ­ã‚°
            self.logger.info(f"[ENGINE_TOKENIZE] Token count: {len(source_tokens)}, Tokens: {source_tokens[:20]}...")

            # ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒã‚§ãƒƒã‚¯
            if len(source_tokens) > self.MAX_TEXT_LENGTH:
                raise TextTooLongError(
                    f"Text too long: {len(source_tokens)} tokens "
                    f"(max: {self.MAX_TEXT_LENGTH})"
                )

            # ç¿»è¨³å®Ÿè¡Œï¼ˆasyncio.to_threadã§éåŒæœŸåŒ–ï¼‰
            # ğŸ”¥ [QUALITY_FIX] beam_size=1â†’4ã«å¤‰æ›´ï¼ˆBLEU +1.0ã€œ1.5å‘ä¸Šï¼‰
            # å‚è€ƒ: https://forum.opennmt.net/t/nllb-200-with-ctranslate2/5090
            def _generate():
                return self.translator.translate_batch(
                    source=[source_tokens],
                    target_prefix=[[tgt_code]],
                    beam_size=4,  # ğŸ”¥ å“è³ªå‘ä¸Šã®ãŸã‚1â†’4ã«å¤‰æ›´
                    max_decoding_length=256,  # é•·ã‚ã«è¨­å®š
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3,
                    length_penalty=1.0,  # ğŸ”¥ è¿½åŠ : é©åˆ‡ãªå‡ºåŠ›é•·ã‚’ä¿ƒé€²
                    return_scores=True
                )

            results = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                _generate
            )

            # ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            output_tokens = results[0].hypotheses[0]

            # ğŸ”§ [ENGINE_DEBUG] ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ­ã‚°
            self.logger.info(f"[ENGINE_DETOKENIZE] Output token count: {len(output_tokens)}, Tokens: {output_tokens[:20]}...")

            translated_text = self._decode_tokens(output_tokens)

            # ğŸ”§ [ENGINE_DEBUG] ç¿»è¨³çµæœãƒ­ã‚°
            self.logger.info(f"[ENGINE_TRANSLATE_OUTPUT] Translated text length: {len(translated_text)}, Text: {translated_text[:100]}...")

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆCTranslate2ã¯ã‚¹ã‚³ã‚¢æä¾›ï¼‰
            confidence = results[0].scores[0] if results[0].scores else -1.0

            # ğŸ”¥ [PHASE1.2] å®šæœŸçš„ãªæ˜ç¤ºçš„ãƒ¡ãƒ¢ãƒªè§£æ”¾ï¼ˆ1000å›ã”ã¨ï¼‰
            self.translation_count += 1
            if self.translation_count % self.max_translations_before_gc == 0:
                self.logger.info(f"[GC_TRIGGER] {self.translation_count} translations, forcing GC")
                gc.collect()

            return (translated_text, confidence)

        except UnsupportedLanguageError:
            raise
        except TextTooLongError:
            raise
        except Exception as e:
            # ğŸ”¥ [PHASE1.2] ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚GCã‚’å®Ÿè¡Œã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾
            self.logger.warning(f"[GC_ON_ERROR] Translation error, forcing GC: {e}")
            gc.collect()
            raise ModelInferenceError(f"Translation failed: {e}")

    async def translate_batch(
        self,
        texts: List[str],
        source_lang: str,
        target_lang: str
    ) -> List[Tuple[str, float]]:
        """ãƒãƒƒãƒç¿»è¨³ï¼ˆGPUæœ€é©åŒ–ï¼‰"""
        if not self.is_loaded or not self.translator or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if len(texts) > self.MAX_BATCH_SIZE:
            raise BatchSizeExceededError(
                f"Batch size {len(texts)} exceeds maximum {self.MAX_BATCH_SIZE}"
            )

        # ç©ºãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_texts = [t for t in texts if t and t.strip()]
        if not valid_texts:
            return [("", 0.0) for _ in texts]

        # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
        src_code = self._get_nllb_lang_code(source_lang)
        tgt_code = self._get_nllb_lang_code(target_lang)

        try:
            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆsource_langã‚’æ¸¡ã™ï¼‰
            source_tokens_batch = [
                self._encode_text(text, source_lang)
                for text in valid_texts
            ]

            # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œï¼ˆasyncio.to_threadã§éåŒæœŸåŒ–ï¼‰
            def _generate_batch():
                return self.translator.translate_batch(
                    source=source_tokens_batch,
                    target_prefix=[[tgt_code]] * len(valid_texts),
                    beam_size=4,
                    max_decoding_length=128,
                    repetition_penalty=1.2,  # ç¹°ã‚Šè¿”ã—é˜²æ­¢
                    no_repeat_ngram_size=3,  # 3-gramç¹°ã‚Šè¿”ã—é˜²æ­¢
                    length_penalty=1.0       # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£
                )

            results = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                _generate_batch
            )

            # ãƒãƒƒãƒãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            translated_texts = [
                self._decode_tokens(result.hypotheses[0])
                for result in results
            ]

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            confidence_scores = [
                result.scores[0] if result.scores else -1.0
                for result in results
            ]

            # çµæœã‚’å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã¨åŒã˜é †åºã§è¿”ã™
            result_list = []
            valid_idx = 0
            for text in texts:
                if text and text.strip():
                    result_list.append((translated_texts[valid_idx], confidence_scores[valid_idx]))
                    valid_idx += 1
                else:
                    result_list.append(("", 0.0))

            # ğŸ”¥ [PHASE1.2] ãƒãƒƒãƒå‡¦ç†å¾Œã‚‚GCãƒˆãƒªã‚¬ãƒ¼ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            self.translation_count += len(valid_texts)
            if self.translation_count % self.max_translations_before_gc == 0:
                self.logger.info(f"[GC_TRIGGER] {self.translation_count} translations (batch), forcing GC")
                gc.collect()

            return result_list

        except Exception as e:
            # ğŸ”¥ [PHASE1.2] ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚GCã‚’å®Ÿè¡Œã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾
            self.logger.warning(f"[GC_ON_ERROR] Batch translation error, forcing GC: {e}")
            gc.collect()
            raise ModelInferenceError(f"Batch translation failed: {e}")

    async def is_ready(self) -> bool:
        """æº–å‚™å®Œäº†ç¢ºèª"""
        return self.is_loaded and self.translator is not None and self.tokenizer is not None

    def get_supported_languages(self) -> List[str]:
        """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹è¨€èªã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ"""
        return list(self.LANGUAGE_MAPPING.keys())
