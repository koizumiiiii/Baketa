"""
ãƒ†ã‚¹ãƒˆç”¨ã®SentencePieceãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Google SentencePieceãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€
ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªSentencePieceãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
pip install sentencepiece

ä½¿ç”¨æ–¹æ³•:
python create_test_sentencepiece_model.py
"""

import sentencepiece as spm
import os

def create_test_model():
    """ãƒ†ã‚¹ãƒˆç”¨ã®SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®èªå½™ã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
    sample_texts = [
        "Hello world",
        "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ",
        "How are you?",
        "å…ƒæ°—ã§ã™ã‹ï¼Ÿ",
        "Good morning",
        "ãŠã¯ã‚ˆã†",
        "Thank you",
        "ã‚ã‚ŠãŒã¨ã†",
        "Yes, No",
        "ã¯ã„ã€ã„ã„ãˆ",
        "I am fine",
        "ç§ã¯å…ƒæ°—ã§ã™",
        "This is a test",
        "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™",
        "machine translation",
        "æ©Ÿæ¢°ç¿»è¨³",
        "artificial intelligence",
        "äººå·¥çŸ¥èƒ½",
        "deep learning",
        "æ·±å±¤å­¦ç¿’",
        "natural language processing",
        "è‡ªç„¶è¨€èªå‡¦ç†"
    ]
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    input_file = "test_corpus.txt"
    with open(input_file, "w", encoding="utf-8") as f:
        for text in sample_texts:
            f.write(text + "\n")
    
    # SentencePieceãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
    model_prefix = "test-sentencepiece"
    
    # å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆvocab_size=1000ï¼‰
    spm.SentencePieceTrainer.train(
        input=input_file,
        model_prefix=model_prefix,
        vocab_size=1000,
        model_type="unigram",
        character_coverage=0.995,
        unk_id=0,
        bos_id=1,
        eos_id=2,
        pad_id=3,
        normalization_rule_name="nfkc"
    )
    
    # ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    model_file = f"{model_prefix}.model"
    vocab_file = f"{model_prefix}.vocab"
    
    if os.path.exists(model_file):
        print(f"âœ… SentencePieceãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ: {model_file}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(model_file)} bytes")
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    if os.path.exists(vocab_file):
        print(f"âœ… èªå½™ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ: {vocab_file}")
        
    # ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    test_model(model_file)
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    os.remove(input_file)
    
    return model_file

def test_model(model_file):
    """ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ:")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    sp = spm.SentencePieceProcessor()
    sp.load(model_file)
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        "Hello",
        "World",
        "ã“ã‚“ã«ã¡ã¯",
        "æ©Ÿæ¢°ç¿»è¨³",
        "This is a test sentence."
    ]
    
    for text in test_cases:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        tokens = sp.encode(text)
        pieces = sp.encode_as_pieces(text)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        decoded = sp.decode(tokens)
        
        print(f"  å…¥åŠ›: {text}")
        print(f"  ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
        print(f"  ãƒ”ãƒ¼ã‚¹: {pieces}")
        print(f"  ãƒ‡ã‚³ãƒ¼ãƒ‰: {decoded}")
        print()

def main():
    print("ğŸš€ ãƒ†ã‚¹ãƒˆç”¨SentencePieceãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 50)
    
    try:
        model_file = create_test_model()
        
        print("\nâœ… ä½œæˆå®Œäº†!")
        print(f"ä½œæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {model_file}")
        print("\nã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Baketaãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®Models/SentencePieceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚")
        
    except ImportError:
        print("âŒ ã‚¨ãƒ©ãƒ¼: sentencepieceãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("pip install sentencepiece")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
