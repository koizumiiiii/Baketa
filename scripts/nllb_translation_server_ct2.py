#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NLLB-200 CTranslate2ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 2.4GB â†’ 0.5GB (80%å‰Šæ¸›)
æ¨è«–é€Ÿåº¦: 20-30%é«˜é€ŸåŒ–
"""

import asyncio
import json
import logging
import signal
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import argparse
from collections import deque
from threading import Lock
from pathlib import Path

import ctranslate2
import sentencepiece as smp
from transformers import AutoTokenizer

# ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–å®šç¾©
class ModelNotLoadedError(Exception):
    """ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class UnsupportedLanguageError(Exception):
    """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èªã®å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class TextTooLongError(Exception):
    """ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class BatchSizeExceededError(Exception):
    """ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class ModelInferenceError(Exception):
    """ãƒ¢ãƒ‡ãƒ«æ¨è«–ä¸­ã®ã‚¨ãƒ©ãƒ¼"""
    pass

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TranslationRequest:
    """ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    text: str
    source_lang: str
    target_lang: str
    request_id: Optional[str] = None

@dataclass
class TranslationResponse:
    """ç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translation: Optional[str] = None
    confidence: float = 0.0
    error: Optional[str] = None
    error_code: Optional[str] = None
    processing_time: float = 0.0

@dataclass
class BatchTranslationRequest:
    """ãƒãƒƒãƒç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    texts: List[str]
    source_lang: str
    target_lang: str
    batch_mode: bool = True
    max_batch_size: int = 50

@dataclass
class BatchTranslationResponse:
    """ãƒãƒƒãƒç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translations: List[str]
    confidence_scores: List[float]
    processing_time: float
    batch_size: int
    errors: Optional[List[str]] = None

class CTranslate2ResourceMonitor:
    """CTranslate2ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    def get_memory_usage_mb(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        try:
            import psutil
            import os
            process = psutil.Process(os.getpid())
            memory_mb = process.memory_info().rss / (1024 * 1024)
            return memory_mb
        except Exception as e:
            self.logger.warning(f"ãƒ¡ãƒ¢ãƒªå–å¾—å¤±æ•—: {e}")
            return 0.0

    def log_memory_status(self):
        """ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        memory_mb = self.get_memory_usage_mb()
        self.logger.info(f"ğŸ“Š [MEMORY] Current: {memory_mb:.1f}MB")

class NLLB200CTranslate2Server:
    """
    NLLB-200 CTranslate2ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼

    ç‰¹å¾´:
    - int8é‡å­åŒ–ã«ã‚ˆã‚Š80%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼ˆ2.4GB â†’ 0.5GBï¼‰
    - 20-30%æ¨è«–é«˜é€ŸåŒ–
    - 200è¨€èªå¯¾å¿œç¶­æŒ
    - æ—¢å­˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®Œå…¨äº’æ›
    """

    def __init__(
        self,
        model_path: str = "models/nllb-200-ct2",
        device: str = "cpu",
        compute_type: str = "int8",
        max_workers: int = 4
    ):
        """
        Args:
            model_path: CTranslate2å¤‰æ›æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ï¼ˆcpu, cuda, autoï¼‰
            compute_type: è¨ˆç®—å‹ï¼ˆint8, int16, float16, float32ï¼‰
            max_workers: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.model_path = Path(model_path)
        self.device = device
        self.compute_type = compute_type
        self.max_workers = max_workers

        self.translator: Optional[ctranslate2.Translator] = None
        self.tokenizer: Optional[AutoTokenizer] = None
        self.use_auto_tokenizer: bool = True  # ğŸ¯ Gemini AIæ¨å¥¨: AutoTokenizerå„ªå…ˆä½¿ç”¨

        self.resource_monitor = CTranslate2ResourceMonitor()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

        # ãƒãƒƒãƒå‡¦ç†é–¢é€£
        self.batch_queue: deque = deque()
        self.batch_lock = Lock()
        self.batch_timeout = 0.1  # 100ms
        self.max_batch_size = 50

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¸¦åˆ—ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ï¼ˆGeminiæŒ‡æ‘˜: Race Conditionå¯¾ç­–ï¼‰
        self.tokenizer_lock = Lock()

        logger.info("ğŸ”¥ CTranslate2ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–")
        logger.info(f"   Model Path: {self.model_path}")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   Compute Type: {self.compute_type}")

        # è¨€èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆNLLB-200å¯¾å¿œï¼‰
        self.language_mapping = {
            "en": "eng_Latn",
            "ja": "jpn_Jpan",
            "english": "eng_Latn",
            "japanese": "jpn_Jpan"
        }

    def _get_available_memory_gb(self) -> float:
        """åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªé‡ï¼ˆGBï¼‰ã‚’å–å¾—"""
        import psutil
        try:
            available_bytes = psutil.virtual_memory().available
            available_gb = available_bytes / (1024**3)
            return available_gb
        except ImportError:
            logger.warning("psutil not available - assuming 8GB memory")
            return 8.0
        except Exception as e:
            logger.warning(f"Memory detection failed: {e} - assuming 4GB")
            return 4.0

    def load_model(self):
        """CTranslate2ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        logger.info("ğŸš€ [CT2_LOAD_START] CTranslate2ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        start_time = time.time()

        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª
            if not self.model_path.exists():
                raise ModelNotLoadedError(
                    f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_path}\n"
                    f"convert_nllb_to_ctranslate2.pyã§å¤‰æ›ã—ã¦ãã ã•ã„"
                )

            # Translatorãƒ­ãƒ¼ãƒ‰
            logger.info("ğŸ§  [CT2_TRANSLATOR] TranslatoråˆæœŸåŒ–ä¸­...")
            self.translator = ctranslate2.Translator(
                str(self.model_path),
                device=self.device,
                compute_type=self.compute_type,
                inter_threads=self.max_workers
            )
            logger.info(f"âœ… Translatorãƒ­ãƒ¼ãƒ‰å®Œäº†")
            logger.info(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.translator.device}")
            logger.info(f"   è¨ˆç®—å‹: {self.translator.compute_type}")

            # ğŸ¯ UltraThink Phase 3: HuggingFace NllbTokenizerä½¿ç”¨ï¼ˆSentencePieceå¯¾å¿œï¼‰
            logger.info("ğŸ“ [CT2_TOKENIZER] HuggingFace NllbTokenizer ãƒ­ãƒ¼ãƒ‰ä¸­...")

            try:
                # facebook/nllb-200-distilled-600M ã®å…¬å¼ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
                # SentencePiece BPEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒè‡ªå‹•çš„ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹
                self.tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
                self.use_auto_tokenizer = True  # HuggingFaceå®Ÿè£…

                logger.info("âœ… [NLLB_TOKENIZER] facebook/nllb-200-distilled-600M ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                logger.info(f"   èªå½™ã‚µã‚¤ã‚º: {len(self.tokenizer)}")
                logger.info(f"   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‹: {type(self.tokenizer).__name__}")

            except Exception as tokenizer_error:
                logger.error(f"âŒ NllbTokenizerãƒ­ãƒ¼ãƒ‰å¤±æ•—: {tokenizer_error}")
                raise ModelNotLoadedError(f"NllbTokenizerãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {tokenizer_error}")

            load_time = time.time() - start_time
            logger.info(f"ğŸ‰ [CT2_LOAD_COMPLETE] ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            self.resource_monitor.log_memory_status()

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            self._warmup_model()

            total_time = time.time() - start_time
            logger.info(f"ğŸ [CT2_READY] ã™ã¹ã¦ã®åˆæœŸåŒ–å®Œäº† - ç·æ™‚é–“: {total_time:.2f}ç§’")
            logger.info("âœ… CTranslate2ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº† - 80%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›é”æˆ")

        except Exception as e:
            logger.error(f"âŒ [CT2_LOAD_FAILED] ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"CTranslate2 model load failed: {e}")

    def _warmup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›æ¨è«–é…å»¶å›é¿ï¼‰"""
        logger.info("ğŸ”¥ [WARMUP] ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
        try:
            test_text = "ã“ã‚“ã«ã¡ã¯"
            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¯æ—¥æœ¬èªâ†’è‹±èªã§å›ºå®šï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
            test_tokens = self._encode_text(test_text, "ja")

            results = self.translator.translate_batch(
                source=[test_tokens],
                target_prefix=[["eng_Latn"]],
                beam_size=1,
                max_decoding_length=64,   # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ç”¨çŸ­ã„é•·ã•
                repetition_penalty=1.2,  # ç¹°ã‚Šè¿”ã—é˜²æ­¢
                no_repeat_ngram_size=3   # 3-gramã®ç¹°ã‚Šè¿”ã—é˜²æ­¢
            )

            logger.info("âœ… [WARMUP] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
        except Exception as e:
            logger.warning(f"âš ï¸ [WARMUP] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—ï¼ˆç„¡è¦–å¯èƒ½ï¼‰: {e}")

    def normalize_language_code(self, lang: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’NLLB-200å½¢å¼ã«æ­£è¦åŒ–"""
        lang_lower = lang.lower().strip()
        return self.language_mapping.get(lang_lower, lang)

    def _encode_text(self, text: str, source_lang: str) -> List[str]:
        """ğŸ¯ UltraThink Phase 3: HuggingFace NllbTokenizer ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""

        if not hasattr(self, 'tokenizer') or not self.tokenizer:
            raise ModelNotLoadedError("NllbTokenizerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # è¨€èªã‚³ãƒ¼ãƒ‰å–å¾—ï¼ˆNLLB-200å½¢å¼: eng_Latn, jpn_Jpanï¼‰
            nllb_lang_code = self.language_mapping.get(source_lang, source_lang)

            # ğŸ”’ GeminiæŒ‡æ‘˜: tokenizer.src_lang ã¯å…±æœ‰çŠ¶æ…‹ã®ãŸã‚ã€ãƒ­ãƒƒã‚¯ã§ä¿è­·
            with self.tokenizer_lock:
                # NllbTokenizerã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                # src_langè¨­å®šã§ã‚½ãƒ¼ã‚¹è¨€èªã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒè‡ªå‹•ä»˜ä¸ã•ã‚Œã‚‹
                self.tokenizer.src_lang = nllb_lang_code
                # ğŸ”¥ UltraThink Phase 4: add_special_tokens=True ã«å¤‰æ›´
                # NLLB-200ã§ã¯è¨€èªã‚³ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: jpn_Jpanï¼‰ãŒå¿…é ˆ
                encoded = self.tokenizer(text, return_tensors=None, add_special_tokens=True)

            # token IDsã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ï¼ˆãƒ­ãƒƒã‚¯å¤–ã§å®Ÿè¡Œå¯èƒ½ï¼‰
            tokens = self.tokenizer.convert_ids_to_tokens(encoded["input_ids"])

            logger.debug(f"âœ… [ENCODE] ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†: {len(tokens)} tokens")
            return tokens

        except Exception as e:
            logger.error(f"âŒ [ENCODE_ERROR] ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    def _decode_tokens(self, tokens: List[str]) -> str:
        """ğŸ¯ UltraThink Phase 3: HuggingFace NllbTokenizer ãƒ‡ã‚³ãƒ¼ãƒ‰"""

        if not hasattr(self, 'tokenizer') or not self.tokenizer:
            raise ModelNotLoadedError("NllbTokenizerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        try:
            # ğŸ”¥ é‡è¦: è¨€èªã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã¨ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
            language_codes = {
                "eng_Latn", "jpn_Jpan", "fra_Latn", "deu_Latn", "spa_Latn",
                "ita_Latn", "por_Latn", "rus_Cyrl", "zho_Hans", "zho_Hant",
                "kor_Hang", "ara_Arab", "hin_Deva", "tha_Thai", "vie_Latn"
            }

            special_tokens = {"<s>", "</s>", "<pad>", "<unk>"}

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_tokens = [
                token for token in tokens
                if token not in special_tokens and token not in language_codes
            ]

            # ğŸ”¥ [TOKEN_DEBUG] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœãƒ‡ãƒãƒƒã‚°
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Original tokens count: {len(tokens)}")
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Filtered tokens count: {len(filtered_tokens)}")
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Filtered tokens (first 20): {filtered_tokens[:20]}")

            # ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            # NllbTokenizer.convert_tokens_to_string()ã§SentencePieceå‡¦ç†ãŒè‡ªå‹•å®Ÿè¡Œã•ã‚Œã‚‹
            decoded_text = self.tokenizer.convert_tokens_to_string(filtered_tokens)

            # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            result = decoded_text.strip()

            # ğŸ”¥ [TOKEN_DEBUG] æœ€çµ‚çµæœãƒ‡ãƒãƒƒã‚°
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Final decoded text: '{result}'")
            logger.info(f"âœ… [DECODE] ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: {len(result)} chars")
            return result

        except Exception as e:
            logger.error(f"âŒ [DECODE_ERROR] ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")

    async def translate(
        self,
        text: str,
        source_lang: str,
        target_lang: str
    ) -> TranslationResponse:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆç¿»è¨³"""
        start_time = time.time()

        try:
            # ğŸ¯ UltraThink Phase 3: NllbTokenizerãƒã‚§ãƒƒã‚¯
            if not self.translator:
                raise ModelNotLoadedError("TranslatorãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

            # HuggingFace NllbTokenizerãƒã‚§ãƒƒã‚¯
            if not hasattr(self, 'tokenizer') or not self.tokenizer:
                raise ModelNotLoadedError("NllbTokenizerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

            # è¨€èªã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–
            source_code = self.normalize_language_code(source_lang)
            target_code = self.normalize_language_code(target_lang)

            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆsource_langã‚’æ¸¡ã™ï¼‰
            source_tokens = self._encode_text(text, source_lang)

            # ğŸ”¥ [TOKEN_DEBUG] å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒãƒƒã‚°
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Input text: '{text[:50]}...'")
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Source tokens (first 20): {source_tokens[:20]}")
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Source lang code: {source_code}, Target lang code: {target_code}")

            # ç¿»è¨³å®Ÿè¡Œ - å¼·åŒ–ã•ã‚ŒãŸç¹°ã‚Šè¿”ã—é˜²æ­¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            results = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                lambda: self.translator.translate_batch(
                    source=[source_tokens],
                    target_prefix=[[target_code]],
                    beam_size=1,             # ãƒ“ãƒ¼ãƒ æ•°ã‚’1ã«å‰Šæ¸›
                    max_decoding_length=64,  # ã•ã‚‰ã«çŸ­ã
                    repetition_penalty=1.5,  # ã‚ˆã‚Šå¼·ã„ç¹°ã‚Šè¿”ã—é˜²æ­¢
                    no_repeat_ngram_size=2,  # ã‚ˆã‚Šå³å¯†ãª2-gramé˜²æ­¢
                    length_penalty=0.8,      # çŸ­ã„ç¿»è¨³ã‚’å„ªå…ˆ
                    disable_unk=True         # æœªçŸ¥ãƒˆãƒ¼ã‚¯ãƒ³ç„¡åŠ¹åŒ–
                )
            )

            # ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            # CTranslate2 ã¯ tokenæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            output_tokens = results[0].hypotheses[0]

            # ğŸ”¥ [TOKEN_DEBUG] å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒãƒƒã‚°
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Output tokens (first 20): {output_tokens[:20]}")
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Output tokens (last 10): {output_tokens[-10:]}")
            logger.info(f"ğŸ”¥ [TOKEN_DEBUG] Total output tokens: {len(output_tokens)}")

            # NllbTokenizerã§ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ— â†’ é€šå¸¸ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            translation = self._decode_tokens(output_tokens)

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            confidence = results[0].scores[0] if results[0].scores else 0.0

            processing_time = time.time() - start_time

            return TranslationResponse(
                success=True,
                translation=translation,
                confidence=confidence,
                processing_time=processing_time
            )

        except Exception as e:
            logger.error(f"âŒ [TRANSLATE_ERROR] ç¿»è¨³å¤±æ•—: {e}")
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="TRANSLATION_FAILED",
                processing_time=time.time() - start_time
            )

    async def translate_batch(
        self,
        texts: List[str],
        source_lang: str,
        target_lang: str
    ) -> BatchTranslationResponse:
        """ãƒãƒƒãƒç¿»è¨³"""
        start_time = time.time()

        try:
            # ğŸ¯ UltraThink Phase 3: NllbTokenizerãƒã‚§ãƒƒã‚¯
            if not self.translator:
                raise ModelNotLoadedError("TranslatorãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

            # HuggingFace NllbTokenizerãƒã‚§ãƒƒã‚¯
            if not hasattr(self, 'tokenizer') or not self.tokenizer:
                raise ModelNotLoadedError("NllbTokenizerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

            # è¨€èªã‚³ãƒ¼ãƒ‰æ­£è¦åŒ–
            source_code = self.normalize_language_code(source_lang)
            target_code = self.normalize_language_code(target_lang)

            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆsource_langã‚’æ¸¡ã™ï¼‰
            source_tokens_batch = [
                self._encode_text(text, source_lang)
                for text in texts
            ]

            # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ - ãƒˆãƒ¼ã‚¯ãƒ³ç¹°ã‚Šè¿”ã—é˜²æ­¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ 
            results = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                lambda: self.translator.translate_batch(
                    source=source_tokens_batch,
                    target_prefix=[[target_code]] * len(texts),
                    beam_size=4,
                    max_decoding_length=128,  # é•·ã™ãã‚‹ã¨ç¹°ã‚Šè¿”ã—ã®ãƒªã‚¹ã‚¯
                    repetition_penalty=1.2,  # ç¹°ã‚Šè¿”ã—é˜²æ­¢
                    no_repeat_ngram_size=3,  # 3-gramã®ç¹°ã‚Šè¿”ã—é˜²æ­¢
                    length_penalty=1.0       # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£
                )
            )

            # ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            translations = [
                self._decode_tokens(result.hypotheses[0])
                for result in results
            ]

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            confidence_scores = [
                result.scores[0] if result.scores else 0.0
                for result in results
            ]

            processing_time = time.time() - start_time

            # ãƒ¡ãƒ¢ãƒªç›£è¦–
            self.resource_monitor.log_memory_status()

            return BatchTranslationResponse(
                success=True,
                translations=translations,
                confidence_scores=confidence_scores,
                processing_time=processing_time,
                batch_size=len(texts)
            )

        except Exception as e:
            logger.error(f"âŒ [BATCH_TRANSLATE_ERROR] ãƒãƒƒãƒç¿»è¨³å¤±æ•—: {e}")
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=time.time() - start_time,
                batch_size=len(texts),
                errors=[str(e)]
            )

    async def handle_command(self, command: Dict):
        """ã‚³ãƒãƒ³ãƒ‰å‡¦ç†ï¼ˆæ—¢å­˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹äº’æ›ï¼‰"""
        cmd_type = command.get("command")

        if cmd_type == "translate":
            text = command.get("text", "")
            source_lang = command.get("source_lang", "ja")
            target_lang = command.get("target_lang", "en")

            response = await self.translate(text, source_lang, target_lang)

            return {
                "success": response.success,
                "translation": response.translation,
                "confidence": response.confidence,
                "error": response.error,
                "processing_time": response.processing_time
            }

        elif cmd_type == "translate_batch":
            texts = command.get("texts", [])
            source_lang = command.get("source_lang", "ja")
            target_lang = command.get("target_lang", "en")

            response = await self.translate_batch(texts, source_lang, target_lang)

            return {
                "success": response.success,
                "translations": response.translations,
                "confidence_scores": response.confidence_scores,
                "processing_time": response.processing_time,
                "batch_size": response.batch_size
            }

        elif cmd_type == "is_ready":
            return {
                "success": True,
                "ready": self.translator is not None,
                "model_loaded": self.translator is not None,
                "engine": "ctranslate2"
            }

        elif cmd_type == "get_supported_languages":
            return {
                "success": True,
                "languages": list(self.language_mapping.keys())
            }

        else:
            return {
                "success": False,
                "error": f"Unknown command: {cmd_type}"
            }

    async def serve_forever(self):
        """ãƒ¡ã‚¤ãƒ³ã‚µãƒ¼ãƒãƒ¼ãƒ«ãƒ¼ãƒ—ï¼ˆstdin/stdouté€šä¿¡ï¼‰"""
        logger.info("ğŸš€ [SERVER_START] CTranslate2ã‚µãƒ¼ãƒãƒ¼èµ·å‹•")

        # ğŸ”¥ UltraPhase 14.18: stdinçŠ¶æ…‹ç¢ºèª
        logger.info(f"ğŸ“Š [STDIN_DEBUG] stdin.readable(): {sys.stdin.readable()}")
        logger.info(f"ğŸ“Š [STDIN_DEBUG] stdin.isatty(): {sys.stdin.isatty()}")

        # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
        sys.stdin.reconfigure(line_buffering=True)
        logger.info("âš¡ [STDIN_DEBUG] stdin ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°èª¿æ•´å®Œäº†")

        # ğŸ”¥ UltraThink Phase 4.4: C#å´ã®stdinæ¥ç¶šç¢ºç«‹ã‚’å¾…æ©Ÿ
        # Windowsã§ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•ç›´å¾Œã«readline()ã™ã‚‹ã¨EOFã«ãªã‚‹å•é¡Œã‚’å›é¿
        if not sys.stdin.isatty():
            logger.info("â³ [STDIN_WAIT] C#ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã®stdinæ¥ç¶šç¢ºç«‹ã‚’å¾…æ©Ÿä¸­...")
            await asyncio.sleep(0.5)  # 500mså¾…æ©Ÿ
            logger.info("âœ… [STDIN_WAIT] å¾…æ©Ÿå®Œäº† - ã‚³ãƒãƒ³ãƒ‰å—ä¿¡é–‹å§‹")

        loop = asyncio.get_event_loop()

        while True:
            try:
                # ğŸ”¥ UltraPhase 14.18: stdinèª­ã¿å–ã‚Šå‰ãƒ­ã‚°
                logger.info("ğŸ”„ [STDIN_DEBUG] stdin.readline() å¾…æ©Ÿé–‹å§‹...")

                # stdin ã‹ã‚‰ã‚³ãƒãƒ³ãƒ‰èª­ã¿å–ã‚Š
                line = await loop.run_in_executor(None, sys.stdin.readline)

                # ğŸ”¥ UltraPhase 14.18: stdinèª­ã¿å–ã‚Šå¾Œãƒ­ã‚°
                logger.info(f"âœ… [STDIN_DEBUG] stdin.readline() å®Œäº†: {repr(line)}")

                if not line:
                    logger.info("ğŸ“­ [EOF] stdinçµ‚äº† - ã‚µãƒ¼ãƒãƒ¼ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³")
                    break

                line = line.strip()
                if not line:
                    logger.info("ğŸ” [STDIN_DEBUG] ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ğŸ”¥ UltraPhase 14.19: JSONãƒ‘ãƒ¼ã‚¹å‰ãƒ­ã‚°
                logger.info(f"ğŸ” [JSON_DEBUG] JSONãƒ‘ãƒ¼ã‚¹é–‹å§‹: {repr(line)}")

                # JSONãƒ‘ãƒ¼ã‚¹
                command = json.loads(line)

                # ğŸ”¥ UltraPhase 14.19: JSONãƒ‘ãƒ¼ã‚¹å¾Œãƒ­ã‚°
                logger.info(f"âœ… [JSON_DEBUG] JSONãƒ‘ãƒ¼ã‚¹æˆåŠŸ: {command}")

                # ğŸ”¥ UltraPhase 14.19: ã‚³ãƒãƒ³ãƒ‰å‡¦ç†å‰ãƒ­ã‚°
                logger.info(f"ğŸ”„ [CMD_DEBUG] handle_command() é–‹å§‹: {command}")

                # ã‚³ãƒãƒ³ãƒ‰å‡¦ç†
                response = await self.handle_command(command)

                # ğŸ”¥ UltraPhase 14.19: ã‚³ãƒãƒ³ãƒ‰å‡¦ç†å¾Œãƒ­ã‚°
                logger.info(f"âœ… [CMD_DEBUG] handle_command() å®Œäº†: {response}")

                # ğŸ”¥ UltraPhase 14.19: stdoutå‡ºåŠ›å‰ãƒ­ã‚°
                logger.info(f"ğŸ“¤ [STDOUT_DEBUG] ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡ºåŠ›é–‹å§‹: {repr(json.dumps(response))}")

                # stdout ã«çµæœå‡ºåŠ›
                print(json.dumps(response), flush=True)

                # ğŸ”¥ UltraPhase 14.19: stdoutå‡ºåŠ›å¾Œãƒ­ã‚°
                logger.info("âœ… [STDOUT_DEBUG] ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡ºåŠ›å®Œäº†")

            except json.JSONDecodeError as e:
                logger.error(f"âŒ [JSON_ERROR] JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                error_response = {"success": False, "error": "Invalid JSON"}
                print(json.dumps(error_response), flush=True)

            except Exception as e:
                logger.error(f"âŒ [SERVER_ERROR] ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                error_response = {"success": False, "error": str(e)}
                print(json.dumps(error_response), flush=True)

        logger.info("ğŸ [SERVER_STOP] CTranslate2ã‚µãƒ¼ãƒãƒ¼åœæ­¢")

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="NLLB-200 CTranslate2ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼"
    )
    parser.add_argument(
        "--model",
        default="models/nllb-200-ct2",
        help="CTranslate2ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--device",
        default="cpu",
        choices=["cpu", "cuda", "auto"],
        help="å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹"
    )
    parser.add_argument(
        "--compute-type",
        default="int8",
        choices=["int8", "int16", "float16", "float32"],
        help="è¨ˆç®—å‹"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=5557,
        help="ãƒãƒ¼ãƒˆç•ªå·ï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰"
    )

    args = parser.parse_args()

    # ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–
    server = NLLB200CTranslate2Server(
        model_path=args.model,
        device=args.device,
        compute_type=args.compute_type
    )

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    server.load_model()

    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
    def signal_handler(sig, frame):
        logger.info("ğŸ›‘ [SIGNAL] ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚·ã‚°ãƒŠãƒ«å—ä¿¡")
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    await server.serve_forever()

if __name__ == "__main__":
    asyncio.run(main())