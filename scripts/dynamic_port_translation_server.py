#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dynamic Port NLLB-200 Translation Server
Baketa PythonServerManager compatible version
"""

import argparse
import asyncio
import json
import logging
import signal
import sys
import time
import socket
import os
from threading import Thread
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, Optional

# æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š - Windowsç’°å¢ƒå¯¾å¿œ
if sys.platform.startswith('win'):
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–
class ModelNotLoadedError(Exception):
    pass

class UnsupportedLanguageError(Exception):
    pass

class ModelInferenceError(Exception):
    pass

@dataclass
class TranslationRequest:
    text: str
    source_lang: str = "en"
    target_lang: str = "ja"
    request_id: Optional[str] = None

@dataclass 
class TranslationResponse:
    translation: str
    processing_time: float
    source_lang: str
    target_lang: str
    success: bool = True
    error_message: Optional[str] = None

class DynamicNLLBTranslationServer:
    """Dynamic Port NLLB-200 Translation Server"""
    
    def __init__(self, port: int = 5000):
        self.port = port
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.server_socket = None
        self.running = False
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # NLLB-200è¨€èªã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        self.language_mapping = {
            "en": "eng_Latn",  # English
            "ja": "jpn_Jpan",  # Japanese
            "zh": "zho_Hans",  # Chinese (Simplified)
            "ko": "kor_Hang",  # Korean
            "es": "spa_Latn",  # Spanish
            "fr": "fra_Latn",  # French
            "de": "deu_Latn",  # German
            "ru": "rus_Cyrl",  # Russian
            "auto": "eng_Latn"  # Auto-detect fallback
        }
        
    def _get_nllb_lang_code(self, lang_code: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’NLLB-200å½¢å¼ã«å¤‰æ›"""
        return self.language_mapping.get(lang_code.lower(), "eng_Latn")
        
    async def load_model(self):
        """NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            logger.info(f"Using device: {self.device}")
            logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            logger.info("ğŸš€ NLLB_MODEL_LOAD_START: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
            
            start_time = time.time()
            model_name = "facebook/nllb-200-distilled-600M"
            logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # é«˜é€Ÿãƒ­ãƒ¼ãƒ‰è¨­å®š
            load_kwargs = {
                "torch_dtype": torch.float16 if self.device.type == "cuda" else torch.float32,
                "low_cpu_mem_usage": True,
                "local_files_only": False  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã€å¿…è¦æ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            }
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            logger.info("Tokenizer loading...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, **load_kwargs)
            
            logger.info("Model loading...")
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **load_kwargs)
            self.model.to(self.device)
            
            load_time = time.time() - start_time
            logger.info(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")
            logger.info("ğŸ‰ NLLB_MODEL_LOAD_COMPLETE: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
            await self.warmup()
            logger.info("ğŸ NLLB_MODEL_READY: ã™ã¹ã¦ã®æº–å‚™å®Œäº†")
            
        except Exception as e:
            logger.error(f"Model loading failed: {e}")
            raise ModelNotLoadedError(f"Failed to load NLLB-200 model: {e}")
    
    async def warmup(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        try:
            # è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            logger.info("ğŸ”„ [NLLB-200] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: 'Hello...' [en->ja]")
            warmup_result = await self.translate_text("Hello, how are you?", "en", "ja")
            logger.info(f"è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
            # æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            logger.info("ğŸ”„ [NLLB-200] ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: 'ã“ã‚“ã«ã¡ã¯...' [ja->en]")
            warmup_result = await self.translate_text("ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ", "ja", "en")
            logger.info("æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
        except Exception as e:
            logger.warning(f"Warmup warning (non-critical): {e}")
    
    async def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç¿»è¨³å®Ÿè¡Œ"""
        if not self.model or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")
        
        try:
            # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
            src_lang = self._get_nllb_lang_code(source_lang)
            tgt_lang = self._get_nllb_lang_code(target_lang)
            
            # åŒè¨€èªãƒã‚§ãƒƒã‚¯
            if src_lang == tgt_lang:
                return text  # åŒã˜è¨€èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            
            # è¨€èªè¨­å®š
            self.tokenizer.src_lang = src_lang
            self.tokenizer.tgt_lang = tgt_lang
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨è«–
            with torch.no_grad():
                if self.device.type == "cuda":
                    with torch.cuda.amp.autocast():
                        outputs = self.model.generate(
                            **inputs,
                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                            max_length=512, 
                            num_beams=4, 
                            early_stopping=True
                        )
                else:
                    outputs = self.model.generate(
                        **inputs,
                        forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                        max_length=512, 
                        num_beams=4, 
                        early_stopping=True
                    )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"Translation: '{text[:50]}...' -> '{translation[:50]}...'")
            return translation
            
        except Exception as e:
            logger.error(f"Translation error: {e}")
            raise ModelInferenceError(f"Translation failed: {e}")
    
    def handle_client(self, client_socket):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šå‡¦ç†"""
        try:
            # ãƒ‡ãƒ¼ã‚¿å—ä¿¡
            data = client_socket.recv(4096).decode('utf-8')
            request_data = json.loads(data.strip())
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
            text = request_data.get('text', '')
            source_lang = request_data.get('source_lang', 'en') 
            target_lang = request_data.get('target_lang', 'ja')
            
            logger.info(f"Processing: '{text[:50]}...' [{source_lang}->{target_lang}]")
            
            # éåŒæœŸç¿»è¨³å®Ÿè¡Œ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            start_time = time.time()
            try:
                translation = loop.run_until_complete(
                    self.translate_text(text, source_lang, target_lang)
                )
                processing_time = time.time() - start_time
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
                response = {
                    "success": True,
                    "translation": translation,
                    "processing_time": processing_time,
                    "source_lang": source_lang,
                    "target_lang": target_lang
                }
                
            except Exception as e:
                processing_time = time.time() - start_time
                logger.error(f"Translation failed: {e}")
                response = {
                    "success": False,
                    "error": str(e),
                    "processing_time": processing_time,
                    "source_lang": source_lang,
                    "target_lang": target_lang
                }
            finally:
                loop.close()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹é€ä¿¡
            response_json = json.dumps(response, ensure_ascii=False) + '\n'
            client_socket.sendall(response_json.encode('utf-8'))
            
        except Exception as e:
            logger.error(f"Client handling error: {e}")
            error_response = {
                "success": False,
                "error": f"Server error: {e}"
            }
            try:
                response_json = json.dumps(error_response, ensure_ascii=False) + '\n'
                client_socket.sendall(response_json.encode('utf-8'))
            except:
                pass
        finally:
            client_socket.close()
    
    async def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            await self.load_model()
            
            # TCP ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.server_socket.bind(('127.0.0.1', self.port))
            self.server_socket.listen(5)
            
            self.running = True
            logger.info(f"NLLB-200 Translation Server listening on 127.0.0.1:{self.port}")
            
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šå¾…æ©Ÿ
            while self.running:
                try:
                    client_socket, address = self.server_socket.accept()
                    logger.debug(f"Client connected: {address}")
                    
                    # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§å‡¦ç†
                    self.executor.submit(self.handle_client, client_socket)
                    
                except Exception as e:
                    if self.running:
                        logger.error(f"Accept error: {e}")
                        
        except Exception as e:
            logger.error(f"Server start error: {e}")
            raise
    
    def stop_server(self):
        """ã‚µãƒ¼ãƒãƒ¼åœæ­¢"""
        logger.info("Stopping translation server...")
        self.running = False
        if self.server_socket:
            self.server_socket.close()
        self.executor.shutdown(wait=True)

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
async def main():
    parser = argparse.ArgumentParser(description='Dynamic Port NLLB-200 Translation Server')
    parser.add_argument('--port', type=int, default=5000, help='Server port (default: 5000)')
    args = parser.parse_args()
    
    server = DynamicNLLBTranslationServer(port=args.port)
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
    def signal_handler(sig, frame):
        logger.info("Received shutdown signal")
        server.stop_server()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        await server.start_server()
    except KeyboardInterrupt:
        logger.info("Server shutdown requested")
    finally:
        server.stop_server()

if __name__ == "__main__":
    asyncio.run(main())