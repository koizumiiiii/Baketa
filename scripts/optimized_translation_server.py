#!/usr/bin/env python3
"""
æœ€é©åŒ–ã•ã‚ŒãŸé«˜é€Ÿç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ï¼ˆç›®æ¨™: 500msä»¥ä¸‹ï¼‰
æ°¸ç¶šåŒ–ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦å‹•ä½œã—ã€TCPçµŒç”±ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
"""

import asyncio
import json
import logging
import signal
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import argparse

import torch
from transformers import MarianMTModel, MarianTokenizer, AutoModelForSeq2SeqLM, AutoTokenizer

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TranslationRequest:
    """ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    text: str
    source_lang: str
    target_lang: str
    request_id: Optional[str] = None

@dataclass
class TranslationResponse:
    """ç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translation: Optional[str] = None
    confidence: float = 0.0
    error: Optional[str] = None
    processing_time: float = 0.0

@dataclass
class BatchTranslationRequest:
    """ãƒãƒƒãƒç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    texts: List[str]
    source_lang: str
    target_lang: str
    batch_mode: bool = True
    max_batch_size: int = 50

@dataclass
class BatchTranslationResponse:
    """ãƒãƒƒãƒç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translations: List[str]
    confidence_scores: List[float]
    processing_time: float
    batch_size: int
    errors: Optional[List[str]] = None

class OptimizedTranslationServer:
    """æœ€é©åŒ–ã•ã‚ŒãŸç¿»è¨³ã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self, port: int = 5555):
        self.port = port
        self.models: Dict[str, Tuple[MarianMTModel, MarianTokenizer]] = {}
        self.executor = ThreadPoolExecutor(max_workers=4)
        # ğŸš¨ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Œå…¨ç„¡åŠ¹åŒ– - æ±šæŸ“å•é¡Œè§£æ±ºã®ãŸã‚
        # self.cache: Dict[str, str] = {}
        # self.max_cache_size = 1000
        self.request_count = 0
        self.total_processing_time = 0.0
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # ğŸ§¹ PyTorãƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨­å®š
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        
    def load_models(self):
        """ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        logger.info("ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        logger.info("ğŸš€ MODEL_LOAD_START: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        start_time = time.time()
        
        # æ—¥æœ¬èªâ†’è‹±èªãƒ¢ãƒ‡ãƒ«
        try:
            model_name_ja_en = "Helsinki-NLP/opus-mt-ja-en"
            tokenizer_ja_en = MarianTokenizer.from_pretrained(model_name_ja_en)
            model_ja_en = MarianMTModel.from_pretrained(model_name_ja_en).to(self.device)
            model_ja_en.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.models["ja-en"] = (model_ja_en, tokenizer_ja_en)
            logger.info("æ—¥æœ¬èªâ†’è‹±èªãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            logger.error(f"æ—¥æœ¬èªâ†’è‹±èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            
        # è‹±èªâ†’æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ï¼ˆNLLB-200ã«å¤‰æ›´ - Helsinki-NLPæ±šæŸ“å•é¡Œå›é¿ï¼‰
        try:
            model_name_en_ja = "facebook/nllb-200-distilled-600M"
            logger.info(f"ğŸ”„ [MODEL_UPGRADE] Helsinki-NLPä»£æ›¿: {model_name_en_ja}ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
            tokenizer_en_ja = AutoTokenizer.from_pretrained(model_name_en_ja)
            model_en_ja = AutoModelForSeq2SeqLM.from_pretrained(model_name_en_ja).to(self.device)
            model_en_ja.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.models["en-ja"] = (model_en_ja, tokenizer_en_ja)
            logger.info("âœ… è‹±èªâ†’æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ï¼ˆNLLB-200ï¼‰ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ±šæŸ“å•é¡Œè§£æ±º")
        except Exception as e:
            logger.error(f"âŒ è‹±èªâ†’æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ï¼ˆNLLB-200ï¼‰ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦å¾“æ¥ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
            try:
                logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Helsinki-NLPãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ")
                model_name_en_ja_fallback = "Helsinki-NLP/opus-mt-en-jap"
                tokenizer_en_ja = MarianTokenizer.from_pretrained(model_name_en_ja_fallback)
                model_en_ja = MarianMTModel.from_pretrained(model_name_en_ja_fallback).to(self.device)
                model_en_ja.eval()
                self.models["en-ja"] = (model_en_ja, tokenizer_en_ja)
                logger.warning("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: Helsinki-NLPãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼ˆæ±šæŸ“ãƒªã‚¹ã‚¯ã‚ã‚Šï¼‰")
            except Exception as fallback_error:
                logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {fallback_error}")
            
        load_time = time.time() - start_time
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")
        logger.info("ğŸ‰ MODEL_LOAD_COMPLETE: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä»˜é–‹å§‹")
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›æ¨è«–ã®é…å»¶ã‚’å›é¿ï¼‰
        self._warmup_models()
        
        # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
        total_time = time.time() - start_time
        logger.info("ğŸ MODEL_READY: ã™ã¹ã¦ã®åˆæœŸåŒ–å®Œäº† - ç·æ™‚é–“: {:.2f}ç§’".format(total_time))
        
    def _warmup_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        logger.info("ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        # æ—¥æœ¬èªâ†’è‹±èª
        if "ja-en" in self.models:
            try:
                self._translate_text("ã“ã‚“ã«ã¡ã¯", "ja", "en")
                logger.info("æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            except Exception as e:
                logger.warning(f"æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                
        # è‹±èªâ†’æ—¥æœ¬èª
        if "en-ja" in self.models:
            try:
                self._translate_text("Hello", "en", "ja")
                logger.info("è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            except Exception as e:
                logger.warning(f"è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
                
    def _get_model_key(self, source_lang: str, target_lang: str) -> str:
        """è¨€èªãƒšã‚¢ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ã‚’å–å¾—"""
        # è¨€èªã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
        source = source_lang.lower()[:2]  # "ja", "en"
        target = target_lang.lower()[:2]
        
        if source == "ja" and target == "en":
            return "ja-en"
        elif source == "en" and target == "ja":
            return "en-ja"
        else:
            raise ValueError(f"Unsupported language pair: {source_lang} -> {target_lang}")
    
    def _cleanup_model_state_before_request(self, model):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # PyTorchãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºå®Ÿã«è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            model.eval()
            
            # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–ï¼ˆå¿µã®ãŸã‚ï¼‰
            for param in model.parameters():
                param.grad = None
                
            logger.debug("Pre-request model state cleanup completed")
            
        except Exception as e:
            logger.warning(f"Pre-request cleanup error: {e}")
    
    def _cleanup_model_state_after_request(self, model, inputs_tensors=None):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¾Œã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ¡ãƒ¢ãƒªè§£æ”¾
            if inputs_tensors:
                for key, tensor in inputs_tensors.items():
                    if hasattr(tensor, 'data'):
                        tensor.data = tensor.data.detach()
                del inputs_tensors
            
            # PyTorchã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆè»½é‡ï¼‰
            import gc
            gc.collect()
            
            logger.debug("Post-request model state cleanup completed")
            
        except Exception as e:
            logger.warning(f"Post-request cleanup error: {e}")
    
    async def _force_model_state_reset(self):
        """å¼·åˆ¶çš„ãªãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹å®Œå…¨ãƒªã‚»ãƒƒãƒˆï¼ˆæ¥ç¶šãƒ—ãƒ¼ãƒ«å¯¾å¿œï¼‰"""
        try:
            logger.debug("ğŸ”„ FORCE MODEL STATE RESET: æ¥ç¶šãƒ—ãƒ¼ãƒ«æ±šæŸ“å¯¾ç­–")
            
            # ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆå®Ÿè¡Œ
            for model_key, (model, tokenizer) in self.models.items():
                # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰å¼·åˆ¶è¨­å®š
                model.eval()
                
                # å‹¾é…æƒ…å ±å®Œå…¨ã‚¯ãƒªã‚¢
                for param in model.parameters():
                    param.grad = None
                
                # ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆã‚ã‚Œã°ï¼‰
                if hasattr(model, 'clear_cache'):
                    model.clear_cache()
            
            # PyTorchå…¨ä½“ã®çŠ¶æ…‹ã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()
            
            logger.debug("âœ… Force model state reset completed")
            
        except Exception as e:
            logger.error(f"âŒ Force model state reset failed: {e}")
            
    def _translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰- NLLB-200å¯¾å¿œç‰ˆ"""
        model_key = self._get_model_key(source_lang, target_lang)
        
        if model_key not in self.models:
            raise ValueError(f"Model not loaded for {model_key}")
            
        model, tokenizer = self.models[model_key]
        
        # ğŸ§¹ PRE-REQUEST STATE CLEANUP - ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰çŠ¶æ…‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._cleanup_model_state_before_request(model)
        
        try:
            # ğŸ†• NLLB-200ãƒ¢ãƒ‡ãƒ«åˆ¤å®šã¨BCP-47è¨€èªã‚³ãƒ¼ãƒ‰ä½¿ç”¨
            is_nllb_model = "nllb" in str(type(tokenizer)).lower() or hasattr(tokenizer, 'lang_code_to_id')
            
            if is_nllb_model and model_key == "en-ja":
                # NLLB-200å°‚ç”¨å‡¦ç†ï¼šBCP-47è¨€èªã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                logger.info(f"ğŸŒ [NLLB-200] é«˜å“è³ªç¿»è¨³å®Ÿè¡Œ: '{text[:30]}...' (eng_Latn -> jpn_Jpan)")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆNLLB-200ã¯ç‰¹åˆ¥ãªå‡¦ç†ãŒå¿…è¦ï¼‰
                inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # NLLB-200ã®å ´åˆã€targetè¨€èªã®BOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¼·åˆ¶
                target_lang_bos_id = tokenizer.convert_tokens_to_ids("jpn_Jpan")
                
                # æ¨è«–å®Ÿè¡Œ
                with torch.no_grad():
                    if self.device.type == "cuda":
                        with torch.cuda.amp.autocast():
                            outputs = model.generate(
                                **inputs, 
                                max_length=512, 
                                num_beams=4,  # NLLB-200ã§ã¯å“è³ªå‘ä¸Šã®ãŸã‚beam_searchã‚’ä½¿ç”¨
                                early_stopping=True,
                                forced_bos_token_id=target_lang_bos_id
                            )
                    else:
                        outputs = model.generate(
                            **inputs, 
                            max_length=512, 
                            num_beams=4, 
                            early_stopping=True,
                            forced_bos_token_id=target_lang_bos_id
                        )
                
                # ãƒ‡ã‚³ãƒ¼ãƒ‰
                translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
                logger.info(f"âœ¨ [NLLB-200] é«˜å“è³ªç¿»è¨³å®Œäº†: '{translation[:50]}...'")
                
            else:
                # å¾“æ¥ã®MarianMTãƒ¢ãƒ‡ãƒ«å‡¦ç†
                logger.info(f"ğŸ”„ [MarianMT] å¾“æ¥ãƒ¢ãƒ‡ãƒ«ç¿»è¨³: '{text[:30]}...'")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
                inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # æ¨è«–ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚no_gradã¨halfç²¾åº¦ã‚’ä½¿ç”¨ï¼‰
                with torch.no_grad():
                    if self.device.type == "cuda":
                        with torch.cuda.amp.autocast():
                            outputs = model.generate(**inputs, max_length=512, num_beams=1, early_stopping=True)
                    else:
                        outputs = model.generate(**inputs, max_length=512, num_beams=1, early_stopping=True)
                        
                # ãƒ‡ã‚³ãƒ¼ãƒ‰
                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ‡ãƒãƒƒã‚°: ç¿»è¨³çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"Translation result: '{translation}' (type: {type(translation)})")
            logger.info(f"Translation bytes: {translation.encode('utf-8')}")
            
            return translation
            
        finally:
            # ğŸ§¹ POST-REQUEST STATE CLEANUP - ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¾ŒçŠ¶æ…‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_model_state_after_request(model, inputs if 'inputs' in locals() else None)
        
    async def translate(self, request: TranslationRequest) -> TranslationResponse:
        """éåŒæœŸç¿»è¨³å‡¦ç†"""
        start_time = time.time()
        
        try:
            # ğŸš¨ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½å®Œå…¨ç„¡åŠ¹åŒ– - æ±šæŸ“å•é¡Œæ ¹æœ¬è§£æ±º
            logger.info(f"ğŸš€ [NO_CACHE] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—æ–°é®®ç¿»è¨³å®Ÿè¡Œ: '{request.text[:30]}...'")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é–¢é€£ã®ã‚³ãƒ¼ãƒ‰ã‚’ã™ã¹ã¦ç„¡åŠ¹åŒ–
                
            # ç¿»è¨³å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰
            loop = asyncio.get_event_loop()
            translation = await loop.run_in_executor(
                self.executor,
                self._translate_text,
                request.text,
                request.source_lang,
                request.target_lang
            )
            
            # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½å®Œå…¨ç„¡åŠ¹åŒ–ã«ã‚ˆã‚Šæ±šæŸ“å•é¡Œè§£æ±º
            logger.info(f"âœ… [TRANSLATION_SUCCESS] æ–°é®®ãªç¿»è¨³å®Œäº†: '{request.text[:30]}...' -> '{translation[:30]}...'")
            
            
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += 1
            self.total_processing_time += processing_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Š
            if processing_time > 500:
                logger.warning(f"Translation exceeded 500ms target: {processing_time:.1f}ms")
            else:
                logger.info(f"Fast translation completed: {processing_time:.1f}ms")
                
            return TranslationResponse(
                success=True,
                translation=translation,
                confidence=0.95,
                processing_time=processing_time / 1000.0
            )
            
        except Exception as e:
            logger.error(f"Translation error: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                processing_time=processing_time / 1000.0
            )

    async def translate_batch(self, request: BatchTranslationRequest) -> BatchTranslationResponse:
        """ãƒãƒƒãƒç¿»è¨³å‡¦ç† - è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§åŠ¹ç‡çš„ã«å‡¦ç†"""
        start_time = time.time()
        
        try:
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™
            if len(request.texts) > request.max_batch_size:
                raise ValueError(f"Batch size {len(request.texts)} exceeds limit {request.max_batch_size}")
            
            # ãƒ¢ãƒ‡ãƒ«å–å¾—
            model_key = self._get_model_key(request.source_lang, request.target_lang)
            if model_key not in self.models:
                raise ValueError(f"Model not loaded for {model_key}")
                
            model, tokenizer = self.models[model_key]
            
            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆåŠ¹ç‡åŒ–ï¼‰
            inputs = tokenizer(
                request.texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ãƒãƒƒãƒæ¨è«–å®Ÿè¡Œï¼ˆGPUæœ€é©åŒ–ï¼‰
            loop = asyncio.get_event_loop()
            translations = await loop.run_in_executor(
                self.executor,
                self._batch_inference,
                model, tokenizer, inputs
            )
            
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆç¾åœ¨ã¯å›ºå®šå€¤ã€å°†æ¥çš„ã«logitsã‹ã‚‰è¨ˆç®—äºˆå®šï¼‰
            confidence_scores = [0.95] * len(request.texts)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += len(request.texts)
            self.total_processing_time += processing_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ãƒ­ã‚°
            avg_time_per_text = processing_time / len(request.texts)
            if avg_time_per_text > 100:
                logger.warning(f"Batch translation exceeded 100ms/text target: {avg_time_per_text:.1f}ms/text")
            else:
                logger.info(f"Fast batch translation completed: {avg_time_per_text:.1f}ms/text, batch size: {len(request.texts)}")
            
            return BatchTranslationResponse(
                success=True,
                translations=translations,
                confidence_scores=confidence_scores,
                processing_time=processing_time / 1000.0,  # seconds
                batch_size=len(request.texts)
            )
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Batch translation error: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=[str(e)]
            )

    def _batch_inference(self, model, tokenizer, inputs):
        """ãƒãƒƒãƒæ¨è«–å‡¦ç†ï¼ˆåŒæœŸå‡¦ç†ã§ThreadPoolExecutorã§å®Ÿè¡Œï¼‰- çŠ¶æ…‹ç®¡ç†ä¿®æ­£ç‰ˆ"""
        # ğŸ§¹ PRE-BATCH STATE CLEANUP - ãƒãƒƒãƒå‰çŠ¶æ…‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._cleanup_model_state_before_request(model)
        
        try:
            with torch.no_grad():
                if self.device.type == "cuda":
                    with torch.cuda.amp.autocast():
                        outputs = model.generate(
                            **inputs, 
                            max_length=512, 
                            num_beams=1, 
                            early_stopping=True
                        )
                else:
                    outputs = model.generate(
                        **inputs, 
                        max_length=512, 
                        num_beams=1, 
                        early_stopping=True
                    )
            
            # ãƒãƒƒãƒãƒ‡ã‚³ãƒ¼ãƒ‰
            translations = []
            for output in outputs:
                translation = tokenizer.decode(output, skip_special_tokens=True)
                translations.append(translation)
                
            return translations
            
        finally:
            # ğŸ§¹ POST-BATCH STATE CLEANUP - ãƒãƒƒãƒå¾ŒçŠ¶æ…‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_model_state_after_request(model, inputs)
            
    async def handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šå‡¦ç†"""
        client_addr = writer.get_extra_info('peername')
        logger.info(f"Client connected: {client_addr}")
        
        try:
            while True:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡
                data = await reader.readline()
                if not data:
                    break
                
                # ğŸ§¹ CRITICAL: å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã«å®Œå…¨ãªçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
                await self._force_model_state_reset()
                    
                try:
                    # JSONãƒ‘ãƒ¼ã‚¹
                    request_data = json.loads(data.decode('utf-8'))
                    
                    # Pingãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¤å®šï¼ˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
                    if 'ping' in request_data:
                        ping_response = {
                            'success': True,
                            'pong': True,
                            'status': 'ready',
                            'processing_time': 0.001
                        }
                        response_json = json.dumps(ping_response, ensure_ascii=False) + '\n'
                        writer.write(response_json.encode('utf-8'))
                        await writer.drain()
                        continue
                    
                    # ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š
                    elif 'texts' in request_data and request_data.get('batch_mode', False):
                        # ãƒãƒƒãƒç¿»è¨³å‡¦ç†
                        batch_request = BatchTranslationRequest(
                            texts=request_data['texts'],
                            source_lang=request_data.get('source_lang', 'ja'),
                            target_lang=request_data.get('target_lang', 'en'),
                            batch_mode=request_data.get('batch_mode', True),
                            max_batch_size=request_data.get('max_batch_size', 50)
                        )
                        
                        batch_response = await self.translate_batch(batch_request)
                        
                        response_data = {
                            'success': batch_response.success,
                            'translations': batch_response.translations,
                            'confidence_scores': batch_response.confidence_scores,
                            'processing_time': batch_response.processing_time,
                            'batch_size': batch_response.batch_size,
                            'errors': batch_response.errors
                        }
                    else:
                        # å˜ä¸€ç¿»è¨³å‡¦ç†ï¼ˆå¾“æ¥ã®å‡¦ç†ï¼‰
                        request = TranslationRequest(
                            text=request_data['text'],
                            source_lang=request_data.get('source_lang', 'ja'),
                            target_lang=request_data.get('target_lang', 'en'),
                            request_id=request_data.get('request_id')
                        )
                        
                        response = await self.translate(request)
                        
                        response_data = {
                            'success': response.success,
                            'translation': response.translation,
                            'confidence': response.confidence,
                            'error': response.error,
                            'processing_time': response.processing_time
                        }
                    
                    response_json = json.dumps(response_data, ensure_ascii=False) + '\n'
                    writer.write(response_json.encode('utf-8'))
                    await writer.drain()
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': 'Invalid JSON format'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
                except Exception as e:
                    logger.error(f"Request processing error: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': str(e)
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(f"Client handler error: {e}")
        finally:
            writer.close()
            await writer.wait_closed()
            logger.info(f"Client disconnected: {client_addr}")
            
    async def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        server = await asyncio.start_server(
            self.handle_client,
            '127.0.0.1',
            self.port
        )
        
        addr = server.sockets[0].getsockname()
        logger.info(f"Optimized Translation Server listening on {addr[0]}:{addr[1]}")
        
        # çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«å‡ºåŠ›
        asyncio.create_task(self._print_stats())
        
        async with server:
            await server.serve_forever()
            
    async def _print_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«å‡ºåŠ›"""
        while True:
            await asyncio.sleep(60)  # 1åˆ†ã”ã¨
            if self.request_count > 0:
                avg_time = self.total_processing_time / self.request_count
                logger.info(f"Stats - Requests: {self.request_count}, Avg time: {avg_time:.1f}ms, State management: Active")
                
    def shutdown(self, signum, frame):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
        logger.info("Shutting down server...")
        self.executor.shutdown(wait=True)
        sys.exit(0)

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(description='Optimized Translation Server')
    parser.add_argument('--port', type=int, default=5555, help='Server port')
    parser.add_argument('--optimized', action='store_true', help='Enable optimizations')
    args = parser.parse_args()
    
    # ã‚µãƒ¼ãƒãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    server = OptimizedTranslationServer(port=args.port)
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
    signal.signal(signal.SIGINT, server.shutdown)
    signal.signal(signal.SIGTERM, server.shutdown)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    server.load_models()
    
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    try:
        asyncio.run(server.start_server())
    except KeyboardInterrupt:
        logger.info("Server stopped by user")

if __name__ == "__main__":
    main()