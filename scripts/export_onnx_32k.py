#!/usr/bin/env python3
"""
Helsinki-NLP/opus-mt-ja-enãƒ¢ãƒ‡ãƒ«ã‚’32,000èªå½™ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from optimum.onnxruntime import ORTModelForSeq2SeqLM

def export_to_onnx_32k():
    """
    Helsinki-NLP/opus-mt-ja-enã‚’ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    """
    # è¨­å®š
    model_id = "Helsinki-NLP/opus-mt-ja-en"
    base_dir = Path(__file__).parent.parent  # E:\dev\Baketa
    output_dir = base_dir / "Models" / "ONNX_32k"
    
    print(f"=== OPUS-MT ONNX Export (32kèªå½™å¯¾å¿œ) ===")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model_id}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    
    try:
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\n1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        print(f"   èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}")
        
        print(f"\n2. PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
        # export=Trueã§è‡ªå‹•çš„ã«ONNXå½¢å¼ã«å¤‰æ›
        model = ORTModelForSeq2SeqLM.from_pretrained(model_id, export=True)
        
        print(f"\n3. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜ä¸­...")
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†!")
        print(f"ä¿å­˜å…ˆ: {output_dir}")
        print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        for item in sorted(output_dir.iterdir()):
            if item.is_file():
                size_mb = item.stat().st_size / (1024 * 1024)
                print(f"   - {item.name} ({size_mb:.1f} MB)")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±ã‚’ç¢ºèª
        print(f"\nğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±:")
        print(f"   - èªå½™ã‚µã‚¤ã‚º: {tokenizer.vocab_size:,}")
        print(f"   - BOS Token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
        print(f"   - EOS Token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        print(f"   - UNK Token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
        print(f"   - PAD Token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
        
        # ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        test_text = "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼"
        tokens = tokenizer.encode(test_text)
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–:")
        print(f"   - å…¥åŠ›: '{test_text}'")
        print(f"   - ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
        print(f"   - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ID: {max(tokens) if tokens else 0}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
    print("  pip install transformers optimum[onnxruntime] sentencepiece torch")
    print()
    
    success = export_to_onnx_32k()
    if success:
        print("\nğŸ‰ ONNXå¤‰æ›ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        print("Baketa.Infrastructure.Translation.Local.Onnx.AlphaOpusMtTranslationEngineã§")
        print("Models/ONNX_32k/encoder_model.onnxã¨decoder_model.onnxã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
    else:
        print("\nğŸ’¥ ONNXå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)