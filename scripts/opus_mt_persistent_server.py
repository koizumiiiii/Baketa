#!/usr/bin/env python3
"""
Baketa OPUS-MT Persistent Translation Server - Issue #147 Phase 1 Optimized
å¸¸é§å‹é«˜é€Ÿç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ - Issue #147 Phase 1: å‹•çš„ãƒ“ãƒ¼ãƒ èª¿æ•´ãƒ»FP16é‡å­åŒ–ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
"""

import sys
import json
import os
import warnings
import socket
import threading
import time
import hashlib
from datetime import datetime, timedelta
import traceback

# è­¦å‘Šã‚’æŠ‘åˆ¶ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
warnings.filterwarnings("ignore", category=UserWarning)
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # tokenizerã®ä¸¦åˆ—å‡¦ç†ã‚’ç„¡åŠ¹åŒ–

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, logging
import torch

# ğŸš€ Issue #147 Phase 1: TTLCache for translation caching
try:
    from cachetools import TTLCache
    CACHE_AVAILABLE = True
    print("âœ… TTLCache available for translation caching", file=sys.stderr, flush=True)
except ImportError:
    CACHE_AVAILABLE = False
    print("âš ï¸ TTLCache not available - caching disabled", file=sys.stderr, flush=True)

# transformersã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ä¸‹ã’ã‚‹
logging.set_verbosity_error()

# ğŸ”„ å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠ: è¨€èªæ–¹å‘ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«IDã‚’æ±ºå®š
MODELS = {
    "ja-en": "Helsinki-NLP/opus-mt-ja-en",  # æ—¥â†’è‹±
    "en-ja": "Helsinki-NLP/opus-mt-en-jap"  # è‹±â†’æ—¥ï¼ˆæ­£å¼åç§°: "jap"ï¼‰
}
SERVER_HOST = "127.0.0.1"
SERVER_PORT = 5556  # Baketaå°‚ç”¨ãƒãƒ¼ãƒˆï¼ˆãƒãƒ¼ãƒˆ5555ãŒä½¿ç”¨ä¸­ã®ãŸã‚5556ã‚’ä½¿ç”¨ï¼‰

class PersistentOpusMtServer:
    """
    OPUS-MTå¸¸é§ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ - Issue #147 Phase 1 Optimized
    - 1å›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã§é«˜é€Ÿç¿»è¨³ã‚’ç¶™ç¶šæä¾›
    - TCP Socketé€šä¿¡ã§C#ã¨é€£æº
    - ãƒ—ãƒ­ã‚»ã‚¹æ­»æ´»ç›£è¦–å¯¾å¿œ
    - ğŸš€ Issue #147 Phase 1: å‹•çš„ãƒ“ãƒ¼ãƒ èª¿æ•´ãƒ»FP16é‡å­åŒ–ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        # ğŸ”„ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«åŒæ™‚ã‚µãƒãƒ¼ãƒˆ: æ—¥â†’è‹±ã€è‹±â†’æ—¥ã®ä¸¡æ–¹å‘
        self.tokenizers = {}  # {"ja-en": tokenizer, "en-ja": tokenizer}
        self.models = {}      # {"ja-en": model, "en-ja": model}
        self.initialized = False
        self.server_socket = None
        self.running = False
        self.translation_count = 0
        self.cache_hits = 0
        self.start_time = datetime.now()
        
        # ğŸš€ Issue #147 Phase 1: GPUè‡ªå‹•æ¤œå‡ºãƒ»FP16æœ€é©åŒ–
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_fp16 = torch.cuda.is_available()  # GPUåˆ©ç”¨æ™‚ã®ã¿FP16ã‚’æœ‰åŠ¹åŒ–
        
        print(f"ğŸ® [{datetime.now().strftime('%H:%M:%S')}] Device selected: {self.device}", file=sys.stderr, flush=True)
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ”¥ [{datetime.now().strftime('%H:%M:%S')}] GPU detected: {gpu_name} ({vram_gb:.1f}GB VRAM)", file=sys.stderr, flush=True)
            print(f"âš¡ [{datetime.now().strftime('%H:%M:%S')}] FP16 optimization enabled for 2x speedup", file=sys.stderr, flush=True)
        else:
            print(f"ğŸ–¥ï¸ [{datetime.now().strftime('%H:%M:%S')}] Using CPU inference", file=sys.stderr, flush=True)
        
        # ğŸš€ Issue #147 Phase 1: ç¿»è¨³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  (TTL: 1æ™‚é–“, æœ€å¤§1000ã‚¨ãƒ³ãƒˆãƒª)
        if CACHE_AVAILABLE:
            self.translation_cache = TTLCache(maxsize=1000, ttl=3600)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            print(f"ğŸ’¾ [{datetime.now().strftime('%H:%M:%S')}] Translation cache initialized (TTL: 3600s, Size: 1000)", file=sys.stderr, flush=True)
        else:
            self.translation_cache = {}
            print(f"ğŸ’¾ [{datetime.now().strftime('%H:%M:%S')}] Simple dictionary cache fallback enabled", file=sys.stderr, flush=True)
        
    def initialize_models(self):
        """
        ğŸ”„ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«åŒæ™‚åˆæœŸåŒ–: æ—¥â†’è‹±ã€è‹±â†’æ—¥ã®ä¸¡æ–¹å‘ã‚µãƒãƒ¼ãƒˆ
        ğŸš€ Issue #147 Phase 1: GPUè»¢é€ãƒ»FP16é‡å­åŒ–å¯¾å¿œ
        """
        try:
            for direction, model_id in MODELS.items():
                print(f"ğŸ”„ [{datetime.now().strftime('%H:%M:%S')}] Loading tokenizer for {direction} ({model_id})...", 
                      file=sys.stderr, flush=True)
                self.tokenizers[direction] = AutoTokenizer.from_pretrained(model_id, local_files_only=False)
                
                print(f"ğŸ”„ [{datetime.now().strftime('%H:%M:%S')}] Loading model for {direction} ({model_id})...", 
                      file=sys.stderr, flush=True)
                
                # ğŸš€ Issue #147 Phase 1: FP16é‡å­åŒ–ã¨GPUè»¢é€
                if self.use_fp16:
                    # GPU + FP16 for maximum speed
                    model = AutoModelForSeq2SeqLM.from_pretrained(
                        model_id, 
                        local_files_only=False,
                        torch_dtype=torch.float16  # ğŸ”¥ 2x speedup + 50% memory reduction
                    )
                    model = model.to(self.device)  # GPUè»¢é€
                    print(f"âš¡ [{datetime.now().strftime('%H:%M:%S')}] Model {direction} loaded with FP16 GPU acceleration", 
                          file=sys.stderr, flush=True)
                else:
                    # CPU fallback
                    model = AutoModelForSeq2SeqLM.from_pretrained(model_id, local_files_only=False)
                    model = model.to(self.device)  # CPUè»¢é€
                    print(f"ğŸ–¥ï¸ [{datetime.now().strftime('%H:%M:%S')}] Model {direction} loaded for CPU inference", 
                          file=sys.stderr, flush=True)
                
                self.models[direction] = model
                
                print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] Model {direction} optimization complete!", 
                      file=sys.stderr, flush=True)
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] All models initialization complete! Ready for optimized translation.", 
                  file=sys.stderr, flush=True)
            self.initialized = True
            return True
        except Exception as e:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] Models initialization failed: {e}", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ“„ Traceback:\n{traceback.format_exc()}", file=sys.stderr, flush=True)
            return False
    
    def select_beam_strategy(self, text_length):
        """
        ğŸš€ Issue #147 Phase 1: å‹•çš„ãƒ“ãƒ¼ãƒ æ•°èª¿æ•´æˆ¦ç•¥
        ãƒ†ã‚­ã‚¹ãƒˆé•·ã«å¿œã˜ã¦æœ€é©ãªãƒ“ãƒ¼ãƒ æ•°ã‚’é¸æŠã—ã€çŸ­æ–‡ã§å¤§å¹…é«˜é€ŸåŒ–
        """
        if text_length <= 30:
            return 1    # çŸ­æ–‡: 3-5å€é«˜é€ŸåŒ–ã€å“è³ªã¯å®Ÿç”¨çš„
        elif text_length <= 100:
            return 2    # ä¸­æ–‡: 2-3å€é«˜é€ŸåŒ–ã€è‰¯å¥½ãªå“è³ª
        else:
            return 3    # é•·æ–‡: ç¾è¡Œå“è³ªç¶­æŒ
    
    def get_cache_key(self, text, direction):
        """
        ğŸš€ Issue #147 Phase 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
        """
        combined = f"{direction}:{text}"
        return hashlib.md5(combined.encode('utf-8')).hexdigest()
    
    def translate(self, text, direction="ja-en"):
        """
        ğŸ”„ åŒæ–¹å‘ç¿»è¨³ï¼ˆé«˜é€Ÿå®Ÿè¡Œï¼š0.1-0.5ç§’ç›®æ¨™ï¼‰
        ğŸš€ Issue #147 Phase 1: å‹•çš„ãƒ“ãƒ¼ãƒ èª¿æ•´ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
        Args:
            text: ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            direction: ç¿»è¨³æ–¹å‘ ("ja-en" or "en-ja")
        """
        if not self.initialized:
            return {"success": False, "error": "Service not initialized"}
        
        if direction not in self.models:
            return {"success": False, "error": f"Unsupported direction: {direction}"}
        
        start_time = time.time()
        
        # ğŸš€ Issue #147 Phase 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = self.get_cache_key(text, direction)
        if cache_key in self.translation_cache:
            cached_result = self.translation_cache[cache_key]
            self.cache_hits += 1
            processing_time = time.time() - start_time
            
            print(f"ğŸ’¾ [{datetime.now().strftime('%H:%M:%S')}] Cache HIT #{self.cache_hits} [{direction}] in {processing_time:.3f}s: '{text}' â†’ '{cached_result}'", 
                  file=sys.stderr, flush=True)
            
            return {
                "success": True,
                "translation": cached_result,
                "source": text,
                "direction": direction,
                "processing_time": processing_time,
                "translation_count": self.translation_count,
                "cache_hit": True
            }
        
        try:
            # æŒ‡å®šã•ã‚ŒãŸè¨€èªæ–¹å‘ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            tokenizer = self.tokenizers[direction]
            model = self.models[direction]
            
            # ğŸš€ Issue #147 Phase 1: å‹•çš„ãƒ“ãƒ¼ãƒ æ•°èª¿æ•´
            text_length = len(text)
            num_beams = self.select_beam_strategy(text_length)
            
            print(f"ğŸ¯ [{datetime.now().strftime('%H:%M:%S')}] Dynamic beam strategy: length={text_length} â†’ beams={num_beams}", 
                  file=sys.stderr, flush=True)
            
            # HuggingFace Transformersæ¨™æº–å‡¦ç† + GPUå¯¾å¿œ
            inputs = tokenizer(text, return_tensors="pt")
            
            # ğŸš€ Issue #147 Phase 1: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚‚GPUã«è»¢é€
            if self.device.type == 'cuda':
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=128,
                    num_beams=num_beams,  # ğŸ¯ å‹•çš„ãƒ“ãƒ¼ãƒ æ•°
                    length_penalty=1.0,
                    early_stopping=True
                )
            
            translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ğŸš€ Issue #147 Phase 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.translation_cache[cache_key] = translation
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¿½åŠ 
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [DEBUG_DECODE] Raw output tensor: {outputs[0][:10] if len(outputs[0]) > 10 else outputs[0]}", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [DEBUG_DECODE] Translation result: {repr(translation)}", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [DEBUG_DECODE] Translation bytes: {translation.encode('utf-8')}", 
                  file=sys.stderr, flush=True)
            
            processing_time = time.time() - start_time
            self.translation_count += 1
            
            print(f"âš¡ [{datetime.now().strftime('%H:%M:%S')}] Translation #{self.translation_count} [{direction}] beams={num_beams} in {processing_time:.3f}s: '{text}' â†’ '{translation}'", 
                  file=sys.stderr, flush=True)
            
            return {
                "success": True,
                "translation": translation,
                "source": text,
                "direction": direction,
                "processing_time": processing_time,
                "translation_count": self.translation_count,
                "beam_count": num_beams,
                "cache_hit": False
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] Translation failed after {processing_time:.3f}s: {e}", 
                  file=sys.stderr, flush=True)
            return {
                "success": False,
                "error": str(e),
                "source": text,
                "processing_time": processing_time
            }

    def translate_batch(self, texts, direction="ja-en"):
        """
        ğŸ”„ åŒæ–¹å‘ãƒãƒƒãƒç¿»è¨³ï¼šè¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åº¦ã«å‡¦ç†ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        Args:
            texts: ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            direction: ç¿»è¨³æ–¹å‘ ("ja-en" or "en-ja")
        """
        print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] translate_batch method called", 
              file=sys.stderr, flush=True)
        print(f"ğŸ“‹ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Parameters: texts={len(texts) if texts else 0}, direction={direction}", 
              file=sys.stderr, flush=True)
        
        if not self.initialized:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Service not initialized", 
                  file=sys.stderr, flush=True)
            return {"success": False, "error": "Service not initialized"}
        
        if direction not in self.models:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Unsupported direction: {direction}", 
                  file=sys.stderr, flush=True)
            return {"success": False, "error": f"Unsupported direction: {direction}"}
        
        start_time = time.time()
        translations = []
        sources = []
        
        try:
            print(f"ğŸ“¦ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Batch translation [{direction}] started - {len(texts)} texts", 
                  file=sys.stderr, flush=True)
            
            # ãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§ã‚’ãƒ­ã‚°å‡ºåŠ›
            for i, text in enumerate(texts[:5]):  # æœ€åˆã®5å€‹ã¾ã§
                print(f"ğŸ“ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Text[{i}]: '{text[:50]}{'...' if len(text) > 50 else ''}'", 
                      file=sys.stderr, flush=True)
            
            # æŒ‡å®šã•ã‚ŒãŸè¨€èªæ–¹å‘ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            print(f"ğŸ”§ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Getting tokenizer and model for direction: {direction}", 
                  file=sys.stderr, flush=True)
            
            tokenizer = self.tokenizers[direction]
            model = self.models[direction]
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Tokenizer and model obtained successfully", 
                  file=sys.stderr, flush=True)
            
            # ğŸš€ Issue #147 Phase 1: ãƒãƒƒãƒç¿»è¨³ã§ã®å‹•çš„ãƒ“ãƒ¼ãƒ æ•°èª¿æ•´
            avg_length = sum(len(text) for text in texts) / len(texts)
            num_beams = self.select_beam_strategy(int(avg_length))
            
            print(f"ğŸ¯ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Dynamic beam strategy: avg_length={avg_length:.1f} â†’ beams={num_beams}", 
                  file=sys.stderr, flush=True)
            
            # ãƒãƒƒãƒå‡¦ç†ï¼šè¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åº¦ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Starting tokenizer encoding...", 
                  file=sys.stderr, flush=True)
            
            inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
            
            # ğŸš€ Issue #147 Phase 1: ãƒãƒƒãƒå…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚‚GPUã«è»¢é€
            if self.device.type == 'cuda':
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                print(f"ğŸ® [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Batch tensors moved to GPU", 
                      file=sys.stderr, flush=True)
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Tokenizer encoding completed", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ“Š [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Input tensor shape: {inputs.input_ids.shape if hasattr(inputs, 'input_ids') else 'N/A'}", 
                  file=sys.stderr, flush=True)
            
            print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Starting optimized model generation...", 
                  file=sys.stderr, flush=True)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=128,
                    num_beams=num_beams,  # ğŸ¯ å‹•çš„ãƒ“ãƒ¼ãƒ æ•°
                    length_penalty=1.0,
                    early_stopping=True
                )
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Model generation completed!", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ“Š [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Output tensor shape: {outputs.shape if hasattr(outputs, 'shape') else 'N/A'}", 
                  file=sys.stderr, flush=True)
            
            # å„å‡ºåŠ›ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Starting output decoding...", 
                  file=sys.stderr, flush=True)
            
            for i, output in enumerate(outputs):
                print(f"ğŸ”„ [{datetime.now().strftime('%H:%M:%S')}] [BATCH_VERIFICATION] Decoding output #{i+1}...", 
                      file=sys.stderr, flush=True)
                
                translation = tokenizer.decode(output, skip_special_tokens=True)
                translations.append(translation)
                sources.append(texts[i] if i < len(texts) else "")
                
                self.translation_count += 1
                
                print(f"âš¡ [{datetime.now().strftime('%H:%M:%S')}] Batch item #{i+1} [{direction}]: '{texts[i] if i < len(texts) else ''}' â†’ '{translation}'", 
                      file=sys.stderr, flush=True)
            
            processing_time = time.time() - start_time
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] Batch translation completed in {processing_time:.3f}s - {len(translations)} translations", 
                  file=sys.stderr, flush=True)
            
            return {
                "success": True,
                "translations": translations,
                "sources": sources,
                "direction": direction,
                "processing_time": processing_time,
                "translation_count": len(translations)
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] Batch translation failed after {processing_time:.3f}s: {e}", 
                  file=sys.stderr, flush=True)
            return {
                "success": False,
                "error": str(e),
                "sources": texts,
                "processing_time": processing_time
            }
    
    def handle_client(self, client_socket, client_address):
        """
        ğŸš€ [GEMINI_FIX] ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°å‡¦ç†ã«ã‚ˆã‚‹TCPã‚¹ãƒ†ã‚£ãƒƒã‚­ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å•é¡Œè§£æ±º
        """
        print(f"ğŸ”— [{datetime.now().strftime('%H:%M:%S')}] Client connected from {client_address}", 
              file=sys.stderr, flush=True)
        
        buffer = ""  # ğŸš€ [GEMINI_FIX] ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°å¤‰æ•°è¿½åŠ 
        try:
            while self.running:
                print(f"ğŸ“¡ [{datetime.now().strftime('%H:%M:%S')}] [BUFFERING] Waiting for client data...", 
                      file=sys.stderr, flush=True)
                
                # ğŸš€ [GEMINI_FIX] recv()ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                raw_data = client_socket.recv(4096)
                if not raw_data:
                    print(f"âš ï¸ [{datetime.now().strftime('%H:%M:%S')}] [BUFFERING] Empty data received - client disconnected", 
                          file=sys.stderr, flush=True)
                    break
                
                # ğŸ”§ [CRITICAL_ENCODING_FIX] å³å¯†ãªUTF-8ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                try:
                    decoded_data = raw_data.decode('utf-8', errors='strict')
                    buffer += decoded_data
                except UnicodeDecodeError as e:
                    print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_ERROR] UTF-8 decode failed: {e}", 
                          file=sys.stderr, flush=True)
                    print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_ERROR] Raw data: {raw_data}", 
                          file=sys.stderr, flush=True)
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¨ãƒ©ãƒ¼æ–‡å­—ã‚’ç½®æ›
                    decoded_data = raw_data.decode('utf-8', errors='replace')
                    buffer += decoded_data
                
                print(f"ğŸ“¨ [{datetime.now().strftime('%H:%M:%S')}] [BUFFERING] Buffer length: {len(buffer)} chars", 
                      file=sys.stderr, flush=True)
                print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [BUFFERING] Buffer preview: {repr(buffer[:200])}", 
                      file=sys.stderr, flush=True)
                
                # ğŸš€ [GEMINI_FIX] ãƒãƒƒãƒ•ã‚¡å†…ã®æ”¹è¡Œã‚’å‡¦ç†ã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ‡ã‚Šå‡ºã™
                while '\n' in buffer:
                    # æœ€åˆã®æ”¹è¡Œã¾ã§ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦åˆ‡ã‚Šå‡ºã™
                    message, buffer = buffer.split('\n', 1)
                    
                    if not message.strip():
                        continue  # ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—
                    
                    print(f"âœ‚ï¸ [{datetime.now().strftime('%H:%M:%S')}] [BUFFERING] Extracted message: {repr(message[:100])}", 
                          file=sys.stderr, flush=True)
                    
                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
                    self.process_message(message.strip(), client_socket)
                    
        except ConnectionResetError:
            print(f"ğŸ’¥ [{datetime.now().strftime('%H:%M:%S')}] Client closed connection abruptly", 
                  file=sys.stderr, flush=True)
        except Exception as e:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] Client handling error: {e}", 
                  file=sys.stderr, flush=True)
        finally:
            client_socket.close()
            print(f"ğŸ”— [{datetime.now().strftime('%H:%M:%S')}] Client {client_address} disconnected", 
                  file=sys.stderr, flush=True)
    
    def process_message(self, data, client_socket):
        """
        ğŸš€ [GEMINI_FIX] å€‹åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ï¼ˆãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‹ã‚‰åˆ†é›¢ï¼‰
        """
        print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Processing message: {repr(data[:100])}", 
              file=sys.stderr, flush=True)
        
        # ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰å‡¦ç†
        if data == "PING":
            print(f"ğŸ“ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] PING command received", 
                  file=sys.stderr, flush=True)
            # ğŸš€ Issue #147 Phase 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’PINGãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«è¿½åŠ 
            cache_size = len(self.translation_cache) if hasattr(self, 'translation_cache') else 0
            response = {
                "status": "alive", 
                "uptime_seconds": (datetime.now() - self.start_time).total_seconds(), 
                "translation_count": self.translation_count,
                "cache_hits": self.cache_hits,
                "cache_size": cache_size,
                "device": str(self.device),
                "fp16_enabled": self.use_fp16
            }
            client_socket.send((json.dumps(response, ensure_ascii=False) + "\n").encode('utf-8'))
            return
            
        elif data == "SHUTDOWN":
            print(f"ğŸ›‘ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] SHUTDOWN command received", 
                  file=sys.stderr, flush=True)
            self.running = False
            response = {"status": "shutting_down"}
            client_socket.send((json.dumps(response, ensure_ascii=False) + "\n").encode('utf-8'))
            return
        
        # ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
        try:
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Attempting JSON parse...", 
                  file=sys.stderr, flush=True)
            
            request = json.loads(data)
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] JSON parse successful: {type(request)}", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Request keys: {list(request.keys())}", 
                  file=sys.stderr, flush=True)
            
            # ãƒãƒƒãƒç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ãƒã‚§ãƒƒã‚¯
            if "batch_texts" in request:
                texts = request.get("batch_texts", [])
                
                # ğŸš€ [LANGUAGE_DIRECTION_FIX] C#å´ã®source_lang/target_langã‹ã‚‰directionã‚’æ§‹ç¯‰
                source_lang = request.get("source_lang", "ja")
                target_lang = request.get("target_lang", "en")
                direction = f"{source_lang}-{target_lang}"  # en-ja ã¾ãŸã¯ ja-en
                
                print(f"ğŸ“¦ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Batch request detected: {len(texts)} texts, direction={direction}", 
                      file=sys.stderr, flush=True)
                
                if not texts or not any(text.strip() for text in texts):
                    print(f"âš ï¸ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Empty batch texts detected", 
                          file=sys.stderr, flush=True)
                    response = {"success": False, "error": "Empty batch texts"}
                else:
                    print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Starting batch translation...", 
                          file=sys.stderr, flush=True)
                    response = self.translate_batch(texts, direction)
                    print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Batch translation completed: success={response.get('success', False)}", 
                          file=sys.stderr, flush=True)
            
            # å˜ä¸€ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            else:
                text = request.get("text", "")
                
                # ğŸš€ [LANGUAGE_DIRECTION_FIX] C#å´ã®source_lang/target_langã‹ã‚‰directionã‚’æ§‹ç¯‰
                source_lang = request.get("source_lang", "ja")
                target_lang = request.get("target_lang", "en")
                direction = f"{source_lang}-{target_lang}"  # en-ja ã¾ãŸã¯ ja-en
                
                print(f"ğŸŒ [{datetime.now().strftime('%H:%M:%S')}] [LANGUAGE_DIRECTION_FIX] C#â†’Python: source='{source_lang}', target='{target_lang}', direction='{direction}'", 
                      file=sys.stderr, flush=True)
                
                if not text:
                    response = {"success": False, "error": "Empty text"}
                else:
                    # æ”¹è¡Œæ–‡å­—ã‚’å«ã‚€å ´åˆã¯ãƒãƒƒãƒå‡¦ç†ã§å¯¾å¿œ
                    if "\n" in text:
                        # æ”¹è¡Œã§åˆ†å‰²ã—ã€ç©ºè¡Œã‚’é™¤å»
                        text_lines = [line.strip() for line in text.split("\n") if line.strip()]
                        
                        if not text_lines:
                            response = {"success": False, "error": "Empty text after splitting"}
                        elif len(text_lines) == 1:
                            # å®Ÿéš›ã¯1è¡Œã ã£ãŸå ´åˆã¯é€šå¸¸ç¿»è¨³
                            response = self.translate(text_lines[0], direction)
                        else:
                            # è¤‡æ•°è¡Œã®å ´åˆã¯ãƒãƒƒãƒç¿»è¨³ã—ã¦çµåˆ
                            print(f"ğŸ“„ [{datetime.now().strftime('%H:%M:%S')}] Multi-line text detected - splitting into {len(text_lines)} lines", 
                                  file=sys.stderr, flush=True)
                            batch_result = self.translate_batch(text_lines, direction)
                            
                            if batch_result["success"]:
                                # ãƒãƒƒãƒçµæœã‚’æ”¹è¡Œã§çµåˆ
                                combined_translation = "\n".join(batch_result["translations"])
                                response = {
                                    "success": True,
                                    "translation": combined_translation,
                                    "source": text,
                                    "processing_time": batch_result["processing_time"],
                                    "translation_count": batch_result["translation_count"],
                                    "split_lines": len(text_lines)  # ãƒ‡ãƒãƒƒã‚°ç”¨
                                }
                            else:
                                response = batch_result
                    else:
                        # æ”¹è¡Œãªã—ã®é€šå¸¸ç¿»è¨³
                        response = self.translate(text, direction)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹é€ä¿¡
            print(f"ğŸ“¤ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Preparing response...", 
                  file=sys.stderr, flush=True)
            
            # ğŸ”§ [CRITICAL_ENCODING_FIX] UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ˜ç¤ºçš„æŒ‡å®šã¨ãƒ‡ãƒãƒƒã‚°
            response_json = json.dumps(response, ensure_ascii=False, indent=None)
            response_with_newline = response_json + "\n"
            
            # ğŸ” [ENCODING_DEBUG] ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‰ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›
            if "translation" in response and response.get("success"):
                translation_text = response["translation"]
                print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_DEBUG] Translation text type: {type(translation_text)}", 
                      file=sys.stderr, flush=True)
                print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_DEBUG] Translation text: {repr(translation_text)}", 
                      file=sys.stderr, flush=True)
                print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_DEBUG] Translation unicode: {[ord(c) for c in translation_text[:10]]}", 
                      file=sys.stderr, flush=True)
            
            # ğŸ”§ [CRITICAL_ENCODING_FIX] å³å¯†ãªUTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            try:
                response_bytes = response_with_newline.encode('utf-8', errors='strict')
                print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_DEBUG] UTF-8 encoding successful", 
                      file=sys.stderr, flush=True)
            except UnicodeEncodeError as e:
                print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_DEBUG] UTF-8 encoding failed: {e}", 
                      file=sys.stderr, flush=True)
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¨ãƒ©ãƒ¼æ–‡å­—ã‚’ç½®æ›
                response_bytes = response_with_newline.encode('utf-8', errors='replace')
            
            print(f"ğŸ“ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Response size: {len(response_bytes)} bytes", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ“„ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Response preview: {response_with_newline[:200]}...", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ” [{datetime.now().strftime('%H:%M:%S')}] [ENCODING_DEBUG] Response bytes preview: {response_bytes[:100]}", 
                  file=sys.stderr, flush=True)
            
            print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Sending response...", 
                  file=sys.stderr, flush=True)
            
            # ğŸ”§ [CRITICAL_ENCODING_FIX] ç¢ºå®Ÿãªãƒ‡ãƒ¼ã‚¿é€ä¿¡
            client_socket.sendall(response_bytes)
            
            print(f"âœ… [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Response sent successfully", 
                  file=sys.stderr, flush=True)
            
        except json.JSONDecodeError as e:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] JSON decode error: {str(e)}", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ“„ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Problem data: {repr(data)}", 
                  file=sys.stderr, flush=True)
            
            error_response = {"success": False, "error": f"Invalid JSON: {str(e)}"}
            error_json = json.dumps(error_response, ensure_ascii=False) + "\n"
            client_socket.send(error_json.encode('utf-8'))
            
            print(f"ğŸ“¤ [{datetime.now().strftime('%H:%M:%S')}] [MESSAGE_PROC] Error response sent", 
                  file=sys.stderr, flush=True)
    
    def start_server(self):
        """
        TCP socketã‚µãƒ¼ãƒãƒ¼é–‹å§‹
        """
        try:
            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.server_socket.bind((SERVER_HOST, SERVER_PORT))
            self.server_socket.listen(5)
            self.running = True
            
            print(f"ğŸš€ [{datetime.now().strftime('%H:%M:%S')}] OPUS-MT Persistent Server started on {SERVER_HOST}:{SERVER_PORT}", 
                  file=sys.stderr, flush=True)
            print(f"ğŸ“Š [{datetime.now().strftime('%H:%M:%S')}] Server status: Model loaded, ready for high-speed translation", 
                  file=sys.stderr, flush=True)
            
            while self.running:
                try:
                    client_socket, client_address = self.server_socket.accept()
                    client_thread = threading.Thread(
                        target=self.handle_client, 
                        args=(client_socket, client_address),
                        daemon=True
                    )
                    client_thread.start()
                except Exception as e:
                    if self.running:  # ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                        print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] Accept error: {e}", 
                              file=sys.stderr, flush=True)
                    
        except Exception as e:
            print(f"âŒ [{datetime.now().strftime('%H:%M:%S')}] Server startup failed: {e}", 
                  file=sys.stderr, flush=True)
            return False
        finally:
            self.cleanup()
        
        return True
    
    def cleanup(self):
        """
        ã‚µãƒ¼ãƒãƒ¼çµ‚äº†å‡¦ç†
        ğŸš€ Issue #147 Phase 1: æœ€é©åŒ–çµ±è¨ˆè¡¨ç¤º
        """
        self.running = False
        if self.server_socket:
            self.server_socket.close()
        
        # ğŸš€ Issue #147 Phase 1: æœ€é©åŒ–åŠ¹æœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        uptime = (datetime.now() - self.start_time).total_seconds()
        cache_efficiency = (self.cache_hits / max(self.translation_count, 1)) * 100 if self.translation_count > 0 else 0
        
        print(f"ğŸ›‘ [{datetime.now().strftime('%H:%M:%S')}] Server stopped - Issue #147 Phase 1 Optimization Summary:", 
              file=sys.stderr, flush=True)
        print(f"ğŸ“Š Total translations: {self.translation_count}", file=sys.stderr, flush=True)
        print(f"ğŸ’¾ Cache hits: {self.cache_hits} ({cache_efficiency:.1f}% efficiency)", file=sys.stderr, flush=True)
        print(f"ğŸ“ Cache size: {len(self.translation_cache) if hasattr(self, 'translation_cache') else 0} entries", file=sys.stderr, flush=True)
        print(f"â±ï¸ Uptime: {uptime:.1f} seconds", file=sys.stderr, flush=True)
        print(f"ğŸ® Device: {self.device} (FP16: {self.use_fp16})", file=sys.stderr, flush=True)

def main():
    """
    å¸¸é§ã‚µãƒ¼ãƒãƒ¼ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    """
    # ğŸ”§ [CRITICAL_ENCODING_FIX] Windowsç’°å¢ƒã§UTF-8å‡ºåŠ›ã‚’å¼·åˆ¶
    import codecs
    import locale
    
    # ã‚·ã‚¹ãƒ†ãƒ æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¼·åˆ¶çš„ã«UTF-8ã«è¨­å®š
    try:
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)
        print(f"ğŸ”§ [ENCODING_INIT] UTF-8 output streams configured", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âš ï¸ [ENCODING_INIT] Failed to configure UTF-8 streams: {e}", file=sys.stderr, flush=True)
    
    # ç’°å¢ƒå¤‰æ•°ã§Pythonã®UTF-8ãƒ¢ãƒ¼ãƒ‰ã‚’å¼·åˆ¶
    import os
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    os.environ['PYTHONLEGACYWINDOWSSTDIO'] = '0'
    
    print(f"ğŸ”§ [ENCODING_INIT] System locale: {locale.getpreferredencoding()}", file=sys.stderr, flush=True)
    print(f"ğŸ”§ [ENCODING_INIT] File system encoding: {sys.getfilesystemencoding()}", file=sys.stderr, flush=True)
    print(f"ğŸ”§ [ENCODING_INIT] Default encoding: {sys.getdefaultencoding()}", file=sys.stderr, flush=True)
    
    print(f"ğŸ¯ [{datetime.now().strftime('%H:%M:%S')}] Baketa OPUS-MT Persistent Server starting...", 
          file=sys.stderr, flush=True)
    
    server = PersistentOpusMtServer()
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    if not server.initialize_models():
        print(f"ğŸ’¥ [{datetime.now().strftime('%H:%M:%S')}] Failed to initialize model. Exiting.", 
              file=sys.stderr, flush=True)
        sys.exit(1)
    
    # ã‚µãƒ¼ãƒãƒ¼é–‹å§‹
    try:
        server.start_server()
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ [{datetime.now().strftime('%H:%M:%S')}] Server interrupted by user", 
              file=sys.stderr, flush=True)
    except Exception as e:
        print(f"ğŸ’¥ [{datetime.now().strftime('%H:%M:%S')}] Server crashed: {e}", 
              file=sys.stderr, flush=True)
        print(f"ğŸ“„ Traceback:\n{traceback.format_exc()}", file=sys.stderr, flush=True)
        sys.exit(1)

if __name__ == "__main__":
    main()