#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NLLB-200ãƒ™ãƒ¼ã‚¹ã®é«˜å“è³ªç¿»è¨³ã‚µãƒ¼ãƒãƒ¼
Metaã®æœ€æ–°å¤šè¨€èªç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ”¹è‰¯ç‰ˆ
"""

import asyncio
import json
import logging
import signal
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import argparse
from collections import deque
from threading import Lock

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–å®šç¾©
class ModelNotLoadedError(Exception):
    """ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class UnsupportedLanguageError(Exception):
    """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èªã®å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class TextTooLongError(Exception):
    """ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class BatchSizeExceededError(Exception):
    """ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class ModelInferenceError(Exception):
    """ãƒ¢ãƒ‡ãƒ«æ¨è«–ä¸­ã®ã‚¨ãƒ©ãƒ¼"""
    pass

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TranslationRequest:
    """ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    text: str
    source_lang: str
    target_lang: str
    request_id: Optional[str] = None

@dataclass
class TranslationResponse:
    """ç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translation: Optional[str] = None
    confidence: float = 0.0
    error: Optional[str] = None
    error_code: Optional[str] = None
    processing_time: float = 0.0

@dataclass
class BatchTranslationRequest:
    """ãƒãƒƒãƒç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    texts: List[str]
    source_lang: str
    target_lang: str
    batch_mode: bool = True
    max_batch_size: int = 50

@dataclass
class BatchTranslationResponse:
    """ãƒãƒƒãƒç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translations: List[str]
    confidence_scores: List[float]
    processing_time: float
    batch_size: int
    errors: Optional[List[str]] = None

class GpuResourceMonitor:
    """GPU ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹ - VRAMä½¿ç”¨é‡ãƒ™ãƒ¼ã‚¹ã®å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—"""
    
    def __init__(self):
        self.vram_threshold = 0.85  # 85%ä½¿ç”¨ç‡ã§åˆ¶é™
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
    def get_optimal_batch_size(self) -> int:
        """VRAMä½¿ç”¨é‡ãƒ™ãƒ¼ã‚¹ã®å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—"""
        try:
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                if device_count > 0:
                    # GPU 0 ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’å–å¾—
                    allocated = torch.cuda.memory_allocated(0)
                    cached = torch.cuda.memory_reserved(0)
                    
                    # åˆ©ç”¨å¯èƒ½ãªç·ãƒ¡ãƒ¢ãƒªã‚’å–å¾—
                    total_memory = torch.cuda.get_device_properties(0).total_memory
                    
                    # ä½¿ç”¨ç‡è¨ˆç®—
                    vram_used = max(allocated, cached) / total_memory
                    
                    self.logger.debug(f"GPU Memory - Used: {vram_used:.2%}, Allocated: {allocated/(1024**3):.1f}GB, Total: {total_memory/(1024**3):.1f}GB")
                    
                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„èª¿æ•´
                    if vram_used < 0.5:
                        return 32  # å¤§ãƒãƒƒãƒ
                    elif vram_used < 0.7:
                        return 16  # ä¸­ãƒãƒƒãƒ
                    else:
                        return 8   # å°ãƒãƒƒãƒ
                        
            return 8  # CPU fallback
            
        except Exception as e:
            self.logger.warning(f"GPU resource monitoring failed: {e}")
            return 8  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

class DynamicBatchAggregator:
    """å‹•çš„ãƒãƒƒãƒé›†ç´„ã‚·ã‚¹ãƒ†ãƒ  - GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"""
    
    def __init__(self, max_batch_size: int = 32, max_wait_time_ms: int = 30):
        self.max_batch_size = max_batch_size
        self.max_wait_time_ms = max_wait_time_ms
        self.pending_requests = asyncio.Queue()
        self.gpu_monitor = GpuResourceMonitor()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.processing_lock = Lock()
        
    async def add_request(self, request: TranslationRequest) -> Optional[str]:
        """ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
        await self.pending_requests.put(request)
        self.logger.debug(f"Request added to batch queue: {request.text[:30]}...")
        return None
    
    async def aggregate_requests(self) -> List[TranslationRequest]:
        """GPUæœ€é©åŒ–ãƒãƒƒãƒé›†ç´„"""
        batch = []
        start_time = time.time()
        
        # GPUãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³ã«å¿œã˜ãŸæœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—
        optimal_batch_size = min(self.max_batch_size, self.gpu_monitor.get_optimal_batch_size())
        self.logger.debug(f"Optimal batch size: {optimal_batch_size}")
        
        while len(batch) < optimal_batch_size:
            try:
                timeout = self.max_wait_time_ms / 1000.0
                request = await asyncio.wait_for(
                    self.pending_requests.get(), 
                    timeout=timeout
                )
                batch.append(request)
                self.logger.debug(f"Added request to batch: {len(batch)}/{optimal_batch_size}")
                
            except asyncio.TimeoutError:
                self.logger.debug(f"Batch timeout reached with {len(batch)} requests")
                break
        
        if batch:
            elapsed_time = (time.time() - start_time) * 1000
            self.logger.info(f"ğŸ”¥ [BATCH_AGGREGATION] Collected {len(batch)} requests in {elapsed_time:.1f}ms")
        
        return batch
    
    def get_queue_size(self) -> int:
        """ç¾åœ¨ã®ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        return self.pending_requests.qsize()

class NllbTranslationServer:
    """NLLB-200ãƒ™ãƒ¼ã‚¹ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self, port: int = 5556):
        self.port = port
        self.model = None
        self.tokenizer = None
        self.executor = ThreadPoolExecutor(max_workers=8)  # ğŸ”§ CONCURRENT_OPTIMIZATION: 4â†’8ã§åŒæ™‚æ¥ç¶šåˆ¶é™ã‚’ç·©å’Œ
        self.request_count = 0
        self.total_processing_time = 0.0
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # ğŸš€ Phase 1: GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
        self.batch_aggregator = DynamicBatchAggregator(max_batch_size=32, max_wait_time_ms=30)  # Geminiæ¨å¥¨: 30ms
        self.gpu_monitor = GpuResourceMonitor()
        self.batch_processing_enabled = True
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç®¡ç†
        self.pending_futures = {}  # request_id -> Future ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.request_id_counter = 0
        
        logger.info("ğŸ”¥ Phase 1: Dynamic batch aggregation system initialized")
        
        # è¨€èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆNLLB-200å¯¾å¿œï¼‰
        self.language_mapping = {
            "en": "eng_Latn",
            "ja": "jpn_Jpan",
            "english": "eng_Latn", 
            "japanese": "jpn_Jpan"
        }
    
    def _get_available_memory_gb(self) -> float:
        """åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªé‡ï¼ˆGBï¼‰ã‚’å–å¾—"""
        import psutil
        try:
            # ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‚’å–å¾—
            available_bytes = psutil.virtual_memory().available
            available_gb = available_bytes / (1024**3)
            return available_gb
        except ImportError:
            logger.warning("psutil not available - assuming 8GB memory")
            return 8.0  # psutilãŒãªã„å ´åˆã¯8GBã¨ä»®å®š
        except Exception as e:
            logger.warning(f"Memory detection failed: {e} - assuming 4GB")
            return 4.0
        
    def load_model(self):
        """NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        logger.info("ğŸš€ NLLB_MODEL_LOAD_START: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        start_time = time.time()
        
        try:
            # ğŸ›¡ï¸ ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–: ãƒ¡ãƒ¢ãƒªé‡ã«å¿œã˜ãŸãƒ¢ãƒ‡ãƒ«é¸æŠ
            lightweight_model = "facebook/nllb-200-distilled-600M"   # 600Mç‰ˆ (ç´„2.4GB) - è»½é‡ãƒ»é«˜å“è³ª
            heavy_model = "facebook/nllb-200-distilled-1.3B"        # 1.3Bç‰ˆ (ç´„5GB) - é‡ã„ãƒ»æœ€é«˜å“è³ª
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ™ãƒ¼ã‚¹ã§ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆæ­£ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            available_memory_gb = self._get_available_memory_gb()
            if available_memory_gb >= 6.0:  # ååˆ†ãªãƒ¡ãƒ¢ãƒªãŒã‚ã‚‹å ´åˆã¯é‡ã„ãƒ¢ãƒ‡ãƒ«
                model_name = heavy_model
                logger.info(f"ğŸš€ ååˆ†ãªãƒ¡ãƒ¢ãƒªã‚ã‚Š({available_memory_gb:.1f}GB) - æœ€é«˜å“è³ªãƒ¢ãƒ‡ãƒ«{model_name}ä½¿ç”¨")
            else:  # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆã¯è»½é‡ãƒ¢ãƒ‡ãƒ«
                model_name = lightweight_model
                logger.info(f"âš¡ ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚ã‚Š({available_memory_gb:.1f}GB) - è»½é‡ãƒ¢ãƒ‡ãƒ«{model_name}ä½¿ç”¨")
            
            logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_name} åˆæœŸåŒ–ä¸­...")
            
            # æ®µéšçš„ãƒ­ãƒ¼ãƒ‰ - ã¾ãšãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                logger.info("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            except Exception as e:
                logger.error(f"âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                raise ModelNotLoadedError(f"Tokenizer load failed: {e}")
            
            # æ¬¡ã«ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ï¼ˆãƒ¡ãƒ¢ãƒªé›†ç´„çš„ï¼‰
            try:
                logger.info("ğŸ§  ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ãƒ­ãƒ¼ãƒ‰ä¸­ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ï¼‰...")
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åŠåˆ†ã«
                    device_map="auto"           # è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é…ç½®
                )
                logger.info("âœ… ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            except torch.cuda.OutOfMemoryError as e:
                logger.warning(f"âš ï¸ GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ - CPUä½¿ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                # CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu"
                )
                self.device = "cpu"
                logger.info("âœ… CPUãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # ãƒ‡ãƒã‚¤ã‚¹é…ç½®ç¢ºèªï¼ˆdevice_mapã§è‡ªå‹•é…ç½®æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if not hasattr(self.model, "hf_device_map"):
                self.model = self.model.to(self.device)
                logger.info(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ã‚’{self.device}ã«é…ç½®å®Œäº†")
            
            self.model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            
            load_time = time.time() - start_time
            logger.info(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")
            logger.info("ğŸ‰ NLLB_MODEL_LOAD_COMPLETE: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä»˜é–‹å§‹")
            
            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›æ¨è«–ã®é…å»¶ã‚’å›é¿ï¼‰
            self._warmup_model()
            
            # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
            total_time = time.time() - start_time
            logger.info("ğŸ NLLB_MODEL_READY: ã™ã¹ã¦ã®åˆæœŸåŒ–å®Œäº† - ç·æ™‚é–“: {:.2f}ç§’".format(total_time))
            
            # ğŸš€ Phase 1ã‚·ã‚¹ãƒ†ãƒ ã®æº–å‚™å®Œäº†ã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info("ğŸ”¥ Phase 1 GPUæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº† - å‹•çš„ãƒãƒƒãƒå‡¦ç†ãƒ»VRAMç›£è¦–æœ‰åŠ¹")
            
        except ImportError as e:
            logger.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
            raise ModelNotLoadedError(f"Required library missing: {e}")
        except OSError as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model file load failed: {e}")
        except torch.cuda.OutOfMemoryError as e:
            logger.error(f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³: {e}")
            raise ModelNotLoadedError(f"GPU memory insufficient: {e}")
        except Exception as e:
            logger.error(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model load failed: {e}")
            
    def _warmup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        try:
            # è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            self._translate_text("Hello", "en", "ja")
            logger.info("è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
            # æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            self._translate_text("ã“ã‚“ã«ã¡ã¯", "ja", "en")
            logger.info("æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
        except Exception as e:
            logger.warning(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            
    def _get_nllb_lang_code(self, lang_code: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’NLLB-200å½¢å¼ã«å¤‰æ›"""
        if not lang_code or not isinstance(lang_code, str):
            raise UnsupportedLanguageError(f"Invalid language code: {lang_code}")
            
        normalized = lang_code.lower()[:2]  # "ja", "en"
        
        if normalized in self.language_mapping:
            return self.language_mapping[normalized]
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
            if normalized == "ja":
                return "jpn_Jpan"
            elif normalized == "en":
                return "eng_Latn"
            else:
                raise UnsupportedLanguageError(f"Unsupported language: {lang_code}. Supported: {list(self.language_mapping.keys())}")
    
    def _translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰- NLLB-200å¯¾å¿œç‰ˆ"""
        if not self.model or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")
            
        # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼
        if not text or not isinstance(text, str):
            raise ValueError("Invalid input text")
        if len(text) > 10000:  # åˆ¶é™è¨­å®š
            raise TextTooLongError(f"Text too long: {len(text)} characters (max: 10000)")
            
        try:
            # ğŸ”„ [NLLB-200] ç¿»è¨³å‡¦ç†
            logger.info(f"ğŸ”„ [NLLB-200] ç¿»è¨³å®Ÿè¡Œ: '{text[:30]}...' [{source_lang}->{target_lang}]")
            
            # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
            src_lang = self._get_nllb_lang_code(source_lang)
            tgt_lang = self._get_nllb_lang_code(target_lang)
            
            # è¨€èªè¨­å®š
            self.tokenizer.src_lang = src_lang
            self.tokenizer.tgt_lang = tgt_lang
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨è«–ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚no_gradã¨æœ€é©åŒ–è¨­å®šã‚’ä½¿ç”¨ï¼‰
            with torch.no_grad():
                if self.device.type == "cuda":
                    with torch.cuda.amp.autocast():
                        outputs = self.model.generate(
                            **inputs,
                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                            max_length=512, 
                            num_beams=4, 
                            early_stopping=True
                        )
                else:
                    outputs = self.model.generate(
                        **inputs,
                        forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                        max_length=512, 
                        num_beams=4, 
                        early_stopping=True
                    )
                    
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ‡ãƒãƒƒã‚°: ç¿»è¨³çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"NLLB Translation result: '{translation}' (type: {type(translation)})")
            
            return translation
            
        except UnsupportedLanguageError:
            raise  # å†ç™ºç”Ÿ
        except torch.cuda.OutOfMemoryError as e:
            logger.error(f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³: {e}")
            raise ModelInferenceError(f"GPU memory insufficient during inference")
        except RuntimeError as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            raise ModelInferenceError(f"Model inference failed: {e}")
        except Exception as e:
            logger.error(f"NLLB-200ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            raise ModelInferenceError(f"Translation failed: {e}")
        
    async def translate_via_batch(self, request: TranslationRequest, timeout: float = 10.0) -> TranslationResponse:
        """ğŸš€ Phase 1: ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ çµŒç”±ã®ç¿»è¨³å‡¦ç†"""
        if not self.batch_processing_enabled:
            return await self.translate(request)
        
        start_time = time.time()
        
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDã‚’ç”Ÿæˆ
            self.request_id_counter += 1
            request_id = f"req_{self.request_id_counter}_{int(time.time() * 1000)}"
            request.request_id = request_id
            
            # Futureã‚’ä½œæˆã—ã¦ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒªã‚¹ãƒˆã«è¿½åŠ 
            future = asyncio.Future()
            self.pending_futures[request_id] = future
            
            # ãƒãƒƒãƒã‚­ãƒ¥ãƒ¼ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¿½åŠ 
            await self.batch_aggregator.add_request(request)
            logger.debug(f"ğŸ”„ [BATCH_REQUEST] Added to queue: {request.text[:30]}... (ID: {request_id})")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            try:
                response = await asyncio.wait_for(future, timeout=timeout)
                processing_time = (time.time() - start_time) * 1000
                logger.info(f"âœ… [BATCH_RESPONSE] Completed via batch: {processing_time:.1f}ms (ID: {request_id})")
                return response
                
            except asyncio.TimeoutError:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if request_id in self.pending_futures:
                    self.pending_futures.pop(request_id)
                logger.warning(f"âš ï¸ [BATCH_TIMEOUT] Request timed out after {timeout}s: {request.text[:30]}...")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥å‡¦ç†
                return await self.translate(request)
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if request.request_id and request.request_id in self.pending_futures:
                self.pending_futures.pop(request.request_id)
            logger.error(f"Batch translation setup error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥å‡¦ç†
            return await self.translate(request)

    async def translate(self, request: TranslationRequest) -> TranslationResponse:
        """éåŒæœŸç¿»è¨³å‡¦ç†"""
        start_time = time.time()
        
        try:
            # ğŸš€ [NLLB-200] ç›´æ¥ç¿»è¨³å®Ÿè¡Œ
            logger.info(f"ğŸš€ [NLLB-200] ç¿»è¨³å®Ÿè¡Œ: '{request.text[:30]}...'")
            
            # ç¿»è¨³å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰
            loop = asyncio.get_event_loop()
            translation = await loop.run_in_executor(
                self.executor,
                self._translate_text,
                request.text,
                request.source_lang,
                request.target_lang
            )
            
            # âœ… NLLB-200ç¿»è¨³å®Œäº†
            logger.info(f"âœ… [NLLB-200_SUCCESS] ç¿»è¨³å®Œäº†: '{request.text[:30]}...' -> '{translation[:30]}...'")
            
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += 1
            self.total_processing_time += processing_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            if processing_time > 1000:  # NLLB-200ã¯é…ã„ãŸã‚é–¾å€¤èª¿æ•´
                logger.warning(f"Translation exceeded 1000ms target: {processing_time:.1f}ms")
            else:
                logger.info(f"Fast translation completed: {processing_time:.1f}ms")
                
            return TranslationResponse(
                success=True,
                translation=translation,
                confidence=0.95,  # NLLB-200ã¯é«˜å“è³ª
                processing_time=processing_time / 1000.0
            )
            
        except ModelNotLoadedError as e:
            logger.error(f"Model not loaded: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="MODEL_NOT_LOADED",
                processing_time=processing_time / 1000.0
            )
        except UnsupportedLanguageError as e:
            logger.error(f"Unsupported language: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="UNSUPPORTED_LANGUAGE",
                processing_time=processing_time / 1000.0
            )
        except TextTooLongError as e:
            logger.error(f"Text too long: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="TEXT_TOO_LONG",
                processing_time=processing_time / 1000.0
            )
        except ModelInferenceError as e:
            logger.error(f"Model inference error: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="MODEL_INFERENCE_ERROR",
                processing_time=processing_time / 1000.0
            )
        except Exception as e:
            logger.error(f"Unexpected translation error: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=f"Unexpected error: {str(e)}",
                error_code="UNKNOWN_ERROR",
                processing_time=processing_time / 1000.0
            )

    async def process_batch_optimized(self, requests: List[TranslationRequest]) -> List[TranslationResponse]:
        """ğŸš€ Phase 1: GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç† - å‹•çš„ãƒãƒƒãƒé›†ç´„å¯¾å¿œ"""
        if not requests:
            return []
            
        start_time = time.time()
        logger.info(f"ğŸš€ [GPU_BATCH_OPTIMIZED] Processing {len(requests)} requests")
        
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨€èªãƒšã‚¢åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆåŠ¹ç‡å‘ä¸Šï¼‰
            language_groups = {}
            for req in requests:
                key = (req.source_lang, req.target_lang)
                if key not in language_groups:
                    language_groups[key] = []
                language_groups[key].append(req)
            
            all_responses = []
            
            # è¨€èªãƒšã‚¢åˆ¥ã«ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            for (source_lang, target_lang), group_requests in language_groups.items():
                texts = [req.text for req in group_requests]
                
                # GPUæœ€é©åŒ–ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ
                loop = asyncio.get_event_loop()
                translations = await loop.run_in_executor(
                    self.executor,
                    self._batch_translate,
                    texts, source_lang, target_lang
                )
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
                group_processing_time = (time.time() - start_time) * 1000
                for i, (req, translation) in enumerate(zip(group_requests, translations)):
                    response = TranslationResponse(
                        success=True,
                        translation=translation,
                        confidence=0.95,
                        processing_time=group_processing_time / 1000.0 / len(group_requests)
                    )
                    all_responses.append(response)
            
            total_processing_time = (time.time() - start_time) * 1000
            avg_time = total_processing_time / len(requests)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += len(requests)
            self.total_processing_time += total_processing_time
            
            logger.info(f"ğŸ‰ [GPU_BATCH_COMPLETED] {len(requests)} requests processed in {total_processing_time:.1f}ms (avg: {avg_time:.1f}ms/req)")
            return all_responses
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            # å¤±æ•—æ™‚ã¯å€‹åˆ¥å‡¦ç†ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._fallback_individual_processing(requests)

    async def _fallback_individual_processing(self, requests: List[TranslationRequest]) -> List[TranslationResponse]:
        """ãƒãƒƒãƒå‡¦ç†å¤±æ•—æ™‚ã®å€‹åˆ¥å‡¦ç†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        logger.warning(f"Falling back to individual processing for {len(requests)} requests")
        responses = []
        for req in requests:
            response = await self.translate(req)
            responses.append(response)
        return responses

    async def translate_batch(self, request: BatchTranslationRequest) -> BatchTranslationResponse:
        """ãƒãƒƒãƒç¿»è¨³å‡¦ç† - Phase 1 GPUæœ€é©åŒ–ç‰ˆ"""
        start_time = time.time()
        
        try:
            # GPUãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã«ã‚ˆã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™
            optimal_batch_size = self.gpu_monitor.get_optimal_batch_size()
            effective_max_batch_size = min(request.max_batch_size, optimal_batch_size)
            
            if len(request.texts) > effective_max_batch_size:
                raise BatchSizeExceededError(f"Batch size {len(request.texts)} exceeds GPU-optimized limit {effective_max_batch_size}")
            
            logger.info(f"ğŸ” [NLLB_BATCH_GPU_OPTIMIZED] ãƒãƒƒãƒç¿»è¨³ - {len(request.texts)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆ (æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimal_batch_size})")
            
            # GPUæœ€é©åŒ–ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            translations = await loop.run_in_executor(
                self.executor,
                self._batch_translate,
                request.texts, request.source_lang, request.target_lang
            )
            
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆNLLB-200ã¯é«˜å“è³ªï¼‰
            confidence_scores = [0.95] * len(request.texts)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += len(request.texts)
            self.total_processing_time += processing_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ãƒ­ã‚°
            avg_time_per_text = processing_time / len(request.texts)
            logger.info(f"ğŸ‰ [GPU_OPTIMIZED_BATCH] å®Œäº†: {avg_time_per_text:.1f}ms/text, batch size: {len(request.texts)}")
            
            return BatchTranslationResponse(
                success=True,
                translations=translations,
                confidence_scores=confidence_scores,
                processing_time=processing_time / 1000.0,  # seconds
                batch_size=len(request.texts)
            )
            
        except BatchSizeExceededError as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Batch size exceeded: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["BATCH_SIZE_EXCEEDED", str(e)]
            )
        except ModelNotLoadedError as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Model not loaded for batch: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["MODEL_NOT_LOADED", str(e)]
            )
        except UnsupportedLanguageError as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Unsupported language in batch: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["UNSUPPORTED_LANGUAGE", str(e)]
            )
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Unexpected batch translation error: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["UNKNOWN_ERROR", str(e)]
            )

    def _batch_translate(self, texts: List[str], source_lang: str, target_lang: str) -> List[str]:
        """ãƒãƒƒãƒç¿»è¨³å‡¦ç†ï¼ˆåŒæœŸå‡¦ç†ã§ThreadPoolExecutorã§å®Ÿè¡Œï¼‰"""
        try:
            # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
            src_lang = self._get_nllb_lang_code(source_lang)
            tgt_lang = self._get_nllb_lang_code(target_lang)
            
            # è¨€èªè¨­å®š
            self.tokenizer.src_lang = src_lang
            self.tokenizer.tgt_lang = tgt_lang
            
            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆåŠ¹ç‡åŒ–ï¼‰
            inputs = self.tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                if self.device.type == "cuda":
                    with torch.cuda.amp.autocast():
                        outputs = self.model.generate(
                            **inputs,
                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                            max_length=512, 
                            num_beams=4, 
                            early_stopping=True
                        )
                else:
                    outputs = self.model.generate(
                        **inputs,
                        forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                        max_length=512, 
                        num_beams=4, 
                        early_stopping=True
                    )
            
            # ãƒãƒƒãƒãƒ‡ã‚³ãƒ¼ãƒ‰
            translations = []
            for output in outputs:
                translation = self.tokenizer.decode(output, skip_special_tokens=True)
                translations.append(translation)
                
            return translations
            
        except Exception as e:
            logger.error(f"Batch translate error: {e}")
            raise
            
    async def handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šå‡¦ç†"""
        client_addr = writer.get_extra_info('peername')
        logger.info(f"Client connected: {client_addr}")
        
        try:
            while True:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡
                data = await reader.readline()
                if not data:
                    break
                    
                try:
                    # ãƒ‡ãƒãƒƒã‚°: å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å‡ºåŠ›
                    raw_data = data.decode('utf-8')
                    logger.info(f"[DEBUG] Received raw data: {repr(raw_data)}")
                    logger.info(f"[DEBUG] Data length: {len(raw_data)}")
                    
                    # JSONãƒ‘ãƒ¼ã‚¹
                    request_data = json.loads(raw_data)
                    
                    # Pingãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¤å®šï¼ˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
                    if 'ping' in request_data:
                        ping_response = {
                            'success': True,
                            'pong': True,
                            'status': 'ready',
                            'model': 'NLLB-200',
                            'processing_time': 0.001
                        }
                        response_json = json.dumps(ping_response, ensure_ascii=False) + '\n'
                        writer.write(response_json.encode('utf-8'))
                        await writer.drain()
                        continue
                    
                    # ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š
                    elif 'texts' in request_data and request_data.get('batch_mode', False):
                        # ãƒãƒƒãƒç¿»è¨³å‡¦ç†
                        batch_request = BatchTranslationRequest(
                            texts=request_data['texts'],
                            source_lang=request_data.get('source_lang', 'en'),
                            target_lang=request_data.get('target_lang', 'ja'),
                            batch_mode=request_data.get('batch_mode', True),
                            max_batch_size=request_data.get('max_batch_size', 50)
                        )
                        
                        batch_response = await self.translate_batch(batch_request)
                        
                        response_data = {
                            'success': batch_response.success,
                            'translations': batch_response.translations,
                            'confidence_scores': batch_response.confidence_scores,
                            'processing_time': batch_response.processing_time,
                            'batch_size': batch_response.batch_size,
                            'errors': batch_response.errors,
                            'model': 'NLLB-200'
                        }
                    else:
                        # å˜ä¸€ç¿»è¨³å‡¦ç†ï¼ˆå¾“æ¥ã®å‡¦ç†ï¼‰
                        request = TranslationRequest(
                            text=request_data['text'],
                            source_lang=request_data.get('source_lang', 'en'),
                            target_lang=request_data.get('target_lang', 'ja'),
                            request_id=request_data.get('request_id')
                        )
                        
                        # ğŸš€ Phase 1: ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ çµŒç”±ã§ç¿»è¨³å®Ÿè¡Œ
                        response = await self.translate_via_batch(request)
                        
                        response_data = {
                            'success': response.success,
                            'translation': response.translation,
                            'confidence': response.confidence,
                            'error': response.error,
                            'error_code': response.error_code,
                            'processing_time': response.processing_time,
                            'model': 'NLLB-200'
                        }
                    
                    response_json = json.dumps(response_data, ensure_ascii=False) + '\n'
                    writer.write(response_json.encode('utf-8'))
                    await writer.drain()
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': 'Invalid JSON format',
                        'error_code': 'INVALID_JSON',
                        'model': 'NLLB-200'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
                except KeyError as e:
                    logger.error(f"Missing required field: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': f'Missing required field: {e}',
                        'error_code': 'MISSING_FIELD',
                        'model': 'NLLB-200'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
                except Exception as e:
                    logger.error(f"Request processing error: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': f'Request processing failed: {str(e)}',
                        'error_code': 'REQUEST_PROCESSING_ERROR',
                        'model': 'NLLB-200'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(f"Client handler error: {e}")
        finally:
            writer.close()
            await writer.wait_closed()
            logger.info(f"Client disconnected: {client_addr}")
            
    async def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        server = await asyncio.start_server(
            self.handle_client,
            '127.0.0.1',
            self.port
        )
        
        addr = server.sockets[0].getsockname()
        logger.info(f"NLLB-200 Translation Server listening on {addr[0]}:{addr[1]}")
        
        # ğŸš€ Phase 1: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒãƒƒãƒå‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        if self.batch_processing_enabled:
            asyncio.create_task(self._batch_processing_worker())
            logger.info("ğŸ”¥ Phase 1: Background batch processing worker started")
        
        # çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«å‡ºåŠ›
        asyncio.create_task(self._print_stats())
        
        async with server:
            await server.serve_forever()
    
    async def _batch_processing_worker(self):
        """ğŸš€ Phase 1: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒãƒƒãƒå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        logger.info("ğŸ”„ Batch processing worker started")
        
        while True:
            try:
                # å‹•çš„ãƒãƒƒãƒé›†ç´„
                batch_requests = await self.batch_aggregator.aggregate_requests()
                
                if batch_requests:
                    logger.info(f"ğŸš€ [BATCH_WORKER] Processing batch of {len(batch_requests)} requests")
                    
                    # GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
                    batch_responses = await self.process_batch_optimized(batch_requests)
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¯¾å¿œã™ã‚‹Futureã«è¨­å®š
                    for request, response in zip(batch_requests, batch_responses):
                        if request.request_id and request.request_id in self.pending_futures:
                            future = self.pending_futures.pop(request.request_id)
                            if not future.done():
                                future.set_result(response)
                    
                    logger.info(f"âœ… [BATCH_WORKER] Completed batch of {len(batch_responses)} responses")
                
                else:
                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒãªã„å ´åˆã¯çŸ­æ™‚é–“å¾…æ©Ÿ
                    await asyncio.sleep(0.001)  # 1mså¾…æ©Ÿ
                    
            except Exception as e:
                logger.error(f"Batch processing worker error: {e}")
                # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯å¾…æ©Ÿä¸­ã®Futureã«ã‚¨ãƒ©ãƒ¼ã‚’è¨­å®š
                error_response = TranslationResponse(
                    success=False,
                    error=f"Batch processing error: {str(e)}",
                    error_code="BATCH_WORKER_ERROR",
                    processing_time=0.0
                )
                for future in list(self.pending_futures.values()):
                    if not future.done():
                        future.set_result(error_response)
                self.pending_futures.clear()
                await asyncio.sleep(0.1)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯100mså¾…æ©Ÿ
            
    async def _print_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«å‡ºåŠ› - Phase 1 GPUæœ€é©åŒ–æƒ…å ±è¿½åŠ """
        while True:
            await asyncio.sleep(60)  # 1åˆ†ã”ã¨
            if self.request_count > 0:
                avg_time = self.total_processing_time / self.request_count
                
                # GPUæƒ…å ±å–å¾—
                gpu_info = ""
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated(0) / (1024**3)  # GB
                    gpu_max_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
                    gpu_usage = (torch.cuda.memory_allocated(0) / torch.cuda.get_device_properties(0).total_memory) * 100
                    gpu_info = f", GPU Memory: {gpu_memory:.1f}GB/{gpu_max_memory:.1f}GB ({gpu_usage:.1f}%)"
                
                # ãƒãƒƒãƒã‚­ãƒ¥ãƒ¼æƒ…å ±
                queue_size = self.batch_aggregator.get_queue_size()
                optimal_batch_size = self.gpu_monitor.get_optimal_batch_size()
                
                logger.info(f"ğŸš€ [PHASE1_STATS] Requests: {self.request_count}, Avg time: {avg_time:.1f}ms"
                           f"{gpu_info}, Queue: {queue_size}, Optimal batch: {optimal_batch_size}")
                
    def shutdown(self, signum, frame):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
        logger.info("Shutting down NLLB-200 server...")
        self.executor.shutdown(wait=True)
        sys.exit(0)

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    # UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®š
    import os
    import locale
    try:
        # Windowsç’°å¢ƒã§ã®UTF-8è¨­å®š
        if sys.platform == 'win32':
            os.environ['PYTHONIOENCODING'] = 'utf-8'
            locale.setlocale(locale.LC_ALL, '')
    except Exception as e:
        logger.warning(f"âš ï¸ UTF-8è¨­å®šè­¦å‘Š: {e}")
    
    parser = argparse.ArgumentParser(description='NLLB-200 Translation Server')
    parser.add_argument('--port', type=int, default=5556, help='Server port')
    args = parser.parse_args()
    
    # ã‚µãƒ¼ãƒãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    server = NllbTranslationServer(port=args.port)
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
    signal.signal(signal.SIGINT, server.shutdown)
    signal.signal(signal.SIGTERM, server.shutdown)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    server.load_model()
    
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    try:
        asyncio.run(server.start_server())
    except KeyboardInterrupt:
        logger.info("NLLB-200 Server stopped by user")

if __name__ == "__main__":
    main()