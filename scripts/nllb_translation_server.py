#!/usr/bin/env python3
"""
NLLB-200ãƒ™ãƒ¼ã‚¹ã®é«˜å“è³ªç¿»è¨³ã‚µãƒ¼ãƒãƒ¼
Metaã®æœ€æ–°å¤šè¨€èªç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ”¹è‰¯ç‰ˆ
"""

import asyncio
import json
import logging
import signal
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import argparse

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–å®šç¾©
class ModelNotLoadedError(Exception):
    """ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class UnsupportedLanguageError(Exception):
    """ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨€èªã®å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class TextTooLongError(Exception):
    """ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class BatchSizeExceededError(Exception):
    """ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆãŸå ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass

class ModelInferenceError(Exception):
    """ãƒ¢ãƒ‡ãƒ«æ¨è«–ä¸­ã®ã‚¨ãƒ©ãƒ¼"""
    pass

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TranslationRequest:
    """ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    text: str
    source_lang: str
    target_lang: str
    request_id: Optional[str] = None

@dataclass
class TranslationResponse:
    """ç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translation: Optional[str] = None
    confidence: float = 0.0
    error: Optional[str] = None
    error_code: Optional[str] = None
    processing_time: float = 0.0

@dataclass
class BatchTranslationRequest:
    """ãƒãƒƒãƒç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    texts: List[str]
    source_lang: str
    target_lang: str
    batch_mode: bool = True
    max_batch_size: int = 50

@dataclass
class BatchTranslationResponse:
    """ãƒãƒƒãƒç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    translations: List[str]
    confidence_scores: List[float]
    processing_time: float
    batch_size: int
    errors: Optional[List[str]] = None

class NllbTranslationServer:
    """NLLB-200ãƒ™ãƒ¼ã‚¹ç¿»è¨³ã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self, port: int = 5556):
        self.port = port
        self.model = None
        self.tokenizer = None
        self.executor = ThreadPoolExecutor(max_workers=8)  # ğŸ”§ CONCURRENT_OPTIMIZATION: 4â†’8ã§åŒæ™‚æ¥ç¶šåˆ¶é™ã‚’ç·©å’Œ
        self.request_count = 0
        self.total_processing_time = 0.0
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # è¨€èªãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆNLLB-200å¯¾å¿œï¼‰
        self.language_mapping = {
            "en": "eng_Latn",
            "ja": "jpn_Jpan",
            "english": "eng_Latn", 
            "japanese": "jpn_Jpan"
        }
        
    def load_model(self):
        """NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        logger.info("ğŸš€ NLLB_MODEL_LOAD_START: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        start_time = time.time()
        
        try:
            # NLLB-200 distilledç‰ˆï¼ˆè»½é‡ã§é«˜å“è³ªï¼‰
            model_name = "facebook/nllb-200-distilled-600M"
            
            logger.info(f"ãƒ¢ãƒ‡ãƒ« {model_name} åˆæœŸåŒ–ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            self.model = self.model.to(self.device)
            self.model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            
            load_time = time.time() - start_time
            logger.info(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’")
            logger.info("ğŸ‰ NLLB_MODEL_LOAD_COMPLETE: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† - ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä»˜é–‹å§‹")
            
            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›æ¨è«–ã®é…å»¶ã‚’å›é¿ï¼‰
            self._warmup_model()
            
            # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
            total_time = time.time() - start_time
            logger.info("ğŸ NLLB_MODEL_READY: ã™ã¹ã¦ã®åˆæœŸåŒ–å®Œäº† - ç·æ™‚é–“: {:.2f}ç§’".format(total_time))
            
        except ImportError as e:
            logger.error(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
            raise ModelNotLoadedError(f"Required library missing: {e}")
        except OSError as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model file load failed: {e}")
        except torch.cuda.OutOfMemoryError as e:
            logger.error(f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³: {e}")
            raise ModelNotLoadedError(f"GPU memory insufficient: {e}")
        except Exception as e:
            logger.error(f"NLLB-200ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            raise ModelNotLoadedError(f"Model load failed: {e}")
            
    def _warmup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        logger.info("NLLB-200ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        try:
            # è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            self._translate_text("Hello", "en", "ja")
            logger.info("è‹±èªâ†’æ—¥æœ¬èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
            # æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            self._translate_text("ã“ã‚“ã«ã¡ã¯", "ja", "en")
            logger.info("æ—¥æœ¬èªâ†’è‹±èªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
            
        except Exception as e:
            logger.warning(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            
    def _get_nllb_lang_code(self, lang_code: str) -> str:
        """è¨€èªã‚³ãƒ¼ãƒ‰ã‚’NLLB-200å½¢å¼ã«å¤‰æ›"""
        if not lang_code or not isinstance(lang_code, str):
            raise UnsupportedLanguageError(f"Invalid language code: {lang_code}")
            
        normalized = lang_code.lower()[:2]  # "ja", "en"
        
        if normalized in self.language_mapping:
            return self.language_mapping[normalized]
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
            if normalized == "ja":
                return "jpn_Jpan"
            elif normalized == "en":
                return "eng_Latn"
            else:
                raise UnsupportedLanguageError(f"Unsupported language: {lang_code}. Supported: {list(self.language_mapping.keys())}")
    
    def _translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰- NLLB-200å¯¾å¿œç‰ˆ"""
        if not self.model or not self.tokenizer:
            raise ModelNotLoadedError("Model not loaded")
            
        # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ¤œè¨¼
        if not text or not isinstance(text, str):
            raise ValueError("Invalid input text")
        if len(text) > 10000:  # åˆ¶é™è¨­å®š
            raise TextTooLongError(f"Text too long: {len(text)} characters (max: 10000)")
            
        try:
            # ğŸ”„ [NLLB-200] ç¿»è¨³å‡¦ç†
            logger.info(f"ğŸ”„ [NLLB-200] ç¿»è¨³å®Ÿè¡Œ: '{text[:30]}...' [{source_lang}->{target_lang}]")
            
            # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
            src_lang = self._get_nllb_lang_code(source_lang)
            tgt_lang = self._get_nllb_lang_code(target_lang)
            
            # è¨€èªè¨­å®š
            self.tokenizer.src_lang = src_lang
            self.tokenizer.tgt_lang = tgt_lang
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨è«–ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚no_gradã¨æœ€é©åŒ–è¨­å®šã‚’ä½¿ç”¨ï¼‰
            with torch.no_grad():
                if self.device.type == "cuda":
                    with torch.cuda.amp.autocast():
                        outputs = self.model.generate(
                            **inputs,
                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                            max_length=512, 
                            num_beams=4, 
                            early_stopping=True
                        )
                else:
                    outputs = self.model.generate(
                        **inputs,
                        forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                        max_length=512, 
                        num_beams=4, 
                        early_stopping=True
                    )
                    
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ‡ãƒãƒƒã‚°: ç¿»è¨³çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"NLLB Translation result: '{translation}' (type: {type(translation)})")
            
            return translation
            
        except UnsupportedLanguageError:
            raise  # å†ç™ºç”Ÿ
        except torch.cuda.OutOfMemoryError as e:
            logger.error(f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³: {e}")
            raise ModelInferenceError(f"GPU memory insufficient during inference")
        except RuntimeError as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            raise ModelInferenceError(f"Model inference failed: {e}")
        except Exception as e:
            logger.error(f"NLLB-200ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            raise ModelInferenceError(f"Translation failed: {e}")
        
    async def translate(self, request: TranslationRequest) -> TranslationResponse:
        """éåŒæœŸç¿»è¨³å‡¦ç†"""
        start_time = time.time()
        
        try:
            # ğŸš€ [NLLB-200] ç›´æ¥ç¿»è¨³å®Ÿè¡Œ
            logger.info(f"ğŸš€ [NLLB-200] ç¿»è¨³å®Ÿè¡Œ: '{request.text[:30]}...'")
            
            # ç¿»è¨³å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰
            loop = asyncio.get_event_loop()
            translation = await loop.run_in_executor(
                self.executor,
                self._translate_text,
                request.text,
                request.source_lang,
                request.target_lang
            )
            
            # âœ… NLLB-200ç¿»è¨³å®Œäº†
            logger.info(f"âœ… [NLLB-200_SUCCESS] ç¿»è¨³å®Œäº†: '{request.text[:30]}...' -> '{translation[:30]}...'")
            
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += 1
            self.total_processing_time += processing_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            if processing_time > 1000:  # NLLB-200ã¯é…ã„ãŸã‚é–¾å€¤èª¿æ•´
                logger.warning(f"Translation exceeded 1000ms target: {processing_time:.1f}ms")
            else:
                logger.info(f"Fast translation completed: {processing_time:.1f}ms")
                
            return TranslationResponse(
                success=True,
                translation=translation,
                confidence=0.95,  # NLLB-200ã¯é«˜å“è³ª
                processing_time=processing_time / 1000.0
            )
            
        except ModelNotLoadedError as e:
            logger.error(f"Model not loaded: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="MODEL_NOT_LOADED",
                processing_time=processing_time / 1000.0
            )
        except UnsupportedLanguageError as e:
            logger.error(f"Unsupported language: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="UNSUPPORTED_LANGUAGE",
                processing_time=processing_time / 1000.0
            )
        except TextTooLongError as e:
            logger.error(f"Text too long: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="TEXT_TOO_LONG",
                processing_time=processing_time / 1000.0
            )
        except ModelInferenceError as e:
            logger.error(f"Model inference error: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=str(e),
                error_code="MODEL_INFERENCE_ERROR",
                processing_time=processing_time / 1000.0
            )
        except Exception as e:
            logger.error(f"Unexpected translation error: {e}")
            processing_time = (time.time() - start_time) * 1000
            return TranslationResponse(
                success=False,
                error=f"Unexpected error: {str(e)}",
                error_code="UNKNOWN_ERROR",
                processing_time=processing_time / 1000.0
            )

    async def translate_batch(self, request: BatchTranslationRequest) -> BatchTranslationResponse:
        """ãƒãƒƒãƒç¿»è¨³å‡¦ç† - NLLB-200æœ€é©åŒ–ç‰ˆ"""
        start_time = time.time()
        
        try:
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™
            if len(request.texts) > request.max_batch_size:
                raise BatchSizeExceededError(f"Batch size {len(request.texts)} exceeds limit {request.max_batch_size}")
            
            logger.info(f"ğŸ” [NLLB_BATCH] ãƒãƒƒãƒç¿»è¨³ - {len(request.texts)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆ")
            
            # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ï¼‰
            loop = asyncio.get_event_loop()
            translations = await loop.run_in_executor(
                self.executor,
                self._batch_translate,
                request.texts, request.source_lang, request.target_lang
            )
            
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆNLLB-200ã¯é«˜å“è³ªï¼‰
            confidence_scores = [0.95] * len(request.texts)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.request_count += len(request.texts)
            self.total_processing_time += processing_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ãƒ­ã‚°
            avg_time_per_text = processing_time / len(request.texts)
            logger.info(f"NLLB batch translation completed: {avg_time_per_text:.1f}ms/text, batch size: {len(request.texts)}")
            
            return BatchTranslationResponse(
                success=True,
                translations=translations,
                confidence_scores=confidence_scores,
                processing_time=processing_time / 1000.0,  # seconds
                batch_size=len(request.texts)
            )
            
        except BatchSizeExceededError as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Batch size exceeded: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["BATCH_SIZE_EXCEEDED", str(e)]
            )
        except ModelNotLoadedError as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Model not loaded for batch: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["MODEL_NOT_LOADED", str(e)]
            )
        except UnsupportedLanguageError as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Unsupported language in batch: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["UNSUPPORTED_LANGUAGE", str(e)]
            )
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"Unexpected batch translation error: {e}")
            
            return BatchTranslationResponse(
                success=False,
                translations=[],
                confidence_scores=[],
                processing_time=processing_time / 1000.0,
                batch_size=len(request.texts),
                errors=["UNKNOWN_ERROR", str(e)]
            )

    def _batch_translate(self, texts: List[str], source_lang: str, target_lang: str) -> List[str]:
        """ãƒãƒƒãƒç¿»è¨³å‡¦ç†ï¼ˆåŒæœŸå‡¦ç†ã§ThreadPoolExecutorã§å®Ÿè¡Œï¼‰"""
        try:
            # è¨€èªã‚³ãƒ¼ãƒ‰å¤‰æ›
            src_lang = self._get_nllb_lang_code(source_lang)
            tgt_lang = self._get_nllb_lang_code(target_lang)
            
            # è¨€èªè¨­å®š
            self.tokenizer.src_lang = src_lang
            self.tokenizer.tgt_lang = tgt_lang
            
            # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆåŠ¹ç‡åŒ–ï¼‰
            inputs = self.tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                if self.device.type == "cuda":
                    with torch.cuda.amp.autocast():
                        outputs = self.model.generate(
                            **inputs,
                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                            max_length=512, 
                            num_beams=4, 
                            early_stopping=True
                        )
                else:
                    outputs = self.model.generate(
                        **inputs,
                        forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(tgt_lang),
                        max_length=512, 
                        num_beams=4, 
                        early_stopping=True
                    )
            
            # ãƒãƒƒãƒãƒ‡ã‚³ãƒ¼ãƒ‰
            translations = []
            for output in outputs:
                translation = self.tokenizer.decode(output, skip_special_tokens=True)
                translations.append(translation)
                
            return translations
            
        except Exception as e:
            logger.error(f"Batch translate error: {e}")
            raise
            
    async def handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šå‡¦ç†"""
        client_addr = writer.get_extra_info('peername')
        logger.info(f"Client connected: {client_addr}")
        
        try:
            while True:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡
                data = await reader.readline()
                if not data:
                    break
                    
                try:
                    # JSONãƒ‘ãƒ¼ã‚¹
                    request_data = json.loads(data.decode('utf-8'))
                    
                    # Pingãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¤å®šï¼ˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
                    if 'ping' in request_data:
                        ping_response = {
                            'success': True,
                            'pong': True,
                            'status': 'ready',
                            'model': 'NLLB-200',
                            'processing_time': 0.001
                        }
                        response_json = json.dumps(ping_response, ensure_ascii=False) + '\n'
                        writer.write(response_json.encode('utf-8'))
                        await writer.drain()
                        continue
                    
                    # ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã©ã†ã‹åˆ¤å®š
                    elif 'texts' in request_data and request_data.get('batch_mode', False):
                        # ãƒãƒƒãƒç¿»è¨³å‡¦ç†
                        batch_request = BatchTranslationRequest(
                            texts=request_data['texts'],
                            source_lang=request_data.get('source_lang', 'en'),
                            target_lang=request_data.get('target_lang', 'ja'),
                            batch_mode=request_data.get('batch_mode', True),
                            max_batch_size=request_data.get('max_batch_size', 50)
                        )
                        
                        batch_response = await self.translate_batch(batch_request)
                        
                        response_data = {
                            'success': batch_response.success,
                            'translations': batch_response.translations,
                            'confidence_scores': batch_response.confidence_scores,
                            'processing_time': batch_response.processing_time,
                            'batch_size': batch_response.batch_size,
                            'errors': batch_response.errors,
                            'model': 'NLLB-200'
                        }
                    else:
                        # å˜ä¸€ç¿»è¨³å‡¦ç†ï¼ˆå¾“æ¥ã®å‡¦ç†ï¼‰
                        request = TranslationRequest(
                            text=request_data['text'],
                            source_lang=request_data.get('source_lang', 'en'),
                            target_lang=request_data.get('target_lang', 'ja'),
                            request_id=request_data.get('request_id')
                        )
                        
                        response = await self.translate(request)
                        
                        response_data = {
                            'success': response.success,
                            'translation': response.translation,
                            'confidence': response.confidence,
                            'error': response.error,
                            'error_code': response.error_code,
                            'processing_time': response.processing_time,
                            'model': 'NLLB-200'
                        }
                    
                    response_json = json.dumps(response_data, ensure_ascii=False) + '\n'
                    writer.write(response_json.encode('utf-8'))
                    await writer.drain()
                    
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': 'Invalid JSON format',
                        'error_code': 'INVALID_JSON',
                        'model': 'NLLB-200'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
                except KeyError as e:
                    logger.error(f"Missing required field: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': f'Missing required field: {e}',
                        'error_code': 'MISSING_FIELD',
                        'model': 'NLLB-200'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
                except Exception as e:
                    logger.error(f"Request processing error: {e}")
                    error_response = json.dumps({
                        'success': False,
                        'error': f'Request processing failed: {str(e)}',
                        'error_code': 'REQUEST_PROCESSING_ERROR',
                        'model': 'NLLB-200'
                    }) + '\n'
                    writer.write(error_response.encode('utf-8'))
                    await writer.drain()
                    
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(f"Client handler error: {e}")
        finally:
            writer.close()
            await writer.wait_closed()
            logger.info(f"Client disconnected: {client_addr}")
            
    async def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        server = await asyncio.start_server(
            self.handle_client,
            '127.0.0.1',
            self.port
        )
        
        addr = server.sockets[0].getsockname()
        logger.info(f"NLLB-200 Translation Server listening on {addr[0]}:{addr[1]}")
        
        # çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«å‡ºåŠ›
        asyncio.create_task(self._print_stats())
        
        async with server:
            await server.serve_forever()
            
    async def _print_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’å®šæœŸçš„ã«å‡ºåŠ›"""
        while True:
            await asyncio.sleep(60)  # 1åˆ†ã”ã¨
            if self.request_count > 0:
                avg_time = self.total_processing_time / self.request_count
                logger.info(f"NLLB-200 Stats - Requests: {self.request_count}, Avg time: {avg_time:.1f}ms")
                
    def shutdown(self, signum, frame):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
        logger.info("Shutting down NLLB-200 server...")
        self.executor.shutdown(wait=True)
        sys.exit(0)

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(description='NLLB-200 Translation Server')
    parser.add_argument('--port', type=int, default=5556, help='Server port')
    args = parser.parse_args()
    
    # ã‚µãƒ¼ãƒãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    server = NllbTranslationServer(port=args.port)
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
    signal.signal(signal.SIGINT, server.shutdown)
    signal.signal(signal.SIGTERM, server.shutdown)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    server.load_model()
    
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    try:
        asyncio.run(server.start_server())
    except KeyboardInterrupt:
        logger.info("NLLB-200 Server stopped by user")

if __name__ == "__main__":
    main()